{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal NER Pipeline with Domain-Adaptive Pretraining (DAPT) - Refactored\n",
        "\n",
        "This notebook demonstrates Domain-Adaptive Pretraining (DAPT) using Masked Language Modeling (MLM) \n",
        "on Serbian legal documents before fine-tuning for NER using shared modules.\n",
        "\n",
        "## Key Features:\n",
        "- **Domain-Adaptive Pretraining**: MLM on unlabeled legal documents\n",
        "- **Two-Stage Training**: MLM pretraining ‚Üí NER fine-tuning\n",
        "- **Legal Domain Knowledge**: Better understanding of legal terminology\n",
        "- **Improved Performance**: Better generalization on legal NER tasks\n",
        "\n",
        "## Training Pipeline:\n",
        "1. **Stage 1**: MLM pretraining on unlabeled legal documents (3 epochs, 5e-5 LR)\n",
        "2. **Stage 2**: NER fine-tuning on labeled data using adapted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the shared modules to path\n",
        "sys.path.append('/content/drive/MyDrive/NER_Master/ner/shared')\n",
        "\n",
        "# Reload shared modules to get latest changes\n",
        "import importlib\n",
        "import shared\n",
        "import shared.model_utils\n",
        "import shared.data_processing\n",
        "import shared.dataset\n",
        "import shared.evaluation\n",
        "import shared.config\n",
        "importlib.reload(shared.config)\n",
        "importlib.reload(shared.data_processing)\n",
        "importlib.reload(shared.dataset)\n",
        "importlib.reload(shared.model_utils)\n",
        "importlib.reload(shared.evaluation)\n",
        "importlib.reload(shared)\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment, get_default_training_args,\n",
        "    \n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data, \n",
        "    analyze_labelstudio_data, validate_bio_examples, load_mlm_documents,\n",
        "    \n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "    \n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "    load_inference_pipeline,\n",
        "    \n",
        "    \n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification, AutoTokenizer\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)\n",
        "print(f\"üîß Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment and paths for Google Colab\n",
        "env_setup = setup_environment(use_local=False, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration for DAPT\n",
        "BASE_MODEL_NAME = \"classla/bcms-bertic\"\n",
        "\n",
        "# MLM Configuration (notebook-specific)\n",
        "mlm_config = {\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 8,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"max_length\": 512,\n",
        "    \"stride\": 256,\n",
        "    \"mlm_probability\": 0.15,\n",
        "    \"save_steps\": 1000,\n",
        "    \"logging_steps\": 100\n",
        "}\n",
        "\n",
        "# NER Configuration\n",
        "experiment_config = {\n",
        "    \"num_train_epochs\": 8,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_length\": 512,\n",
        "    \"stride\": 128\n",
        "}\n",
        "\n",
        "# Output directories\n",
        "MLM_OUTPUT_DIR = f\"{paths['models_dir']}/bertic_dapt_mlm_adapted\"\n",
        "NER_OUTPUT_DIR = f\"{paths['models_dir']}/bertic_dapt_ner_refactored\"\n",
        "os.makedirs(MLM_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(NER_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß DAPT Configuration:\")\n",
        "print(f\"  Base model: {BASE_MODEL_NAME}\")\n",
        "print(f\"  MLM output: {MLM_OUTPUT_DIR}\")\n",
        "print(f\"  NER output: {NER_OUTPUT_DIR}\")\n",
        "print(f\"  MLM data: {paths['mlm_data_dir']}\")\n",
        "print(f\"\\nüìö MLM Configuration:\")\n",
        "print(f\"  Epochs: {mlm_config['num_epochs']}\")\n",
        "print(f\"  Batch size: {mlm_config['batch_size']}\")\n",
        "print(f\"  Learning rate: {mlm_config['learning_rate']}\")\n",
        "print(f\"  Max length: {mlm_config['max_length']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MLM Pretraining Functions (Notebook-specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLM Pretraining Functions (notebook-specific)\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM, DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_mlm_data(documents: List[str], tokenizer, max_length: int = 512, stride: int = 256):\n",
        "    \"\"\"Preprocess documents for MLM training\"\"\"\n",
        "    # Split long documents into chunks\n",
        "    chunks = []\n",
        "    for doc in tqdm(documents, desc=\"Processing MLM documents\"):\n",
        "        # Tokenize document\n",
        "        tokens = tokenizer.tokenize(doc)\n",
        "        \n",
        "        # Split into overlapping chunks\n",
        "        for i in range(0, len(tokens), stride):\n",
        "            chunk_tokens = tokens[i:i + max_length - 2]  # -2 for [CLS] and [SEP]\n",
        "            if len(chunk_tokens) > 10:  # Only keep chunks with sufficient content\n",
        "                chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "                chunks.append(chunk_text)\n",
        "    \n",
        "    print(f\"üìÑ Created {len(chunks)} text chunks for MLM training\")\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_dict({\"text\": chunks})\n",
        "    \n",
        "    # Tokenize for MLM\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_length,\n",
        "            return_special_tokens_mask=True\n",
        "        )\n",
        "    \n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"]\n",
        "    )\n",
        "    \n",
        "    return tokenized_dataset\n",
        "\n",
        "def perform_mlm_pretraining(model, tokenizer, train_dataset, output_dir: str, mlm_config: dict):\n",
        "    \"\"\"Perform MLM pretraining on legal documents\"\"\"\n",
        "    # Convert model to MLM model if needed\n",
        "    if not hasattr(model, 'cls'):\n",
        "        model = AutoModelForMaskedLM.from_pretrained(model.name_or_path)\n",
        "    \n",
        "    # Data collator for MLM\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=True,\n",
        "        mlm_probability=mlm_config['mlm_probability']\n",
        "    )\n",
        "    \n",
        "    # Training arguments for MLM\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=mlm_config['num_epochs'],\n",
        "        per_device_train_batch_size=mlm_config['batch_size'],\n",
        "        save_steps=mlm_config['save_steps'],\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        learning_rate=mlm_config['learning_rate'],\n",
        "        weight_decay=mlm_config['weight_decay'],\n",
        "        warmup_steps=mlm_config['warmup_steps'],\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=mlm_config['logging_steps'],\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        report_to=None\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "    \n",
        "    print(f\"üöÄ Starting MLM pretraining...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the adapted model\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    \n",
        "    print(f\"‚úÖ MLM pretraining completed. Model saved to {output_dir}\")\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Stage 1: Domain-Adaptive Pretraining (MLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MLM documents\n",
        "print(\"üìö Loading MLM documents for domain adaptation...\")\n",
        "mlm_documents = load_mlm_documents(paths['mlm_data_dir'])\n",
        "\n",
        "if mlm_documents:\n",
        "    print(f\"‚úÖ Loaded {len(mlm_documents)} documents for MLM training\")\n",
        "    \n",
        "    # Show document statistics\n",
        "    total_chars = sum(len(doc) for doc in mlm_documents)\n",
        "    avg_chars = total_chars / len(mlm_documents)\n",
        "    print(f\"üìä MLM Data Statistics:\")\n",
        "    print(f\"  Total documents: {len(mlm_documents)}\")\n",
        "    print(f\"  Total characters: {total_chars:,}\")\n",
        "    print(f\"  Average document length: {avg_chars:.0f} characters\")\n",
        "else:\n",
        "    print(\"‚ùå No MLM documents found. Please check your MLM data directory.\")\n",
        "    print(f\"Expected path: {paths['mlm_data_dir']}\")\n",
        "    raise Exception(\"MLM data loading failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform MLM pretraining\n",
        "print(\"üöÄ Starting Domain-Adaptive Pretraining (MLM)...\")\n",
        "\n",
        "try:\n",
        "    # Load base model and tokenizer for MLM\n",
        "    from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(BASE_MODEL_NAME)\n",
        "    \n",
        "    # Preprocess MLM data\n",
        "    print(\"üìÑ Preprocessing MLM data...\")\n",
        "    mlm_dataset = preprocess_mlm_data(\n",
        "        mlm_documents, \n",
        "        tokenizer, \n",
        "        max_length=mlm_config['max_length'], \n",
        "        stride=mlm_config['stride']\n",
        "    )\n",
        "    \n",
        "    # Perform MLM pretraining\n",
        "    print(\"üöÄ Starting MLM pretraining...\")\n",
        "    mlm_trainer = perform_mlm_pretraining(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=mlm_dataset,\n",
        "        output_dir=MLM_OUTPUT_DIR,\n",
        "        mlm_config=mlm_config\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ MLM pretraining completed successfully!\")\n",
        "    print(f\"üìÅ Domain-adapted model saved to: {MLM_OUTPUT_DIR}\")\n",
        "    ADAPTED_MODEL_NAME = MLM_OUTPUT_DIR\n",
        "    mlm_success = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå MLM pretraining failed: {e}\")\n",
        "    print(\"Using base model for NER training\")\n",
        "    ADAPTED_MODEL_NAME = BASE_MODEL_NAME\n",
        "    mlm_success = False\n",
        "\n",
        "print(f\"\\nüéØ Will use model for NER: {ADAPTED_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Stage 2: NER Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze LabelStudio data for NER\n",
        "print(\"üìã Loading NER training data...\")\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "    \n",
        "    # Convert to BIO format\n",
        "    converter = LabelStudioToBIOConverter(\n",
        "        judgments_dir=paths['judgments_dir'],\n",
        "        labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        "    )\n",
        "    \n",
        "    bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "    print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "    \n",
        "    # Validate BIO examples\n",
        "    valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "    print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")\n",
        "else:\n",
        "    print(\"‚ùå No NER data loaded. Please check your paths.\")\n",
        "    raise Exception(\"NER data loading failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. NER Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "# Split dataset\n",
        "train_examples, val_examples, test_examples = split_dataset(\n",
        "    prepared_examples, test_size=0.2, val_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"üìä NER Dataset split:\")\n",
        "print(f\"  Training: {len(train_examples)} examples\")\n",
        "print(f\"  Validation: {len(val_examples)} examples\")\n",
        "print(f\"  Test: {len(test_examples)} examples\")\n",
        "print(f\"  Total labels: {ner_dataset.get_num_labels()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Domain-Adapted Model for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load domain-adapted model and tokenizer for NER\n",
        "print(f\"üîÑ Loading domain-adapted model for NER fine-tuning...\")\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(\n",
        "    ADAPTED_MODEL_NAME,  # Use the MLM-adapted model\n",
        "    ner_dataset.get_num_labels(),\n",
        "    ner_dataset.id_to_label,\n",
        "    ner_dataset.label_to_id\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Domain-adapted model loaded for NER\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"üß† Model has been pre-adapted to legal domain via MLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. NER Data Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize NER datasets with sliding window\n",
        "print(\"üî§ Tokenizing NER datasets with domain-adapted tokenizer...\")\n",
        "\n",
        "train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    train_examples, tokenizer, ner_dataset.label_to_id, \n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "test_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    test_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "# Create HuggingFace datasets\n",
        "train_dataset, val_dataset, test_dataset = create_huggingface_datasets(\n",
        "    train_tokenized, val_tokenized, test_tokenized\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ NER tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. NER Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training arguments for NER fine-tuning\n",
        "training_args = create_training_arguments(\n",
        "    output_dir=NER_OUTPUT_DIR,\n",
        "    num_train_epochs=experiment_config['num_train_epochs'],\n",
        "    per_device_train_batch_size=experiment_config['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=experiment_config['per_device_eval_batch_size'],\n",
        "    learning_rate=experiment_config['learning_rate'],\n",
        "    warmup_steps=experiment_config['warmup_steps'],\n",
        "    weight_decay=experiment_config['weight_decay'],\n",
        "    logging_steps=50,\n",
        "    eval_steps=100,\n",
        "    save_steps=500,\n",
        "    early_stopping_patience=3\n",
        ")\n",
        "\n",
        "# Create trainer for NER fine-tuning\n",
        "trainer = create_trainer(\n",
        "    model=model,\n",
        "    training_args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    id_to_label=ner_dataset.id_to_label,\n",
        "    early_stopping_patience=3\n",
        ")\n",
        "\n",
        "print(\"üèãÔ∏è  NER trainer created for domain-adapted model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. NER Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start NER fine-tuning on domain-adapted model\n",
        "print(\"üöÄ Starting NER fine-tuning on domain-adapted model...\")\n",
        "print(\"üß† Model already adapted to legal domain via MLM pretraining\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"üíæ Saving DAPT NER model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(NER_OUTPUT_DIR)\n",
        "\n",
        "# Save model info with DAPT details\n",
        "save_model_info(\n",
        "    output_dir=NER_OUTPUT_DIR,\n",
        "    model_name=BASE_MODEL_NAME,\n",
        "    model_type=\"dapt_ner\",\n",
        "    num_labels=ner_dataset.get_num_labels(),\n",
        "    id_to_label=ner_dataset.id_to_label,\n",
        "    label_to_id=ner_dataset.label_to_id,\n",
        "    training_args=training_args,\n",
        "    additional_info={\n",
        "        \"base_model\": BASE_MODEL_NAME,\n",
        "        \"mlm_adapted_model\": ADAPTED_MODEL_NAME,\n",
        "        \"mlm_epochs\": mlm_config['num_epochs'],\n",
        "        \"mlm_lr\": mlm_config['learning_rate'],\n",
        "        \"uses_dapt\": True,\n",
        "        \"training_stages\": [\"MLM_pretraining\", \"NER_finetuning\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DAPT NER training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate DAPT model on test set\n",
        "print(\"üìä Evaluating DAPT model on test set...\")\n",
        "\n",
        "test_results = detailed_evaluation(\n",
        "    trainer=trainer,\n",
        "    dataset=test_dataset,\n",
        "    dataset_name=\"Test (DAPT)\",\n",
        "    id_to_label=ner_dataset.id_to_label\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà DAPT Test Results:\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Expected improvements with DAPT:\")\n",
        "print(f\"  ‚úÖ Better understanding of legal terminology\")\n",
        "print(f\"  ‚úÖ Improved contextual representations\")\n",
        "print(f\"  ‚úÖ Better generalization on legal texts\")\n",
        "print(f\"  ‚úÖ Enhanced domain-specific knowledge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Comprehensive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive evaluation report\n",
        "evaluation_report = generate_evaluation_report(\n",
        "    true_labels=test_results['true_labels'],\n",
        "    predictions=test_results['true_predictions'],\n",
        "    dataset_name=\"Test (DAPT)\",\n",
        "    focus_entities=[\"COURT\", \"JUDGE\", \"DEFENDANT\", \"CRIMINAL_ACT\", \"PROVISION\"]\n",
        ")\n",
        "\n",
        "print(\"\\nüß† DAPT Benefits Analysis:\")\n",
        "print(\"\\nüìö MLM Pretraining Stage:\")\n",
        "print(f\"  ‚Ä¢ Trained on {len(mlm_documents)} legal documents\")\n",
        "print(f\"  ‚Ä¢ {mlm_config['num_epochs']} epochs of domain adaptation\")\n",
        "print(f\"  ‚Ä¢ Learning rate: {mlm_config['learning_rate']}\")\n",
        "print(f\"  ‚Ä¢ Model adapted to legal vocabulary and syntax\")\n",
        "\n",
        "print(\"\\nüéØ NER Fine-tuning Stage:\")\n",
        "print(f\"  ‚Ä¢ Started from domain-adapted model\")\n",
        "print(f\"  ‚Ä¢ {experiment_config['num_train_epochs']} epochs of NER training\")\n",
        "print(f\"  ‚Ä¢ Learning rate: {experiment_config['learning_rate']}\")\n",
        "print(f\"  ‚Ä¢ Better initialization for legal NER task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Training History and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(trainer)\n",
        "\n",
        "# Plot entity distribution\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "plot_entity_distribution(label_stats['entity_counts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Inference Pipeline Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load DAPT inference pipeline\n",
        "pipeline = load_inference_pipeline(\n",
        "    model_path=NER_OUTPUT_DIR,\n",
        "    max_length=experiment_config['max_length'],\n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "# Test with sample text\n",
        "sample_text = \"\"\"–û—Å–Ω–æ–≤–Ω–∏ —Å—É–¥ —É –ë–µ–æ–≥—Ä–∞–¥—É –¥–æ–Ω–µ–æ —ò–µ –ø—Ä–µ—Å—É–¥—É —É –∫—Ä–∏–≤–∏—á–Ω–æ–º –ø—Ä–µ–¥–º–µ—Ç—É –ö-1234/2023 –ø—Ä–æ—Ç–∏–≤ –æ–ø—Ç—É–∂–µ–Ω–æ–≥ –ú–∞—Ä–∫–∞ –ü–µ—Ç—Ä–æ–≤–∏—õ–∞ –∑–∞ –∫—Ä–∏–≤–∏—á–Ω–æ –¥–µ–ª–æ –∫—Ä–∞—í–µ –∏–∑ —á–ª–∞–Ω–∞ 203 –ö—Ä–∏–≤–∏—á–Ω–æ–≥ –∑–∞–∫–æ–Ω–∏–∫–∞. –°—É–¥–∏—ò–∞ –ê–Ω–∞ –ù–∏–∫–æ–ª–∏—õ –∏–∑—Ä–µ–∫–ª–∞ —ò–µ –∫–∞–∑–Ω—É –∑–∞—Ç–≤–æ—Ä–∞ —É —Ç—Ä–∞—ò–∞—ö—É –æ–¥ 6 –º–µ—Å–µ—Ü–∏.\"\"\"\n",
        "\n",
        "print(\"üîç Testing DAPT inference pipeline:\")\n",
        "print(f\"Input text: {sample_text}\")\n",
        "print(\"\\nüìã Detected entities (with DAPT):\")\n",
        "\n",
        "entities = pipeline.predict(sample_text)\n",
        "for entity in entities:\n",
        "    print(f\"  {entity['label']}: '{entity['text']}' (tokens {entity['start']}-{entity['end']})\")\n",
        "\n",
        "print(f\"\\n‚úÖ Found {len(entities)} entities using DAPT model\")\n",
        "print(\"üß† Model benefits from legal domain knowledge via MLM pretraining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Summary and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéØ DAPT FINAL SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
        "print(f\"Training examples: {len(train_examples)}\")\n",
        "print(f\"Validation examples: {len(val_examples)}\")\n",
        "print(f\"Test examples: {len(test_examples)}\")\n",
        "print(f\"Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"BIO labels: {len(BIO_LABELS)}\")\n",
        "print(f\"\\nüß† DAPT Configuration:\")\n",
        "print(f\"  MLM documents: {len(mlm_documents)}\")\n",
        "print(f\"  MLM epochs: {mlm_config['num_epochs']}\")\n",
        "print(f\"  MLM learning rate: {mlm_config['learning_rate']}\")\n",
        "print(f\"  NER epochs: {experiment_config['num_train_epochs']}\")\n",
        "print(f\"  NER learning rate: {experiment_config['learning_rate']}\")\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
        "print(f\"\\nModels saved to:\")\n",
        "print(f\"  MLM adapted: {MLM_OUTPUT_DIR}\")\n",
        "print(f\"  NER final: {NER_OUTPUT_DIR}\")\n",
        "print(\"\\n‚úÖ DAPT pipeline completed successfully!\")\n",
        "print(\"\\nüí° DAPT advantages:\")\n",
        "print(\"   ‚Ä¢ Domain-specific language understanding\")\n",
        "print(\"   ‚Ä¢ Better legal terminology comprehension\")\n",
        "print(\"   ‚Ä¢ Improved contextual representations\")\n",
        "print(\"   ‚Ä¢ Enhanced generalization on legal texts\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
