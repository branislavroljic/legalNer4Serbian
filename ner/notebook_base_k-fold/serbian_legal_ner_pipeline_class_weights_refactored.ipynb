{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal NER Pipeline with Class Weights - Refactored\n",
        "\n",
        "This notebook demonstrates the class-weighted approach for Serbian Legal NER using shared modules.\n",
        "Class weights help address the imbalanced distribution of entity types in legal documents.\n",
        "\n",
        "## Key Features:\n",
        "- **Class Weighting**: Higher weights for rare entity types\n",
        "- **Imbalance Handling**: Better performance on underrepresented entities\n",
        "- **Weighted Loss Function**: Penalizes misclassification of rare entities more heavily\n",
        "- **Improved Recall**: Better detection of low-frequency entities like CASE_NUMBER, JUDGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the shared modules to path\n",
        "sys.path.append('/content/drive/MyDrive/NER_Master/ner/')\n",
        "\n",
        "# Reload shared modules to get latest changes\n",
        "import importlib\n",
        "import shared\n",
        "import shared.model_utils\n",
        "import shared.data_processing\n",
        "import shared.dataset\n",
        "import shared.evaluation\n",
        "import shared.config\n",
        "importlib.reload(shared.config)\n",
        "importlib.reload(shared.data_processing)\n",
        "importlib.reload(shared.dataset)\n",
        "importlib.reload(shared.model_utils)\n",
        "importlib.reload(shared.evaluation)\n",
        "importlib.reload(shared)\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment, get_default_training_args,\n",
        "    \n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data, \n",
        "    analyze_labelstudio_data, validate_bio_examples,\n",
        "    \n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "    \n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "    load_inference_pipeline,\n",
        "\n",
        "    \n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification, Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Class Weights Configuration (notebook-specific)\n",
        "DEFAULT_CLASS_WEIGHTS = {\n",
        "    \"CASE_NUMBER\": 43.24,\n",
        "    \"JUDGE\": 22.90,\n",
        "    \"REGISTRAR\": 23.68,\n",
        "    \"SANCTION_TYPE\": 25.78,\n",
        "    \"PROCEDURE_COSTS\": 23.13,\n",
        "    \"COURT\": 1.0,\n",
        "    \"CRIMINAL_ACT\": 1.0,\n",
        "    \"DEFENDANT\": 1.0,\n",
        "    \"PROSECUTOR\": 1.0,\n",
        "    \"PROVISION\": 1.0\n",
        "}\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)\n",
        "print(f\"üîß Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Weights Functions to Add to Class Weights Notebook\n",
        "# Copy this code into a new cell in the Class Weights notebook after the imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Class Weights Configuration (notebook-specific)\n",
        "DEFAULT_CLASS_WEIGHTS = {\n",
        "    \"CASE_NUMBER\": 43.24,\n",
        "    \"JUDGE\": 22.90,\n",
        "    \"REGISTRAR\": 23.68,\n",
        "    \"SANCTION_TYPE\": 25.78,\n",
        "    \"PROCEDURE_COSTS\": 23.13,\n",
        "    \"COURT\": 1.0,\n",
        "    \"CRIMINAL_ACT\": 1.0,\n",
        "    \"DEFENDANT\": 1.0,\n",
        "    \"PROSECUTOR\": 1.0,\n",
        "    \"PROVISION\": 1.0\n",
        "}\n",
        "\n",
        "def calculate_class_weights(examples, label_to_id, method=\"inverse_frequency\"):\n",
        "    \"\"\"Calculate class weights based on label frequency\"\"\"\n",
        "    \n",
        "    # Count label frequencies\n",
        "    label_counts = Counter()\n",
        "    total_tokens = 0\n",
        "    \n",
        "    for example in examples:\n",
        "        for label in example['labels']:\n",
        "            if label != 'O':  # Ignore 'O' labels\n",
        "                entity_type = label.split('-')[-1] if '-' in label else label\n",
        "                label_counts[entity_type] += 1\n",
        "                total_tokens += 1\n",
        "    \n",
        "    # Calculate weights\n",
        "    weights = {}\n",
        "    \n",
        "    if method == \"inverse_frequency\":\n",
        "        # Inverse frequency weighting\n",
        "        for entity_type in label_counts:\n",
        "            frequency = label_counts[entity_type] / total_tokens\n",
        "            weights[entity_type] = 1.0 / frequency\n",
        "            \n",
        "        # Normalize weights\n",
        "        min_weight = min(weights.values())\n",
        "        for entity_type in weights:\n",
        "            weights[entity_type] = weights[entity_type] / min_weight\n",
        "            \n",
        "    elif method == \"balanced\":\n",
        "        # Balanced class weights (sklearn style)\n",
        "        n_classes = len(label_counts)\n",
        "        for entity_type in label_counts:\n",
        "            weights[entity_type] = total_tokens / (n_classes * label_counts[entity_type])\n",
        "    \n",
        "    return weights\n",
        "\n",
        "# Custom Weighted Trainer (notebook-specific)\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Custom trainer that applies class weights to the loss function\"\"\"\n",
        "    \n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "        if class_weights is not None:\n",
        "            # Convert to tensor and move to device\n",
        "            self.class_weights_tensor = torch.tensor(\n",
        "                list(class_weights.values()), \n",
        "                dtype=torch.float32\n",
        "            )\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"\n",
        "        Override compute_loss to apply class weights\n",
        "        \"\"\"\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        if labels is not None and self.class_weights is not None:\n",
        "            # Move class weights to the same device as the model\n",
        "            if self.class_weights_tensor.device != outputs.logits.device:\n",
        "                self.class_weights_tensor = self.class_weights_tensor.to(outputs.logits.device)\n",
        "            \n",
        "            # Compute weighted cross entropy loss\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights_tensor, ignore_index=-100)\n",
        "            \n",
        "            # Flatten for loss computation\n",
        "            active_loss = inputs[\"attention_mask\"].view(-1) == 1\n",
        "            active_logits = outputs.logits.view(-1, self.model.config.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, \n",
        "                labels.view(-1), \n",
        "                torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            \n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "        else:\n",
        "            # Use default loss if no class weights\n",
        "            loss = outputs.loss\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "print(\"‚úÖ Class weights functions and WeightedTrainer defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment and paths for Google Colab\n",
        "env_setup = setup_environment(use_local=False, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration for class weights\n",
        "MODEL_NAME = \"classla/bcms-bertic\"\n",
        "experiment_config = get_experiment_config(\"class_weights\")\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_class_weights_refactored\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß Class Weights Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"  BIO labels: {len(BIO_LABELS)}\")\n",
        "print(f\"  Learning rate: {experiment_config['learning_rate']}\")\n",
        "print(f\"  Epochs: {experiment_config['num_train_epochs']}\")\n",
        "print(f\"  Batch size: {experiment_config['per_device_train_batch_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze LabelStudio data\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "    \n",
        "    # Convert to BIO format\n",
        "    converter = LabelStudioToBIOConverter(\n",
        "        judgments_dir=paths['judgments_dir'],\n",
        "        labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        "    )\n",
        "    \n",
        "    bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "    print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "    \n",
        "    # Validate BIO examples\n",
        "    valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "    print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
        "    raise Exception(\"Data loading failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Preparation and Class Weight Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "# Split dataset\n",
        "train_examples, val_examples, test_examples = split_dataset(\n",
        "    prepared_examples, test_size=0.2, val_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Calculate class weights based on training data\n",
        "class_weights = calculate_class_weights(\n",
        "    train_examples, \n",
        "    ner_dataset.label_to_id,\n",
        "    method='inverse_frequency'\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset split:\")\n",
        "print(f\"  Training: {len(train_examples)} examples\")\n",
        "print(f\"  Validation: {len(val_examples)} examples\")\n",
        "print(f\"  Test: {len(test_examples)} examples\")\n",
        "print(f\"  Total labels: {ner_dataset.get_num_labels()}\")\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  Calculated class weights:\")\n",
        "for label_id, weight in enumerate(class_weights):\n",
        "    label_name = ner_dataset.id_to_label[label_id]\n",
        "    print(f\"  {label_name}: {weight:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Creation with Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(\n",
        "    MODEL_NAME, \n",
        "    ner_dataset.get_num_labels(),\n",
        "    ner_dataset.id_to_label,\n",
        "    ner_dataset.label_to_id\n",
        ")\n",
        "\n",
        "# Convert class weights to tensor and move to device\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Create custom weighted loss function\n",
        "import torch.nn as nn\n",
        "\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
        "    \n",
        "    def forward(self, logits, labels):\n",
        "        # Reshape for loss calculation\n",
        "        active_loss = labels.view(-1) != -100\n",
        "        active_logits = logits.view(-1, logits.shape[-1])[active_loss]\n",
        "        active_labels = labels.view(-1)[active_loss]\n",
        "        \n",
        "        return self.loss_fn(active_logits, active_labels)\n",
        "\n",
        "# Replace model's loss function\n",
        "weighted_loss = WeightedCrossEntropyLoss(class_weights_tensor)\n",
        "\n",
        "print(f\"‚úÖ Model created with weighted loss function\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize datasets with sliding window\n",
        "print(\"üî§ Tokenizing datasets...\")\n",
        "\n",
        "train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    train_examples, tokenizer, ner_dataset.label_to_id, \n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "test_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    test_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=experiment_config['max_length'], \n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "# Create HuggingFace datasets\n",
        "train_dataset, val_dataset, test_dataset = create_huggingface_datasets(\n",
        "    train_tokenized, val_tokenized, test_tokenized\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Setup with Weighted Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom trainer with weighted loss\n",
        "from transformers import Trainer\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, weighted_loss_fn, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weighted_loss_fn = weighted_loss_fn\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        \n",
        "        # Use weighted loss\n",
        "        loss = self.weighted_loss_fn(logits, labels)\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Create training arguments\n",
        "training_args = create_training_arguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=experiment_config['num_train_epochs'],\n",
        "    per_device_train_batch_size=experiment_config['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=experiment_config['per_device_eval_batch_size'],\n",
        "    learning_rate=experiment_config['learning_rate'],\n",
        "    warmup_steps=experiment_config['warmup_steps'],\n",
        "    weight_decay=experiment_config['weight_decay'],\n",
        "    logging_steps=50,\n",
        "    eval_steps=100,\n",
        "    save_steps=500,\n",
        "    early_stopping_patience=3\n",
        ")\n",
        "\n",
        "# Create weighted trainer\n",
        "trainer = WeightedTrainer(\n",
        "    weighted_loss_fn=weighted_loss,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=lambda eval_pred: {\n",
        "        'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.0\n",
        "    }  # Placeholder metrics\n",
        ")\n",
        "\n",
        "print(\"üèãÔ∏è  Weighted trainer created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training with Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training with class weights\n",
        "print(\"üöÄ Starting training with class weights...\")\n",
        "print(\"‚öñÔ∏è  Using weighted loss to handle class imbalance\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"üíæ Saving class-weighted model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save model info with class weights details\n",
        "save_model_info(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    model_name=MODEL_NAME,\n",
        "    model_type=\"class_weighted\",\n",
        "    num_labels=ner_dataset.get_num_labels(),\n",
        "    id_to_label=ner_dataset.id_to_label,\n",
        "    label_to_id=ner_dataset.label_to_id,\n",
        "    training_args=training_args,\n",
        "    additional_info={\n",
        "        \"class_weights\": class_weights.tolist(),\n",
        "        \"uses_class_weights\": True,\n",
        "        \"weight_method\": \"inverse_frequency\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Class-weighted training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate class-weighted model on test set\n",
        "print(\"üìä Evaluating class-weighted model on test set...\")\n",
        "\n",
        "test_results = detailed_evaluation(\n",
        "    trainer=trainer,\n",
        "    dataset=test_dataset,\n",
        "    dataset_name=\"Test (Class Weighted)\",\n",
        "    id_to_label=ner_dataset.id_to_label\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Class-Weighted Test Results:\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Expected improvements with class weights:\")\n",
        "print(f\"  ‚úÖ Better recall for rare entities (CASE_NUMBER, JUDGE)\")\n",
        "print(f\"  ‚úÖ More balanced precision/recall across entity types\")\n",
        "print(f\"  ‚úÖ Reduced bias towards frequent entities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Comprehensive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive evaluation report\n",
        "evaluation_report = generate_evaluation_report(\n",
        "    true_labels=test_results['true_labels'],\n",
        "    predictions=test_results['true_predictions'],\n",
        "    dataset_name=\"Test (Class Weighted)\",\n",
        "    focus_entities=[\"CASE_NUMBER\", \"JUDGE\", \"REGISTRAR\", \"SANCTION_TYPE\", \"PROCEDURE_COSTS\"]\n",
        ")\n",
        "\n",
        "# Show class weight impact\n",
        "print(\"\\n‚öñÔ∏è  Class Weight Impact Analysis:\")\n",
        "print(\"\\nHighest weighted entities (should see improved recall):\")\n",
        "weight_label_pairs = [(class_weights[ner_dataset.label_to_id[label]], label) \n",
        "                     for label in ner_dataset.label_to_id.keys() if label != 'O']\n",
        "weight_label_pairs.sort(reverse=True)\n",
        "\n",
        "for weight, label in weight_label_pairs[:10]:\n",
        "    print(f\"  {label}: weight = {weight:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training History and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(trainer)\n",
        "\n",
        "# Plot entity distribution\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "plot_entity_distribution(label_stats['entity_counts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Inference Pipeline Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load class-weighted inference pipeline\n",
        "pipeline = load_inference_pipeline(\n",
        "    model_path=OUTPUT_DIR,\n",
        "    max_length=experiment_config['max_length'],\n",
        "    stride=experiment_config['stride']\n",
        ")\n",
        "\n",
        "# Test with sample text\n",
        "sample_text = \"\"\"–û—Å–Ω–æ–≤–Ω–∏ —Å—É–¥ —É –ë–µ–æ–≥—Ä–∞–¥—É –¥–æ–Ω–µ–æ —ò–µ –ø—Ä–µ—Å—É–¥—É —É –∫—Ä–∏–≤–∏—á–Ω–æ–º –ø—Ä–µ–¥–º–µ—Ç—É –ö-1234/2023 –ø—Ä–æ—Ç–∏–≤ –æ–ø—Ç—É–∂–µ–Ω–æ–≥ –ú–∞—Ä–∫–∞ –ü–µ—Ç—Ä–æ–≤–∏—õ–∞ –∑–∞ –∫—Ä–∏–≤–∏—á–Ω–æ –¥–µ–ª–æ –∫—Ä–∞—í–µ –∏–∑ —á–ª–∞–Ω–∞ 203 –ö—Ä–∏–≤–∏—á–Ω–æ–≥ –∑–∞–∫–æ–Ω–∏–∫–∞. –°—É–¥–∏—ò–∞ –ê–Ω–∞ –ù–∏–∫–æ–ª–∏—õ –∏–∑—Ä–µ–∫–ª–∞ —ò–µ –∫–∞–∑–Ω—É –∑–∞—Ç–≤–æ—Ä–∞ —É —Ç—Ä–∞—ò–∞—ö—É –æ–¥ 6 –º–µ—Å–µ—Ü–∏.\"\"\"\n",
        "\n",
        "print(\"üîç Testing class-weighted inference pipeline:\")\n",
        "print(f\"Input text: {sample_text}\")\n",
        "print(\"\\nüìã Detected entities (with class weighting):\")\n",
        "\n",
        "entities = pipeline.predict(sample_text)\n",
        "for entity in entities:\n",
        "    weight = class_weights[ner_dataset.label_to_id.get(f\"B-{entity['label']}\", 0)]\n",
        "    print(f\"  {entity['label']}: '{entity['text']}' (weight: {weight:.2f})\")\n",
        "\n",
        "print(f\"\\n‚úÖ Found {len(entities)} entities using class-weighted model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéØ CLASS-WEIGHTED FINAL SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {MODEL_NAME} + Class Weights\")\n",
        "print(f\"Training examples: {len(train_examples)}\")\n",
        "print(f\"Validation examples: {len(val_examples)}\")\n",
        "print(f\"Test examples: {len(test_examples)}\")\n",
        "print(f\"Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"BIO labels: {len(BIO_LABELS)}\")\n",
        "print(f\"\\nClass Weighting Configuration:\")\n",
        "print(f\"  Weight method: inverse_frequency\")\n",
        "print(f\"  Learning rate: {experiment_config['learning_rate']}\")\n",
        "print(f\"  Epochs: {experiment_config['num_train_epochs']}\")\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
        "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\n‚úÖ Class-weighted pipeline completed successfully!\")\n",
        "print(\"\\nüí° Class weighting helps with:\")\n",
        "print(\"   ‚Ä¢ Better recall for rare entities\")\n",
        "print(\"   ‚Ä¢ Handling class imbalance\")\n",
        "print(\"   ‚Ä¢ More balanced performance across entity types\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
