{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal Named Entity Recognition (NER) Pipeline - XLM-R-BERTiƒá 5-Fold Cross-Validation\n",
        "\n",
        "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using XLM-R-BERTiƒá model.\n",
        "XLM-R-BERTiƒá combines multilingual XLM-RoBERTa with Serbian-specific fine-tuning for better performance on Serbian text.\n",
        "\n",
        "## Key Features\n",
        "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
        "- **XLM-R-BERTiƒá Architecture**: Multilingual model with Serbian specialization\n",
        "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
        "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
        "- **Statistical Analysis**: Mean and standard deviation across folds\n",
        "\n",
        "## XLM-R-BERTiƒá Advantages\n",
        "- **Multilingual Foundation**: Built on XLM-RoBERTa's strong multilingual capabilities\n",
        "- **Serbian Specialization**: Fine-tuned specifically for Serbian language tasks\n",
        "- **Better Generalization**: Potential for better performance on diverse Serbian legal texts\n",
        "- **Cross-lingual Transfer**: Benefits from multilingual pretraining\n",
        "\n",
        "## Entity Types\n",
        "- **COURT**: Court institutions\n",
        "- **DECISION_DATE**: Dates of legal decisions\n",
        "- **CASE_NUMBER**: Case identifiers\n",
        "- **CRIMINAL_ACT**: Criminal acts/charges\n",
        "- **PROSECUTOR**: Prosecutor entities\n",
        "- **DEFENDANT**: Defendant entities\n",
        "- **JUDGE**: Judge names\n",
        "- **REGISTRAR**: Court registrar\n",
        "- **SANCTION**: Sanctions/penalties\n",
        "- **SANCTION_TYPE**: Type of sanction\n",
        "- **SANCTION_VALUE**: Value/duration of sanction\n",
        "- **PROVISION**: Legal provisions\n",
        "- **PROCEDURE_COSTS**: Legal procedure costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    USE_COLAB = True\n",
        "except ImportError:\n",
        "    USE_COLAB = False\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHG6Rs8jTOo"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11kIuCNPjTOo"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ig3oNUjTOo"
      },
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the shared modules to path\n",
        "if USE_COLAB:\n",
        "    sys.path.append('/content/drive/MyDrive/NER_Master/ner/')\n",
        "else:\n",
        "    sys.path.append('../shared')\n",
        "\n",
        "import importlib\n",
        "import shared\n",
        "import shared.model_utils\n",
        "import shared.data_processing\n",
        "import shared.dataset\n",
        "import shared.evaluation\n",
        "import shared.config\n",
        "importlib.reload(shared.config)\n",
        "importlib.reload(shared.data_processing)\n",
        "importlib.reload(shared.dataset)\n",
        "importlib.reload(shared.model_utils)\n",
        "importlib.reload(shared.evaluation)\n",
        "importlib.reload(shared)\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment,\n",
        "\n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
        "    analyze_labelstudio_data, validate_bio_examples,\n",
        "\n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "\n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "    PerClassMetricsCallback,\n",
        "\n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution,\n",
        "    # Comprehensive tracking\n",
        "    analyze_entity_distribution_per_fold,\n",
        "    generate_detailed_classification_report,\n",
        "    # Aggregate functions across all folds\n",
        "    create_aggregate_report_across_folds\n",
        ")\n",
        "\n",
        "# Standard imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "from transformers import DataCollatorForTokenClassification, AutoTokenizer\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R56QtmR7fIY2"
      },
      "source": [
        "## 2. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i3CBXt2fIY3"
      },
      "outputs": [],
      "source": [
        "# Setup environment and paths\n",
        "env_setup = setup_environment(use_local=not USE_COLAB, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration - XLM-R-BERTiƒá\n",
        "MODEL_NAME = \"classla/xlm-r-bertic\"\n",
        "model_config = get_default_model_config()\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = f\"{paths['models_dir']}/xlm_r_bertic_5fold_cv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Architecture: XLM-R-BERTiƒá\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVSwtVS1fIY3"
      },
      "source": [
        "## 3. Data Loading and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmq8DjhyfIY4"
      },
      "outputs": [],
      "source": [
        "# Load LabelStudio data\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "# Analyze the data\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAyV4gQfIY4"
      },
      "source": [
        "## 4. Data Preprocessing and BIO Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOe5xEDnfIY5"
      },
      "outputs": [],
      "source": [
        "# Convert LabelStudio data to BIO format\n",
        "converter = LabelStudioToBIOConverter(\n",
        "    judgments_dir=paths['judgments_dir'],\n",
        "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        ")\n",
        "\n",
        "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "\n",
        "# Validate BIO examples\n",
        "valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HcyAbInfIY5"
      },
      "source": [
        "## 5. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cX7CQA3fIY5"
      },
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "print(f\"üìä Dataset statistics:\")\n",
        "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
        "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
        "\n",
        "# Get label statistics\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
        "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtKaFXofIY5"
      },
      "source": [
        "## 6. K-Fold Cross-Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOZoMYBNfIY6"
      },
      "outputs": [],
      "source": [
        "# Set up 5-fold cross-validation\n",
        "N_FOLDS = 5\n",
        "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert to numpy array for easier indexing\n",
        "examples_array = np.array(prepared_examples, dtype=object)\n",
        "\n",
        "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
        "print(f\"Total examples: {len(prepared_examples)}\")\n",
        "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
        "\n",
        "# Load tokenizer (will be used across all folds)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Store results from all folds\n",
        "fold_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_helper_functions"
      },
      "source": [
        "## 7. K-Fold Cross-Validation Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_helper_functions_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# XLM-R-BERTiƒá K-FOLD CROSS-VALIDATION HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_fold_data(train_examples, val_examples, tokenizer, ner_dataset):\n",
        "    \"\"\"\n",
        "    Prepare training and validation datasets for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        train_examples: Training examples for this fold\n",
        "        val_examples: Validation examples for this fold\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset, data_collator)\n",
        "    \"\"\"\n",
        "    # Tokenize datasets with sliding window\n",
        "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    # Create HuggingFace datasets\n",
        "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
        "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    return train_dataset, val_dataset, data_collator\n",
        "\n",
        "print(\"‚úÖ XLM-R-BERTiƒá data preparation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlm_r_model_creation"
      },
      "outputs": [],
      "source": [
        "def create_xlm_r_model_and_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device):\n",
        "    \"\"\"\n",
        "    Create XLM-R-BERTiƒá model and trainer for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        fold_num: Current fold number\n",
        "        train_dataset: Training dataset for this fold\n",
        "        val_dataset: Validation dataset for this fold\n",
        "        data_collator: Data collator\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "        device: Device to use (cuda/cpu)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, trainer, metrics_callback, fold_output_dir)\n",
        "    \"\"\"\n",
        "    # Create fold-specific output directory\n",
        "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
        "    import os\n",
        "    os.makedirs(fold_output_dir, exist_ok=True)\n",
        "    \n",
        "    # Load fresh XLM-R-BERTiƒá model for this fold\n",
        "    model, _ = load_model_and_tokenizer(\n",
        "        MODEL_NAME,\n",
        "        ner_dataset.get_num_labels(),\n",
        "        ner_dataset.id_to_label,\n",
        "        ner_dataset.label_to_id\n",
        "    )\n",
        "    \n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    \n",
        "    # Create training arguments for this fold\n",
        "    training_args = create_training_arguments(\n",
        "        output_dir=fold_output_dir,\n",
        "        num_epochs=model_config['num_epochs'],\n",
        "        batch_size=model_config['batch_size'],\n",
        "        learning_rate=model_config['learning_rate'],\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_steps=100,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        report_to=\"none\",  # Disable wandb for cleaner output\n",
        "        run_name=f\"xlm_r_bertic_fold_{fold_num}\"\n",
        "    )\n",
        "    \n",
        "    # Create metrics callback for comprehensive tracking\n",
        "    metrics_callback = PerClassMetricsCallback(id_to_label=ner_dataset.id_to_label)\n",
        "    \n",
        "    # Create trainer with metrics callback\n",
        "    trainer = create_trainer(\n",
        "        model=model,\n",
        "        training_args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        id_to_label=ner_dataset.id_to_label,\n",
        "        early_stopping_patience=3,\n",
        "        additional_callbacks=[metrics_callback]\n",
        "    )\n",
        "    \n",
        "    print(f\"XLM-R-BERTiƒá Trainer initialized for fold {fold_num} with comprehensive metrics tracking\")\n",
        "    return model, trainer, metrics_callback, fold_output_dir\n",
        "\n",
        "print(\"‚úÖ XLM-R-BERTiƒá model and trainer creation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlm_r_evaluation"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_xlm_r_fold(fold_num, trainer, val_dataset, ner_dataset, fold_output_dir):\n",
        "    \"\"\"\n",
        "    Train and evaluate an XLM-R-BERTiƒá model for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        fold_num: Current fold number\n",
        "        trainer: Trainer instance\n",
        "        val_dataset: Validation dataset for this fold\n",
        "        ner_dataset: NER dataset instance\n",
        "        fold_output_dir: Output directory for this fold\n",
        "    \n",
        "    Returns:\n",
        "        dict: Fold results including comprehensive metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\nüèãÔ∏è  Training XLM-R-BERTiƒá fold {fold_num}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    \n",
        "    print(f\"üíæ Saving XLM-R-BERTiƒá model for fold {fold_num}...\")\n",
        "    trainer.save_model()\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(f\"üìä Evaluating XLM-R-BERTiƒá fold {fold_num}...\")\n",
        "    eval_results = detailed_evaluation(\n",
        "        trainer, val_dataset, f\"XLM-R-BERTiƒá Fold {fold_num} Validation\", ner_dataset.id_to_label\n",
        "    )\n",
        "    \n",
        "    # Get predictions for confusion matrix and detailed analysis\n",
        "    true_labels = eval_results['true_labels']\n",
        "    pred_labels = eval_results['true_predictions']\n",
        "    \n",
        "    # Flatten for confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    flat_true = [label for seq in true_labels for label in seq]\n",
        "    flat_pred = [label for seq in pred_labels for label in seq]\n",
        "    all_labels = sorted(list(set(flat_true + flat_pred)))\n",
        "    cm = confusion_matrix(flat_true, flat_pred, labels=all_labels)\n",
        "    \n",
        "    # Generate classification report for this fold\n",
        "    per_class_metrics = generate_detailed_classification_report(\n",
        "        true_labels, pred_labels, fold_output_dir, fold_num, \"Validation\"\n",
        "    )\n",
        "    \n",
        "    # Extract metrics\n",
        "    fold_result = {\n",
        "        'fold': fold_num,\n",
        "        'precision': eval_results['precision'],\n",
        "        'recall': eval_results['recall'],\n",
        "        'f1': eval_results['f1'],\n",
        "        'accuracy': eval_results['accuracy'],\n",
        "        'per_class_metrics': per_class_metrics,\n",
        "        'confusion_matrix': cm,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nXLM-R-BERTiƒá Fold {fold_num} completed successfully!\")\n",
        "    return fold_result\n",
        "\n",
        "print(\"‚úÖ XLM-R-BERTiƒá training and evaluation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_main_training"
      },
      "source": [
        "## 8. K-Fold Cross-Validation Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_main_loop"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Main K-Fold Cross-Validation Loop for XLM-R-BERTiƒá\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION - XLM-R-BERTiƒá\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total examples: {len(examples_array)}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Execute K-Fold training\n",
        "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"XLM-R-BERTiƒá FOLD {fold_num}/{N_FOLDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
        "    \n",
        "    # Get fold data\n",
        "    train_examples = examples_array[train_idx].tolist()\n",
        "    val_examples = examples_array[val_idx].tolist()\n",
        "    \n",
        "    print(f\"Training examples: {len(train_examples)}\")\n",
        "    print(f\"Validation examples: {len(val_examples)}\")\n",
        "    \n",
        "    # Analyze entity distribution for this fold\n",
        "    print(f\"\\nüìä Analyzing entity distribution for fold {fold_num}...\")\n",
        "    train_dist = analyze_entity_distribution_per_fold(train_examples, f\"Fold {fold_num} - Training\")\n",
        "    val_dist = analyze_entity_distribution_per_fold(val_examples, f\"Fold {fold_num} - Validation\")\n",
        "    \n",
        "    # Prepare data for this fold\n",
        "    print(f\"\\nüî§ Preparing data for XLM-R-BERTiƒá fold {fold_num}...\")\n",
        "    train_dataset, val_dataset, data_collator = prepare_fold_data(\n",
        "        train_examples, val_examples, tokenizer, ner_dataset\n",
        "    )\n",
        "    \n",
        "    print(f\"üì¶ XLM-R-BERTiƒá Fold {fold_num} datasets:\")\n",
        "    print(f\"  Training: {len(train_dataset)} examples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
        "    \n",
        "    # Create XLM-R-BERTiƒá model and trainer for this fold\n",
        "    print(f\"\\nü§ñ Creating XLM-R-BERTiƒá model and trainer for fold {fold_num}...\")\n",
        "    model, trainer, metrics_callback, fold_output_dir = create_xlm_r_model_and_trainer(\n",
        "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device\n",
        "    )\n",
        "    \n",
        "    # Train and evaluate this fold\n",
        "    fold_result = train_and_evaluate_xlm_r_fold(\n",
        "        fold_num, trainer, val_dataset, ner_dataset, fold_output_dir\n",
        "    )\n",
        "    \n",
        "    # Store comprehensive data for aggregation\n",
        "    fold_result['distributions'] = {'train': train_dist, 'val': val_dist}\n",
        "    fold_result['training_history'] = metrics_callback.get_training_history()\n",
        "    \n",
        "    fold_results.append(fold_result)\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del model, trainer, metrics_callback, train_dataset, val_dataset\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    print(f\"\\n‚úÖ XLM-R-BERTiƒá Fold {fold_num} completed!\")\n",
        "    print(f\"   Precision: {fold_result['precision']:.4f}\")\n",
        "    print(f\"   Recall: {fold_result['recall']:.4f}\")\n",
        "    print(f\"   F1-Score: {fold_result['f1']:.4f}\")\n",
        "    print(f\"   Accuracy: {fold_result['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"XLM-R-BERTiƒá K-FOLD CROSS-VALIDATION COMPLETED!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggregate_results"
      },
      "source": [
        "## 9. Aggregate Results Across Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggregate_results_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AGGREGATE RESULTS ACROSS FOLDS WITH COMPREHENSIVE VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "# Create comprehensive aggregate report with all visualizations\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"GENERATING COMPREHENSIVE AGGREGATE REPORT FOR XLM-R-BERTiƒá\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "aggregate_report = create_aggregate_report_across_folds(\n",
        "    fold_results=fold_results,\n",
        "    model_name=\"XLM-R-BERTiƒá (classla/bcms-bertic)\",\n",
        "    display=True\n",
        ")\n",
        "\n",
        "# Extract summary metrics\n",
        "precisions = [result['precision'] for result in fold_results]\n",
        "recalls = [result['recall'] for result in fold_results]\n",
        "f1_scores = [result['f1'] for result in fold_results]\n",
        "accuracies = [result['accuracy'] for result in fold_results]\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"XLM-R-BERTiƒá FINAL RESULTS ACROSS {N_FOLDS} FOLDS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
        "print(f\"Accuracy:  {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 10. Visualization of K-Fold Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION OF K-FOLD RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create visualization of fold results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle(f'{N_FOLDS}-Fold Cross-Validation Results - XLM-R-BERTiƒá Model', fontsize=16, fontweight='bold')\n",
        "\n",
        "fold_numbers = list(range(1, N_FOLDS + 1))\n",
        "\n",
        "# Plot precision\n",
        "ax1.plot(fold_numbers, precisions, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
        "ax1.axhline(y=np.mean(precisions), color='r', linestyle='--', label=f'Mean: {np.mean(precisions):.4f}')\n",
        "ax1.set_xlabel('Fold Number', fontsize=12)\n",
        "ax1.set_ylabel('Precision', fontsize=12)\n",
        "ax1.set_title('Precision Across Folds', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot recall\n",
        "ax2.plot(fold_numbers, recalls, marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
        "ax2.axhline(y=np.mean(recalls), color='r', linestyle='--', label=f'Mean: {np.mean(recalls):.4f}')\n",
        "ax2.set_xlabel('Fold Number', fontsize=12)\n",
        "ax2.set_ylabel('Recall', fontsize=12)\n",
        "ax2.set_title('Recall Across Folds', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot F1-score\n",
        "ax3.plot(fold_numbers, f1_scores, marker='^', linewidth=2, markersize=8, color='#F18F01')\n",
        "ax3.axhline(y=np.mean(f1_scores), color='r', linestyle='--', label=f'Mean: {np.mean(f1_scores):.4f}')\n",
        "ax3.set_xlabel('Fold Number', fontsize=12)\n",
        "ax3.set_ylabel('F1-Score', fontsize=12)\n",
        "ax3.set_title('F1-Score Across Folds', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.legend()\n",
        "ax3.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot accuracy\n",
        "ax4.plot(fold_numbers, accuracies, marker='D', linewidth=2, markersize=8, color='#6A994E')\n",
        "ax4.axhline(y=np.mean(accuracies), color='r', linestyle='--', label=f'Mean: {np.mean(accuracies):.4f}')\n",
        "ax4.set_xlabel('Fold Number', fontsize=12)\n",
        "ax4.set_ylabel('Accuracy', fontsize=12)\n",
        "ax4.set_title('Accuracy Across Folds', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.legend()\n",
        "ax4.set_xticks(fold_numbers)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/xlm_r_bertic_5fold_cv_results.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Visualization saved to: {OUTPUT_DIR}/xlm_r_bertic_5fold_cv_results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## 11. Save Results to File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAVE RESULTS TO FILE\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': MODEL_NAME,\n",
        "        'architecture': 'XLM-R-BERTiƒá',\n",
        "        'n_folds': N_FOLDS,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'overall_metrics': {\n",
        "        'precision_mean': float(np.mean(precisions)),\n",
        "        'precision_std': float(np.std(precisions)),\n",
        "        'recall_mean': float(np.mean(recalls)),\n",
        "        'recall_std': float(np.std(recalls)),\n",
        "        'f1_mean': float(np.mean(f1_scores)),\n",
        "        'f1_std': float(np.std(f1_scores)),\n",
        "        'accuracy_mean': float(np.mean(accuracies)),\n",
        "        'accuracy_std': float(np.std(accuracies))\n",
        "    },\n",
        "    'fold_results': [\n",
        "        {\n",
        "            'fold': result['fold'],\n",
        "            'precision': float(result['precision']),\n",
        "            'recall': float(result['recall']),\n",
        "            'f1': float(result['f1']),\n",
        "            'accuracy': float(result['accuracy'])\n",
        "        }\n",
        "        for result in fold_results\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "results_file = f\"{OUTPUT_DIR}/5fold_cv_results.json\"\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Results saved to: {results_file}\")\n",
        "\n",
        "# Create CSV for easy analysis\n",
        "df_results = pd.DataFrame([\n",
        "    {\n",
        "        'Fold': result['fold'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1'],\n",
        "        'Accuracy': result['accuracy']\n",
        "    }\n",
        "    for result in fold_results\n",
        "])\n",
        "\n",
        "# Add summary row\n",
        "summary_row = {\n",
        "    'Fold': 'Mean ¬± Std',\n",
        "    'Precision': f\"{np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\",\n",
        "    'Recall': f\"{np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\",\n",
        "    'F1-Score': f\"{np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\",\n",
        "    'Accuracy': f\"{np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\"\n",
        "}\n",
        "\n",
        "df_results = pd.concat([df_results, pd.DataFrame([summary_row])], ignore_index=True)\n",
        "\n",
        "csv_file = f\"{OUTPUT_DIR}/5fold_cv_results.csv\"\n",
        "df_results.to_csv(csv_file, index=False)\n",
        "print(f\"‚úÖ Results CSV saved to: {csv_file}\")\n",
        "\n",
        "# Display final summary table\n",
        "print(f\"\\nüìä FINAL RESULTS TABLE:\")\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## 12. Conclusion\n",
        "\n",
        "This notebook successfully implemented 5-fold cross-validation for the Serbian Legal NER pipeline using XLM-R-BERTiƒá model.\n",
        "\n",
        "### Key Achievements:\n",
        "- ‚úÖ **Multilingual Model**: XLM-R-BERTiƒá combines multilingual capabilities with Serbian specialization\n",
        "- ‚úÖ **Robust Evaluation**: 5-fold cross-validation provides reliable performance estimates\n",
        "- ‚úÖ **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracked across all folds\n",
        "- ‚úÖ **Statistical Analysis**: Mean and standard deviation calculated for all metrics\n",
        "- ‚úÖ **Visualization**: Clear charts showing performance across folds\n",
        "- ‚úÖ **Results Persistence**: JSON and CSV files saved for further analysis\n",
        "\n",
        "### XLM-R-BERTiƒá Advantages:\n",
        "- **Multilingual Foundation**: Benefits from XLM-RoBERTa's strong multilingual pretraining\n",
        "- **Serbian Specialization**: Fine-tuned specifically for Serbian language tasks\n",
        "- **Better Generalization**: Potential for improved performance on diverse Serbian legal texts\n",
        "- **Cross-lingual Transfer**: Can leverage knowledge from multiple languages\n",
        "\n",
        "### Next Steps:\n",
        "1. **Compare with Other Models**: Analyze performance differences with base BERT, BERT-CRF, and class weights approaches\n",
        "2. **Error Analysis**: Examine misclassified entities to identify improvement opportunities\n",
        "3. **Hyperparameter Tuning**: Optimize learning rate, batch size, and other parameters\n",
        "4. **Ensemble Methods**: Combine predictions from multiple folds for better performance\n",
        "\n",
        "The 5-fold cross-validation framework successfully evaluated XLM-R-BERTiƒá for Serbian Legal NER!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
