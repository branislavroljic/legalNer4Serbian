{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCxJ3lfIjTOm"
   },
   "source": [
    "# Serbian Legal Named Entity Recognition (NER) Pipeline - Class Weights 5-Fold Cross-Validation\n",
    "\n",
    "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using class weights to handle class imbalance.\n",
    "Class weights help address the imbalanced distribution of entity types in the dataset by giving more importance to underrepresented classes.\n",
    "\n",
    "## Key Features\n",
    "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
    "- **Class Weights**: Automatic calculation and application of class weights\n",
    "- **Imbalance Handling**: Better performance on minority entity classes\n",
    "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
    "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
    "- **Statistical Analysis**: Mean and standard deviation across folds\n",
    "\n",
    "## Class Weights Advantages\n",
    "- **Balanced Learning**: Addresses class imbalance in entity distribution\n",
    "- **Improved Minority Class Performance**: Better recognition of rare entity types\n",
    "- **Automatic Calculation**: Weights computed based on class frequencies\n",
    "- **No Architecture Changes**: Uses standard BERT with weighted loss\n",
    "\n",
    "## Entity Types\n",
    "- **COURT**: Court institutions\n",
    "- **DECISION_DATE**: Dates of legal decisions\n",
    "- **CASE_NUMBER**: Case identifiers\n",
    "- **CRIMINAL_ACT**: Criminal acts/charges\n",
    "- **PROSECUTOR**: Prosecutor entities\n",
    "- **DEFENDANT**: Defendant entities\n",
    "- **JUDGE**: Judge names\n",
    "- **REGISTRAR**: Court registrar\n",
    "- **SANCTION**: Sanctions/penalties\n",
    "- **SANCTION_TYPE**: Type of sanction\n",
    "- **SANCTION_VALUE**: Value/duration of sanction\n",
    "- **PROVISION**: Legal provisions\n",
    "- **PROCEDURE_COSTS**: Legal procedure costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxHG6Rs8jTOo"
   },
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:10:22.242272Z",
     "iopub.status.busy": "2025-10-04T22:10:22.241789Z",
     "iopub.status.idle": "2025-10-04T22:10:25.019511Z",
     "shell.execute_reply": "2025-10-04T22:10:25.018953Z",
     "shell.execute_reply.started": "2025-10-04T22:10:22.242253Z"
    },
    "id": "11kIuCNPjTOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:20:57.895447Z",
     "iopub.status.busy": "2025-10-04T22:20:57.895218Z",
     "iopub.status.idle": "2025-10-04T22:20:57.904815Z",
     "shell.execute_reply": "2025-10-04T22:20:57.904193Z",
     "shell.execute_reply.started": "2025-10-04T22:20:57.895431Z"
    },
    "id": "j3ig3oNUjTOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setup complete:\n",
      "  PyTorch version: 2.1.1+cu121\n",
      "  CUDA available: True\n",
      "  CUDA device: NVIDIA RTX A4000\n",
      "  Device: cuda\n",
      "  Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Import shared modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/shared/')\n",
    "\n",
    "import importlib\n",
    "import shared\n",
    "import shared.model_utils\n",
    "import shared.data_processing\n",
    "import shared.dataset\n",
    "import shared.evaluation\n",
    "import shared.config\n",
    "importlib.reload(shared.config)\n",
    "importlib.reload(shared.data_processing)\n",
    "importlib.reload(shared.dataset)\n",
    "importlib.reload(shared.model_utils)\n",
    "importlib.reload(shared.evaluation)\n",
    "importlib.reload(shared)\n",
    "\n",
    "# Import from shared modules\n",
    "from shared import (\n",
    "    # Configuration\n",
    "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
    "    get_default_model_config, get_paths, setup_environment,\n",
    "\n",
    "    # Data processing\n",
    "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
    "    analyze_labelstudio_data, validate_bio_examples,\n",
    "\n",
    "    # Dataset\n",
    "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
    "    print_sequence_analysis, create_huggingface_datasets,\n",
    "\n",
    "    # Model utilities\n",
    "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
    "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
    "    PerClassMetricsCallback, compute_metrics,\n",
    "\n",
    "    # Evaluation\n",
    "    generate_evaluation_report, plot_training_history, plot_entity_distribution,\n",
    "    # Comprehensive tracking\n",
    "    analyze_entity_distribution_per_fold,\n",
    "    generate_detailed_classification_report,\n",
    "    # Aggregate functions across all folds\n",
    "    create_aggregate_report_across_folds\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DataCollatorForTokenClassification, AutoTokenizer, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Setup device and random seed\n",
    "device = setup_device_and_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "class_weights_imports"
   },
   "source": [
    "## 2. Class Weights Specific Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:20:59.494580Z",
     "iopub.status.busy": "2025-10-04T22:20:59.493786Z",
     "iopub.status.idle": "2025-10-04T22:20:59.498151Z",
     "shell.execute_reply": "2025-10-04T22:20:59.497443Z",
     "shell.execute_reply.started": "2025-10-04T22:20:59.494560Z"
    },
    "id": "class_weights_imports_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class weights specific imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Class weights specific imports\n",
    "from collections import Counter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "print(\"‚úÖ Class weights specific imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R56QtmR7fIY2"
   },
   "source": [
    "## 3. Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:00.918630Z",
     "iopub.status.busy": "2025-10-04T22:21:00.918009Z",
     "iopub.status.idle": "2025-10-04T22:21:00.955117Z",
     "shell.execute_reply": "2025-10-04T22:21:00.954602Z",
     "shell.execute_reply.started": "2025-10-04T22:21:00.918623Z"
    },
    "id": "9i3CBXt2fIY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment setup (cloud):\n",
      "  ‚úÖ labelstudio_json: /datasets/annotations/annotations.json\n",
      "  ‚úÖ judgments_dir: /datasets/judgments\n",
      "  ‚úÖ labelstudio_files_dir: /datasets/judgments\n",
      "  ‚ùå mlm_data_dir: /datasets/dapt-mlm\n",
      "  ‚úÖ models_dir: /storage/models\n",
      "  ‚úÖ logs_dir: /storage/logs\n",
      "  ‚úÖ results_dir: /storage/results\n",
      "üîß Configuration:\n",
      "  Model: classla/bcms-bertic\n",
      "  Architecture: BERT + Class Weights\n",
      "  Output directory: /storage/models/bertic_class_weights_5fold_cv\n",
      "  Entity types: 16\n",
      "  BIO labels: 33\n"
     ]
    }
   ],
   "source": [
    "# Setup environment and paths\n",
    "env_setup = setup_environment(use_local=False, create_dirs=True)\n",
    "paths = env_setup['paths']\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"classla/bcms-bertic\"\n",
    "model_config = get_default_model_config()\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_class_weights_5fold_cv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Architecture: BERT + Class Weights\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVSwtVS1fIY3"
   },
   "source": [
    "## 4. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:02.956773Z",
     "iopub.status.busy": "2025-10-04T22:21:02.956242Z",
     "iopub.status.idle": "2025-10-04T22:21:03.016873Z",
     "shell.execute_reply": "2025-10-04T22:21:03.016487Z",
     "shell.execute_reply.started": "2025-10-04T22:21:02.956752Z"
    },
    "id": "Vmq8DjhyfIY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 225 annotated documents from /datasets/annotations/annotations.json\n",
      "üìä Analysis Results:\n",
      "Total documents: 225\n",
      "Total annotations: 225\n",
      "Unique entity types: 14\n",
      "\n",
      "Entity distribution:\n",
      "  DEFENDANT: 1240\n",
      "  PROVISION_MATERIAL: 1177\n",
      "  CRIMINAL_ACT: 792\n",
      "  PROVISION_PROCEDURAL: 686\n",
      "  REGISTRAR: 460\n",
      "  COURT: 458\n",
      "  JUDGE: 451\n",
      "  PROSECUTOR: 395\n",
      "  DECISION_DATE: 359\n",
      "  SANCTION_TYPE: 248\n",
      "  SANCTION_VALUE: 241\n",
      "  VERDICT: 238\n",
      "  PROCEDURE_COSTS: 231\n",
      "  CASE_NUMBER: 225\n"
     ]
    }
   ],
   "source": [
    "# Load LabelStudio data\n",
    "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
    "\n",
    "# Analyze the data\n",
    "if labelstudio_data:\n",
    "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
    "else:\n",
    "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAyV4gQfIY4"
   },
   "source": [
    "## 5. Data Preprocessing and BIO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:04.821996Z",
     "iopub.status.busy": "2025-10-04T22:21:04.821577Z",
     "iopub.status.idle": "2025-10-04T22:21:05.579373Z",
     "shell.execute_reply": "2025-10-04T22:21:05.578884Z",
     "shell.execute_reply.started": "2025-10-04T22:21:04.821973Z"
    },
    "id": "rOe5xEDnfIY5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converted 225 examples to BIO format\n",
      "üìä BIO Validation Results:\n",
      "Total examples: 225\n",
      "Valid examples: 225\n",
      "Invalid examples: 0\n",
      "Empty examples: 0\n",
      "üìä Validation complete: 225 valid examples\n"
     ]
    }
   ],
   "source": [
    "# Convert LabelStudio data to BIO format\n",
    "converter = LabelStudioToBIOConverter(\n",
    "    judgments_dir=paths['judgments_dir'],\n",
    "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
    ")\n",
    "\n",
    "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
    "print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
    "\n",
    "# Validate BIO examples\n",
    "valid_examples, stats = validate_bio_examples(bio_examples)\n",
    "print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HcyAbInfIY5"
   },
   "source": [
    "## 6. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:07.053966Z",
     "iopub.status.busy": "2025-10-04T22:21:07.053729Z",
     "iopub.status.idle": "2025-10-04T22:21:07.075794Z",
     "shell.execute_reply": "2025-10-04T22:21:07.075095Z",
     "shell.execute_reply.started": "2025-10-04T22:21:07.053948Z"
    },
    "id": "3cX7CQA3fIY5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset statistics:\n",
      "  Number of unique labels: 29\n",
      "  Prepared examples: 225\n",
      "  Total tokens: 232475\n",
      "  Entity types found: 14\n"
     ]
    }
   ],
   "source": [
    "# Create NER dataset\n",
    "ner_dataset = NERDataset(valid_examples)\n",
    "prepared_examples = ner_dataset.prepare_for_training()\n",
    "\n",
    "print(f\"üìä Dataset statistics:\")\n",
    "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
    "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
    "\n",
    "# Get label statistics\n",
    "label_stats = ner_dataset.get_label_statistics()\n",
    "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
    "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "class_weights_calculation"
   },
   "source": [
    "## 7. Class Weights Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:09.838659Z",
     "iopub.status.busy": "2025-10-04T22:21:09.838444Z",
     "iopub.status.idle": "2025-10-04T22:21:09.843268Z",
     "shell.execute_reply": "2025-10-04T22:21:09.842702Z",
     "shell.execute_reply.started": "2025-10-04T22:21:09.838643Z"
    },
    "id": "class_weights_calc_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class weights calculation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(examples, label_to_id):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on label frequency in the training data.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of tokenized training examples (with integer label IDs)\n",
    "        label_to_id: Dictionary mapping labels to IDs\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Class weights tensor\n",
    "    \"\"\"\n",
    "    # Collect all label IDs from training examples, filtering out -100 (ignore index)\n",
    "    all_label_ids = []\n",
    "    for example in examples:\n",
    "        # Filter out -100 values (used for padding/subword tokens)\n",
    "        valid_labels = [label for label in example['labels'] if label != -100]\n",
    "        all_label_ids.extend(valid_labels)\n",
    "    \n",
    "    # Count label frequencies\n",
    "    label_counts = Counter(all_label_ids)\n",
    "    \n",
    "    # Get unique classes (as integers)\n",
    "    classes = np.array(list(range(len(label_to_id))))\n",
    "    \n",
    "    # Calculate class weights using sklearn's balanced approach\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=np.array(all_label_ids)\n",
    "    )\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "    \n",
    "    print(f\"üìä Class weights calculated:\")\n",
    "    print(f\"  Number of classes: {len(classes)}\")\n",
    "    print(f\"  Total valid labels: {len(all_label_ids)}\")\n",
    "    print(f\"  Weight range: {class_weights.min():.4f} - {class_weights.max():.4f}\")\n",
    "    print(f\"  Mean weight: {class_weights.mean():.4f}\")\n",
    "    \n",
    "    return class_weights_tensor\n",
    "\n",
    "print(\"‚úÖ Class weights calculation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weighted_trainer"
   },
   "source": [
    "## 8. Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:12.707054Z",
     "iopub.status.busy": "2025-10-04T22:21:12.706459Z",
     "iopub.status.idle": "2025-10-04T22:21:12.711978Z",
     "shell.execute_reply": "2025-10-04T22:21:12.711575Z",
     "shell.execute_reply.started": "2025-10-04T22:21:12.707035Z"
    },
    "id": "weighted_trainer_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weighted Trainer class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that uses weighted CrossEntropyLoss for handling class imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute weighted loss for token classification.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Move class weights to the same device as logits\n",
    "            if self.class_weights is not None:\n",
    "                class_weights = self.class_weights.to(logits.device)\n",
    "            else:\n",
    "                class_weights = None\n",
    "            \n",
    "            # Create weighted loss function\n",
    "            loss_fct = CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "            \n",
    "            # Flatten for loss calculation\n",
    "            active_loss = labels.view(-1) != -100\n",
    "            active_logits = logits.view(-1, logits.shape[-1])\n",
    "            active_labels = torch.where(\n",
    "                active_loss,\n",
    "                labels.view(-1),\n",
    "                torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "            \n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"‚úÖ Weighted Trainer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEtKaFXofIY5"
   },
   "source": [
    "## 9. K-Fold Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:14.678230Z",
     "iopub.status.busy": "2025-10-04T22:21:14.677680Z",
     "iopub.status.idle": "2025-10-04T22:21:14.753560Z",
     "shell.execute_reply": "2025-10-04T22:21:14.753204Z",
     "shell.execute_reply.started": "2025-10-04T22:21:14.678209Z"
    },
    "id": "iOZoMYBNfIY6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 5-fold cross-validation\n",
      "Total examples: 225\n",
      "Examples per fold (approx): 45\n",
      "\n",
      "Loaded tokenizer for classla/bcms-bertic\n",
      "Tokenizer vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Set up 5-fold cross-validation\n",
    "N_FOLDS = 5\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to numpy array for easier indexing\n",
    "examples_array = np.array(prepared_examples, dtype=object)\n",
    "\n",
    "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
    "print(f\"Total examples: {len(prepared_examples)}\")\n",
    "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
    "\n",
    "# Load tokenizer (will be used across all folds)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Store results from all folds\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_helper_functions"
   },
   "source": [
    "## 10. K-Fold Cross-Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:21:16.430193Z",
     "iopub.status.busy": "2025-10-04T22:21:16.429464Z",
     "iopub.status.idle": "2025-10-04T22:21:16.434235Z",
     "shell.execute_reply": "2025-10-04T22:21:16.433811Z",
     "shell.execute_reply.started": "2025-10-04T22:21:16.430175Z"
    },
    "id": "class_weights_helper_functions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class weights data preparation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLASS WEIGHTS K-FOLD CROSS-VALIDATION HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_fold_data_with_weights(train_examples, val_examples, tokenizer, ner_dataset):\n",
    "    \"\"\"\n",
    "    Prepare training and validation datasets for a specific fold, including class weights calculation.\n",
    "    \n",
    "    Args:\n",
    "        train_examples: Training examples for this fold\n",
    "        val_examples: Validation examples for this fold\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, data_collator, class_weights)\n",
    "    \"\"\"\n",
    "    # Tokenize datasets with sliding window\n",
    "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "    \n",
    "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights from training data\n",
    "    class_weights = calculate_class_weights(train_tokenized, ner_dataset.label_to_id)\n",
    "    \n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
    "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, data_collator, class_weights\n",
    "\n",
    "print(\"‚úÖ Class weights data preparation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:22:48.186324Z",
     "iopub.status.busy": "2025-10-04T22:22:48.185746Z",
     "iopub.status.idle": "2025-10-04T22:22:48.192985Z",
     "shell.execute_reply": "2025-10-04T22:22:48.192548Z",
     "shell.execute_reply.started": "2025-10-04T22:22:48.186297Z"
    },
    "id": "class_weights_model_creation"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class weights model and trainer creation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_class_weights_model_and_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device):\n",
    "    \"\"\"\n",
    "    Create model and weighted trainer for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_num: Current fold number\n",
    "        train_dataset: Training dataset for this fold\n",
    "        val_dataset: Validation dataset for this fold\n",
    "        data_collator: Data collator\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "        class_weights: Class weights tensor\n",
    "        device: Device to use (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, trainer, fold_output_dir)\n",
    "    \"\"\"\n",
    "    # Create fold-specific output directory\n",
    "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
    "    import os\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load fresh model for this fold\n",
    "    model, _ = load_model_and_tokenizer(\n",
    "        MODEL_NAME,\n",
    "        ner_dataset.get_num_labels(),\n",
    "        ner_dataset.id_to_label,\n",
    "        ner_dataset.label_to_id\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create training arguments for this fold\n",
    "    training_args = create_training_arguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        num_epochs=model_config['num_epochs'],\n",
    "        batch_size=model_config['batch_size'],\n",
    "        learning_rate=model_config['learning_rate'],\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_steps=100,\n",
    "        save_steps=500,\n",
    "        early_stopping_patience=3\n",
    "    )\n",
    "    \n",
    "    # Create metrics callback for comprehensive tracking\n",
    "    metrics_callback = PerClassMetricsCallback(id_to_label=ner_dataset.id_to_label)\n",
    "    \n",
    "    # Create compute_metrics function with id_to_label bound\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        return compute_metrics(eval_pred, ner_dataset.id_to_label)\n",
    "    \n",
    "    # Create weighted trainer with metrics callback and compute_metrics\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "    print(f\"Class Weights Trainer initialized for fold {fold_num} with comprehensive metrics tracking\")\n",
    "    print(f\"  Class weights shape: {class_weights.shape}\")\n",
    "    print(f\"  Weight range: {class_weights.min():.4f} - {class_weights.max():.4f}\")\n",
    "    \n",
    "    return model, trainer, metrics_callback, fold_output_dir\n",
    "\n",
    "print(\"‚úÖ Class weights model and trainer creation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_main_training"
   },
   "source": [
    "## 11. K-Fold Cross-Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T22:22:52.677612Z",
     "iopub.status.busy": "2025-10-04T22:22:52.677097Z",
     "iopub.status.idle": "2025-10-04T22:26:54.753447Z",
     "shell.execute_reply": "2025-10-04T22:26:54.752370Z",
     "shell.execute_reply.started": "2025-10-04T22:22:52.677593Z"
    },
    "id": "k_fold_main_loop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "STARTING 5-FOLD CROSS-VALIDATION - CLASS WEIGHTS\n",
      "================================================================================\n",
      "Total examples: 225\n",
      "Model: classla/bcms-bertic + Class Weights\n",
      "Device: cuda\n",
      "\n",
      "================================================================================\n",
      "CLASS WEIGHTS FOLD 1/5\n",
      "================================================================================\n",
      "Train indices: 180, Val indices: 45\n",
      "Training examples: 180\n",
      "Validation examples: 45\n",
      "\n",
      "üìä Analyzing entity distribution for fold 1...\n",
      "\n",
      "üìä Entity Distribution - Fold 1 - Training\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   7320          35.29%\n",
      "PROVISION_PROCEDURAL                 3210          15.48%\n",
      "CRIMINAL_ACT                         2252          10.86%\n",
      "COURT                                1641           7.91%\n",
      "DEFENDANT                            1445           6.97%\n",
      "SANCTION_VALUE                        956           4.61%\n",
      "JUDGE                                 734           3.54%\n",
      "REGISTRAR                             731           3.52%\n",
      "VERDICT                               573           2.76%\n",
      "PROSECUTOR                            510           2.46%\n",
      "SANCTION_TYPE                         448           2.16%\n",
      "PROCEDURE_COSTS                       391           1.89%\n",
      "DECISION_DATE                         348           1.68%\n",
      "CASE_NUMBER                           181           0.87%\n",
      "------------------------------------------------------------\n",
      "TOTAL                               20740         100.00%\n",
      "\n",
      "üìä Entity Distribution - Fold 1 - Validation\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   2284          37.58%\n",
      "PROVISION_PROCEDURAL                  958          15.76%\n",
      "CRIMINAL_ACT                          695          11.44%\n",
      "DEFENDANT                             458           7.54%\n",
      "COURT                                 405           6.66%\n",
      "SANCTION_VALUE                        242           3.98%\n",
      "REGISTRAR                             183           3.01%\n",
      "JUDGE                                 176           2.90%\n",
      "PROSECUTOR                            155           2.55%\n",
      "SANCTION_TYPE                         155           2.55%\n",
      "VERDICT                               148           2.44%\n",
      "DECISION_DATE                          87           1.43%\n",
      "PROCEDURE_COSTS                        86           1.42%\n",
      "CASE_NUMBER                            45           0.74%\n",
      "------------------------------------------------------------\n",
      "TOTAL                                6077         100.00%\n",
      "\n",
      "üî§ Preparing data for class weights fold 1...\n",
      "üìä Class weights calculated:\n",
      "  Number of classes: 29\n",
      "  Total valid labels: 597099\n",
      "  Weight range: 0.0380 - 6863.2069\n",
      "  Mean weight: 269.0455\n",
      "üì¶ Created HuggingFace datasets:\n",
      "  Training: 1845 examples\n",
      "  Validation: 500 examples\n",
      "  Test: 500 examples\n",
      "üì¶ Class Weights Fold 1 datasets:\n",
      "  Training: 1845 examples\n",
      "  Validation: 500 examples\n",
      "\n",
      "ü§ñ Creating class weights model and trainer for fold 1...\n",
      "üîÑ Loading model and tokenizer...\n",
      "üì• Model: classla/bcms-bertic\n",
      "üè∑Ô∏è  Number of labels: 29\n",
      "‚úÖ Loaded tokenizer (vocab size: 32000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model (parameters: 110,049,053)\n",
      "üñ•Ô∏è  Device: cuda\n",
      "‚öôÔ∏è  Training configuration:\n",
      "  Epochs: 8\n",
      "  Batch size: 8\n",
      "  Learning rate: 3e-05\n",
      "  Warmup steps: 500\n",
      "  Weight decay: 0.01\n",
      "  Early stopping patience: 3\n",
      "Class Weights Trainer initialized for fold 1 with comprehensive metrics tracking\n",
      "  Class weights shape: torch.Size([29])\n",
      "  Weight range: 0.0380 - 6863.2070\n",
      "\n",
      "üèãÔ∏è  Training class weights fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpericapero1\u001b[0m (\u001b[33mpericapero1-faculty-of-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20251004_222301-geqgsw78</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pericapero1-faculty-of-/huggingface/runs/geqgsw78' target=\"_blank\">serene-valley-42</a></strong> to <a href='https://wandb.ai/pericapero1-faculty-of-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pericapero1-faculty-of-/huggingface' target=\"_blank\">https://wandb.ai/pericapero1-faculty-of-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pericapero1-faculty-of-/huggingface/runs/geqgsw78' target=\"_blank\">https://wandb.ai/pericapero1-faculty-of-/huggingface/runs/geqgsw78</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1848 03:43 < 10:02, 2.24 it/s, Epoch 2.16/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.232100</td>\n",
       "      <td>3.052753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.392700</td>\n",
       "      <td>1.948806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.299100</td>\n",
       "      <td>0.942161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.468095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.368171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_f1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train and evaluate this fold\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müèãÔ∏è  Training class weights fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ Saving class weights model for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2407\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metric_to_check\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2406\u001b[0m     metric_to_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2407\u001b[0m metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2409\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m operator(metric_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric)\n\u001b[1;32m   2414\u001b[0m ):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_f1'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcc5cafda10>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc828f8f10, execution_count=40 error_before_exec=None error_in_exec='eval_f1' info=<ExecutionInfo object at 7fcc810ebcd0, raw_cell=\"# Check device availability\n",
      "device = torch.device(..\" store_history=True silent=False shell_futures=True cell_id=536a7a8d-badc-411d-819c-510b0036fcf0> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Main K-Fold Cross-Validation Loop for Class Weights\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION - CLASS WEIGHTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total examples: {len(examples_array)}\")\n",
    "print(f\"Model: {MODEL_NAME} + Class Weights\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Execute K-Fold training\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLASS WEIGHTS FOLD {fold_num}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
    "    \n",
    "    # Get fold data\n",
    "    train_examples = examples_array[train_idx].tolist()\n",
    "    val_examples = examples_array[val_idx].tolist()\n",
    "    \n",
    "    print(f\"Training examples: {len(train_examples)}\")\n",
    "    print(f\"Validation examples: {len(val_examples)}\")\n",
    "    \n",
    "    # Analyze entity distribution for this fold\n",
    "    print(f\"\\nüìä Analyzing entity distribution for fold {fold_num}...\")\n",
    "    train_dist = analyze_entity_distribution_per_fold(train_examples, f\"Fold {fold_num} - Training\")\n",
    "    val_dist = analyze_entity_distribution_per_fold(val_examples, f\"Fold {fold_num} - Validation\")\n",
    "    \n",
    "    # Prepare data for this fold (including class weights calculation)\n",
    "    print(f\"\\nüî§ Preparing data for class weights fold {fold_num}...\")\n",
    "    train_dataset, val_dataset, data_collator, class_weights = prepare_fold_data_with_weights(\n",
    "        train_examples, val_examples, tokenizer, ner_dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"üì¶ Class Weights Fold {fold_num} datasets:\")\n",
    "    print(f\"  Training: {len(train_dataset)} examples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "    \n",
    "    # Create model and weighted trainer for this fold\n",
    "    print(f\"\\nü§ñ Creating class weights model and trainer for fold {fold_num}...\")\n",
    "    model, trainer, metrics_callback, fold_output_dir = create_class_weights_model_and_trainer(\n",
    "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate this fold\n",
    "    print(f\"\\nüèãÔ∏è  Training class weights fold {fold_num}...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"üíæ Saving class weights model for fold {fold_num}...\")\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"üìä Evaluating class weights fold {fold_num}...\")\n",
    "    eval_results = detailed_evaluation(\n",
    "        trainer, val_dataset, f\"Class Weights Fold {fold_num} Validation\", ner_dataset.id_to_label\n",
    "    )\n",
    "    \n",
    "    # Get predictions for confusion matrix and detailed analysis\n",
    "    true_labels = eval_results['true_labels']\n",
    "    pred_labels = eval_results['true_predictions']\n",
    "    \n",
    "    # Flatten for confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    flat_true = [label for seq in true_labels for label in seq]\n",
    "    flat_pred = [label for seq in pred_labels for label in seq]\n",
    "    all_labels = sorted(list(set(flat_true + flat_pred)))\n",
    "    cm = confusion_matrix(flat_true, flat_pred, labels=all_labels)\n",
    "    \n",
    "    # Generate classification report for this fold\n",
    "    per_class_metrics = generate_detailed_classification_report(\n",
    "        true_labels, pred_labels, fold_output_dir, fold_num, \"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    fold_result = {\n",
    "        'fold': fold_num,\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f1': eval_results['f1'],\n",
    "        'accuracy': eval_results['accuracy'],\n",
    "        'per_class_metrics': per_class_metrics,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': all_labels,\n",
    "        'distributions': {'train': train_dist, 'val': val_dist},\n",
    "        'training_history': metrics_callback.get_training_history()\n",
    "    }\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model, trainer, metrics_callback, train_dataset, val_dataset\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Class Weights Fold {fold_num} completed!\")\n",
    "    print(f\"   Precision: {fold_result['precision']:.4f}\")\n",
    "    print(f\"   Recall: {fold_result['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {fold_result['f1']:.4f}\")\n",
    "    print(f\"   Accuracy: {fold_result['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CLASS WEIGHTS K-FOLD CROSS-VALIDATION COMPLETED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aggregate_results"
   },
   "source": [
    "## 12. Aggregate Results Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T21:59:25.832701Z",
     "iopub.status.idle": "2025-10-04T21:59:25.832842Z",
     "shell.execute_reply": "2025-10-04T21:59:25.832817Z",
     "shell.execute_reply.started": "2025-10-04T21:59:25.832808Z"
    },
    "id": "aggregate_results_code"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS ACROSS FOLDS WITH COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Create comprehensive aggregate report with all visualizations\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GENERATING COMPREHENSIVE AGGREGATE REPORT FOR CLASS WEIGHTS MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "aggregate_report = create_aggregate_report_across_folds(\n",
    "    fold_results=fold_results,\n",
    "    model_name=\"BERTiƒá with Class Weights (classla/bcms-bertic)\",\n",
    "    display=True\n",
    ")\n",
    "\n",
    "# Extract summary metrics\n",
    "precisions = [result['precision'] for result in fold_results]\n",
    "recalls = [result['recall'] for result in fold_results]\n",
    "f1_scores = [result['f1'] for result in fold_results]\n",
    "accuracies = [result['accuracy'] for result in fold_results]\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CLASS WEIGHTS FINAL RESULTS ACROSS {N_FOLDS} FOLDS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall:    {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\")\n",
    "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
    "print(f\"Accuracy:  {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 13. Visualization of K-Fold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T21:59:25.833595Z",
     "iopub.status.idle": "2025-10-04T21:59:25.833754Z",
     "shell.execute_reply": "2025-10-04T21:59:25.833662Z",
     "shell.execute_reply.started": "2025-10-04T21:59:25.833662Z"
    },
    "id": "visualization_code"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION OF K-FOLD RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization of fold results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'{N_FOLDS}-Fold Cross-Validation Results - Class Weights Model', fontsize=16, fontweight='bold')\n",
    "\n",
    "fold_numbers = list(range(1, N_FOLDS + 1))\n",
    "\n",
    "# Plot precision\n",
    "ax1.plot(fold_numbers, precisions, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "ax1.axhline(y=np.mean(precisions), color='r', linestyle='--', label=f'Mean: {np.mean(precisions):.4f}')\n",
    "ax1.set_xlabel('Fold Number', fontsize=12)\n",
    "ax1.set_ylabel('Precision', fontsize=12)\n",
    "ax1.set_title('Precision Across Folds', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_xticks(fold_numbers)\n",
    "\n",
    "# Plot recall\n",
    "ax2.plot(fold_numbers, recalls, marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
    "ax2.axhline(y=np.mean(recalls), color='r', linestyle='--', label=f'Mean: {np.mean(recalls):.4f}')\n",
    "ax2.set_xlabel('Fold Number', fontsize=12)\n",
    "ax2.set_ylabel('Recall', fontsize=12)\n",
    "ax2.set_title('Recall Across Folds', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_xticks(fold_numbers)\n",
    "\n",
    "# Plot F1-score\n",
    "ax3.plot(fold_numbers, f1_scores, marker='^', linewidth=2, markersize=8, color='#F18F01')\n",
    "ax3.axhline(y=np.mean(f1_scores), color='r', linestyle='--', label=f'Mean: {np.mean(f1_scores):.4f}')\n",
    "ax3.set_xlabel('Fold Number', fontsize=12)\n",
    "ax3.set_ylabel('F1-Score', fontsize=12)\n",
    "ax3.set_title('F1-Score Across Folds', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "ax3.set_xticks(fold_numbers)\n",
    "\n",
    "# Plot accuracy\n",
    "ax4.plot(fold_numbers, accuracies, marker='D', linewidth=2, markersize=8, color='#6A994E')\n",
    "ax4.axhline(y=np.mean(accuracies), color='r', linestyle='--', label=f'Mean: {np.mean(accuracies):.4f}')\n",
    "ax4.set_xlabel('Fold Number', fontsize=12)\n",
    "ax4.set_ylabel('Accuracy', fontsize=12)\n",
    "ax4.set_title('Accuracy Across Folds', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "ax4.set_xticks(fold_numbers)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/class_weights_5fold_cv_results.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to: {OUTPUT_DIR}/class_weights_5fold_cv_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 14. Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T21:59:25.834577Z",
     "iopub.status.idle": "2025-10-04T21:59:25.834784Z",
     "shell.execute_reply": "2025-10-04T21:59:25.834681Z",
     "shell.execute_reply.started": "2025-10-04T21:59:25.834681Z"
    },
    "id": "save_results_code"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS TO FILE\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    'experiment_info': {\n",
    "        'model': MODEL_NAME,\n",
    "        'architecture': 'BERT + Class Weights',\n",
    "        'n_folds': N_FOLDS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'random_seed': 42\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'precision_mean': float(np.mean(precisions)),\n",
    "        'precision_std': float(np.std(precisions)),\n",
    "        'recall_mean': float(np.mean(recalls)),\n",
    "        'recall_std': float(np.std(recalls)),\n",
    "        'f1_mean': float(np.mean(f1_scores)),\n",
    "        'f1_std': float(np.std(f1_scores)),\n",
    "        'accuracy_mean': float(np.mean(accuracies)),\n",
    "        'accuracy_std': float(np.std(accuracies))\n",
    "    },\n",
    "    'fold_results': [\n",
    "        {\n",
    "            'fold': result['fold'],\n",
    "            'precision': float(result['precision']),\n",
    "            'recall': float(result['recall']),\n",
    "            'f1': float(result['f1']),\n",
    "            'accuracy': float(result['accuracy'])\n",
    "        }\n",
    "        for result in fold_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = f\"{OUTPUT_DIR}/5fold_cv_results.json\"\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_file}\")\n",
    "\n",
    "# Create CSV for easy analysis\n",
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        'Fold': result['fold'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1'],\n",
    "        'Accuracy': result['accuracy']\n",
    "    }\n",
    "    for result in fold_results\n",
    "])\n",
    "\n",
    "# Add summary row\n",
    "summary_row = {\n",
    "    'Fold': 'Mean ¬± Std',\n",
    "    'Precision': f\"{np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\",\n",
    "    'Recall': f\"{np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\",\n",
    "    'F1-Score': f\"{np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\",\n",
    "    'Accuracy': f\"{np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\"\n",
    "}\n",
    "\n",
    "df_results = pd.concat([df_results, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "csv_file = f\"{OUTPUT_DIR}/5fold_cv_results.csv\"\n",
    "df_results.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Results CSV saved to: {csv_file}\")\n",
    "\n",
    "# Display final summary table\n",
    "print(f\"\\nüìä FINAL RESULTS TABLE:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "This notebook successfully implemented 5-fold cross-validation for the Serbian Legal NER pipeline using class weights to handle class imbalance.\n",
    "\n",
    "### Key Achievements:\n",
    "- ‚úÖ **Class Imbalance Handling**: Automatic calculation and application of class weights\n",
    "- ‚úÖ **Robust Evaluation**: 5-fold cross-validation provides reliable performance estimates\n",
    "- ‚úÖ **Improved Minority Class Performance**: Better recognition of rare entity types\n",
    "- ‚úÖ **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracked across all folds\n",
    "- ‚úÖ **Statistical Analysis**: Mean and standard deviation calculated for all metrics\n",
    "- ‚úÖ **Visualization**: Clear charts showing performance across folds\n",
    "- ‚úÖ **Results Persistence**: JSON and CSV files saved for further analysis\n",
    "\n",
    "### Class Weights Advantages:\n",
    "- **Balanced Learning**: Addresses class imbalance in entity distribution\n",
    "- **No Architecture Changes**: Uses standard BERT with weighted loss\n",
    "- **Automatic Calculation**: Weights computed based on class frequencies\n",
    "- **Better Minority Performance**: Improved recognition of underrepresented entity types\n",
    "\n",
    "### Next Steps:\n",
    "1. **Compare with Other Models**: Analyze performance differences with base BERT, BERT-CRF, and XLM-R-BERTiƒá\n",
    "2. **Error Analysis**: Examine misclassified entities, especially minority classes\n",
    "3. **Hyperparameter Tuning**: Optimize learning rate, batch size, and weight calculation method\n",
    "4. **Ensemble Methods**: Combine predictions from multiple folds for better performance\n",
    "\n",
    "The 5-fold cross-validation framework successfully evaluated class weights approach for Serbian Legal NER!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
