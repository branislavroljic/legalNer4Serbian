{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCxJ3lfIjTOm"
   },
   "source": [
    "# Serbian Legal Named Entity Recognition (NER) Pipeline - BERT-CRF 5-Fold Cross-Validation\n",
    "\n",
    "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using BERT-CRF architecture.\n",
    "BERT-CRF combines BERT embeddings with a Conditional Random Field (CRF) layer for better sequence modeling.\n",
    "\n",
    "## Key Features\n",
    "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
    "- **BERT-CRF Architecture**: BERT embeddings + CRF layer for sequence constraints\n",
    "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
    "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
    "- **Statistical Analysis**: Mean and standard deviation across folds\n",
    "\n",
    "## BERT-CRF Advantages\n",
    "- **Better Sequence Modeling**: CRF enforces valid BIO sequence constraints\n",
    "- **Improved Entity Boundaries**: More accurate entity span detection\n",
    "- **Global Optimization**: Considers entire sequence for optimal labeling\n",
    "\n",
    "## Entity Types\n",
    "- **COURT**: Court institutions\n",
    "- **DECISION_DATE**: Dates of legal decisions\n",
    "- **CASE_NUMBER**: Case identifiers\n",
    "- **CRIMINAL_ACT**: Criminal acts/charges\n",
    "- **PROSECUTOR**: Prosecutor entities\n",
    "- **DEFENDANT**: Defendant entities\n",
    "- **JUDGE**: Judge names\n",
    "- **REGISTRAR**: Court registrar\n",
    "- **SANCTION**: Sanctions/penalties\n",
    "- **SANCTION_TYPE**: Type of sanction\n",
    "- **SANCTION_VALUE**: Value/duration of sanction\n",
    "- **PROVISION**: Legal provisions\n",
    "- **PROCEDURE_COSTS**: Legal procedure costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxHG6Rs8jTOo"
   },
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:14.157781Z",
     "iopub.status.busy": "2025-10-04T20:04:14.157143Z",
     "iopub.status.idle": "2025-10-04T20:04:17.807026Z",
     "shell.execute_reply": "2025-10-04T20:04:17.805761Z",
     "shell.execute_reply.started": "2025-10-04T20:04:14.157750Z"
    },
    "id": "11kIuCNPjTOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.11/dist-packages (0.7.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages including pytorch-crf for CRF layer\n",
    "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:17.809501Z",
     "iopub.status.busy": "2025-10-04T20:04:17.808840Z",
     "iopub.status.idle": "2025-10-04T20:04:24.847296Z",
     "shell.execute_reply": "2025-10-04T20:04:24.846416Z",
     "shell.execute_reply.started": "2025-10-04T20:04:17.809472Z"
    },
    "id": "j3ig3oNUjTOo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 20:04:21.980643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-04 20:04:21.980739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-04 20:04:21.982850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-04 20:04:21.992336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 20:04:23.186929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setup complete:\n",
      "  PyTorch version: 2.1.1+cu121\n",
      "  CUDA available: True\n",
      "  CUDA device: Quadro RTX 5000\n",
      "  Device: cuda\n",
      "  Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Import shared modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/shared/')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import shared\n",
    "import shared.model_utils\n",
    "import shared.data_processing\n",
    "import shared.dataset\n",
    "import shared.evaluation\n",
    "import shared.config\n",
    "importlib.reload(shared.config)\n",
    "importlib.reload(shared.data_processing)\n",
    "importlib.reload(shared.dataset)\n",
    "importlib.reload(shared.model_utils)\n",
    "importlib.reload(shared.evaluation)\n",
    "importlib.reload(shared)\n",
    "\n",
    "# Import from shared modules\n",
    "from shared import (\n",
    "    # Configuration\n",
    "    ENTITY_TYPES, BIO_LABELS,\n",
    "    get_default_model_config, setup_environment,\n",
    "\n",
    "    # Data processing\n",
    "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
    "    analyze_labelstudio_data, validate_bio_examples,\n",
    "\n",
    "    # Dataset\n",
    "    NERDataset, tokenize_and_align_labels_with_sliding_window,\n",
    "    create_huggingface_datasets,\n",
    "\n",
    "    # Model utilities\n",
    "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
    "    detailed_evaluation, setup_device_and_seed,\n",
    "    PerClassMetricsCallback,\n",
    "\n",
    "    # Comprehensive tracking\n",
    "    analyze_entity_distribution_per_fold,\n",
    "    generate_detailed_classification_report,\n",
    "    # Aggregate functions\n",
    "    create_aggregate_report_across_folds\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from transformers import DataCollatorForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Setup device and random seed\n",
    "device = setup_device_and_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bert_crf_imports"
   },
   "source": [
    "## 2. BERT-CRF Specific Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:24.849086Z",
     "iopub.status.busy": "2025-10-04T20:04:24.848462Z",
     "iopub.status.idle": "2025-10-04T20:04:24.853979Z",
     "shell.execute_reply": "2025-10-04T20:04:24.853244Z",
     "shell.execute_reply.started": "2025-10-04T20:04:24.849007Z"
    },
    "id": "bert_crf_imports_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT-CRF specific imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# BERT-CRF specific imports\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "print(\"✅ BERT-CRF specific imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R56QtmR7fIY2"
   },
   "source": [
    "## 3. Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:24.856564Z",
     "iopub.status.busy": "2025-10-04T20:04:24.856287Z",
     "iopub.status.idle": "2025-10-04T20:04:24.903857Z",
     "shell.execute_reply": "2025-10-04T20:04:24.903092Z",
     "shell.execute_reply.started": "2025-10-04T20:04:24.856541Z"
    },
    "id": "9i3CBXt2fIY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Environment setup (cloud):\n",
      "  ✅ labelstudio_json: /datasets/annotations/annotations.json\n",
      "  ✅ judgments_dir: /datasets/judgments\n",
      "  ✅ labelstudio_files_dir: /datasets/judgments\n",
      "  ❌ mlm_data_dir: /datasets/dapt-mlm\n",
      "  ✅ models_dir: /storage/models\n",
      "  ✅ logs_dir: /storage/logs\n",
      "  ✅ results_dir: /storage/results\n",
      "🔧 Configuration:\n",
      "  Model: classla/bcms-bertic\n",
      "  Architecture: BERT (no CRF - argmax decoding)\n",
      "  USE_CRF: False\n",
      "  Output directory: /storage/models/bertic_no_crf_5fold_cv\n",
      "  Entity types: 16\n",
      "  BIO labels: 33\n"
     ]
    }
   ],
   "source": [
    "# Setup environment and paths\n",
    "env_setup = setup_environment(use_local=False, create_dirs=True)\n",
    "paths = env_setup['paths']\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"classla/bcms-bertic\"\n",
    "model_config = get_default_model_config()\n",
    "\n",
    "# ============================================================================\n",
    "# CRF CONFIGURATION - Set to False to test BERT logits alone (argmax decoding)\n",
    "# ============================================================================\n",
    "USE_CRF = False  # Set to True to enable CRF layer, False for standard BERT (argmax)\n",
    "\n",
    "# Output directory\n",
    "suffix = \"crf\" if USE_CRF else \"no_crf\"\n",
    "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_{suffix}_5fold_cv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"🔧 Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Architecture: BERT{'+ CRF' if USE_CRF else ' (no CRF - argmax decoding)'}\")\n",
    "print(f\"  USE_CRF: {USE_CRF}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVSwtVS1fIY3"
   },
   "source": [
    "## 4. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:24.904968Z",
     "iopub.status.busy": "2025-10-04T20:04:24.904749Z",
     "iopub.status.idle": "2025-10-04T20:04:25.276000Z",
     "shell.execute_reply": "2025-10-04T20:04:25.275117Z",
     "shell.execute_reply.started": "2025-10-04T20:04:24.904948Z"
    },
    "id": "Vmq8DjhyfIY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 225 annotated documents from /datasets/annotations/annotations.json\n",
      "📊 Analysis Results:\n",
      "Total documents: 225\n",
      "Total annotations: 225\n",
      "Unique entity types: 14\n",
      "\n",
      "Entity distribution:\n",
      "  DEFENDANT: 1240\n",
      "  PROVISION_MATERIAL: 1177\n",
      "  CRIMINAL_ACT: 792\n",
      "  PROVISION_PROCEDURAL: 686\n",
      "  REGISTRAR: 460\n",
      "  COURT: 458\n",
      "  JUDGE: 451\n",
      "  PROSECUTOR: 395\n",
      "  DECISION_DATE: 359\n",
      "  SANCTION_TYPE: 248\n",
      "  SANCTION_VALUE: 241\n",
      "  VERDICT: 238\n",
      "  PROCEDURE_COSTS: 231\n",
      "  CASE_NUMBER: 225\n"
     ]
    }
   ],
   "source": [
    "# Load LabelStudio data\n",
    "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
    "\n",
    "# Analyze the data\n",
    "if labelstudio_data:\n",
    "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
    "else:\n",
    "    print(\"❌ No data loaded. Please check your paths.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAyV4gQfIY4"
   },
   "source": [
    "## 5. Data Preprocessing and BIO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:25.277487Z",
     "iopub.status.busy": "2025-10-04T20:04:25.277229Z",
     "iopub.status.idle": "2025-10-04T20:04:26.123322Z",
     "shell.execute_reply": "2025-10-04T20:04:26.122466Z",
     "shell.execute_reply.started": "2025-10-04T20:04:25.277425Z"
    },
    "id": "rOe5xEDnfIY5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 225 examples to BIO format\n",
      "📊 BIO Validation Results:\n",
      "Total examples: 225\n",
      "Valid examples: 225\n",
      "Invalid examples: 0\n",
      "Empty examples: 0\n",
      "📊 Validation complete: 225 valid examples\n"
     ]
    }
   ],
   "source": [
    "# Convert LabelStudio data to BIO format\n",
    "converter = LabelStudioToBIOConverter(\n",
    "    judgments_dir=paths['judgments_dir'],\n",
    "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
    ")\n",
    "\n",
    "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
    "print(f\"✅ Converted {len(bio_examples)} examples to BIO format\")\n",
    "\n",
    "# Validate BIO examples\n",
    "valid_examples, stats = validate_bio_examples(bio_examples)\n",
    "print(f\"📊 Validation complete: {stats['valid_examples']} valid examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HcyAbInfIY5"
   },
   "source": [
    "## 6. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.124967Z",
     "iopub.status.busy": "2025-10-04T20:04:26.124348Z",
     "iopub.status.idle": "2025-10-04T20:04:26.151383Z",
     "shell.execute_reply": "2025-10-04T20:04:26.150623Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.124942Z"
    },
    "id": "3cX7CQA3fIY5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset statistics:\n",
      "  Number of unique labels: 29\n",
      "  Prepared examples: 225\n",
      "  Total tokens: 232475\n",
      "  Entity types found: 14\n"
     ]
    }
   ],
   "source": [
    "# Create NER dataset\n",
    "ner_dataset = NERDataset(valid_examples)\n",
    "prepared_examples = ner_dataset.prepare_for_training()\n",
    "\n",
    "print(f\"📊 Dataset statistics:\")\n",
    "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
    "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
    "\n",
    "# Get label statistics\n",
    "label_stats = ner_dataset.get_label_statistics()\n",
    "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
    "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bert_crf_model_definition"
   },
   "source": [
    "## 7. BERT-CRF Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.152765Z",
     "iopub.status.busy": "2025-10-04T20:04:26.152527Z",
     "iopub.status.idle": "2025-10-04T20:04:26.164197Z",
     "shell.execute_reply": "2025-10-04T20:04:26.163401Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.152742Z"
    },
    "id": "bert_crf_model_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT-CRF model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class BertCrfForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model with CRF layer for token classification.\n",
    "    Combines BERT embeddings with CRF for better sequence modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_labels, id2label, label2id, use_crf=True):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf  # Flag to enable/disable CRF\n",
    "        \n",
    "        # Load pretrained BERT model with token classification head\n",
    "        # This ensures we start with pretrained weights, not random initialization\n",
    "        self.bert_model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "        \n",
    "        # Extract config for later use\n",
    "        self.config = self.bert_model.config\n",
    "        \n",
    "        # CRF layer (optional)\n",
    "        if self.use_crf:\n",
    "            self.crf = CRF(num_labels, batch_first=True)\n",
    "        else:\n",
    "            self.crf = None\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        if self.use_crf:\n",
    "            # CRF mode: Get logits without loss, then compute CRF loss\n",
    "            outputs = self.bert_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=None,  # Don't compute loss, we'll use CRF\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                # CRF loss calculation\n",
    "                # Use attention_mask if available, otherwise create from labels\n",
    "                if attention_mask is not None:\n",
    "                    mask = attention_mask.bool()\n",
    "                else:\n",
    "                    mask = labels != -100\n",
    "                \n",
    "                # CRF requires that the first timestep must all be True\n",
    "                # Ensure first token is always unmasked\n",
    "                mask[:, 0] = True\n",
    "                \n",
    "                # Replace -100 with 0 for CRF (will be masked anyway)\n",
    "                labels_masked = labels.clone()\n",
    "                labels_masked[labels == -100] = 0\n",
    "                \n",
    "                # Calculate CRF loss\n",
    "                log_likelihood = self.crf(logits, labels_masked, mask=mask, reduction='mean')\n",
    "                loss = -log_likelihood\n",
    "            \n",
    "            return TokenClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "        else:\n",
    "            # No CRF: Use the pretrained model's built-in loss calculation\n",
    "            # This is EXACTLY how the base notebook works\n",
    "            outputs = self.bert_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,  # Let the model compute its own loss\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Return the outputs directly from the pretrained model\n",
    "            return outputs\n",
    "    \n",
    "    def predict(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Predict using CRF decoding or argmax\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get logits from pretrained model\n",
    "            outputs = self.bert_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if self.use_crf:\n",
    "                # CRF decoding\n",
    "                mask = attention_mask.bool() if attention_mask is not None else None\n",
    "                predictions = self.crf.decode(logits, mask=mask)\n",
    "                return predictions\n",
    "            else:\n",
    "                # Argmax decoding (same as base BERT model)\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                return predictions.cpu().numpy()\n",
    "\n",
    "print(\"✅ BERT-CRF model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEtKaFXofIY5"
   },
   "source": [
    "## 8. K-Fold Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.165656Z",
     "iopub.status.busy": "2025-10-04T20:04:26.165439Z",
     "iopub.status.idle": "2025-10-04T20:04:26.381862Z",
     "shell.execute_reply": "2025-10-04T20:04:26.381067Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.165637Z"
    },
    "id": "iOZoMYBNfIY6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 5-fold cross-validation\n",
      "Total examples: 225\n",
      "Examples per fold (approx): 45\n",
      "\n",
      "Loaded tokenizer for classla/bcms-bertic\n",
      "Tokenizer vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Set up 5-fold cross-validation\n",
    "N_FOLDS = 5\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to numpy array for easier indexing\n",
    "examples_array = np.array(prepared_examples, dtype=object)\n",
    "\n",
    "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
    "print(f\"Total examples: {len(prepared_examples)}\")\n",
    "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
    "\n",
    "# Load tokenizer (will be used across all folds)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Store results from all folds\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_helper_functions"
   },
   "source": [
    "## 9. K-Fold Cross-Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.384157Z",
     "iopub.status.busy": "2025-10-04T20:04:26.383760Z",
     "iopub.status.idle": "2025-10-04T20:04:26.390174Z",
     "shell.execute_reply": "2025-10-04T20:04:26.389489Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.384111Z"
    },
    "id": "k_fold_helper_functions_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data preparation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BERT-CRF K-FOLD CROSS-VALIDATION HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_fold_data(train_examples, val_examples, tokenizer, ner_dataset):\n",
    "    \"\"\"\n",
    "    Prepare training and validation datasets for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        train_examples: Training examples for this fold\n",
    "        val_examples: Validation examples for this fold\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, data_collator)\n",
    "    \"\"\"\n",
    "    # Tokenize datasets with sliding window\n",
    "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "    \n",
    "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "    \n",
    "    # DEBUG: Print tokenized lengths\n",
    "    print(f\"🔍 DEBUG - After tokenization:\")\n",
    "    print(f\"  train_examples input: {len(train_examples)}\")\n",
    "    print(f\"  val_examples input: {len(val_examples)}\")\n",
    "    print(f\"  train_tokenized output: {len(train_tokenized)}\")\n",
    "    print(f\"  val_tokenized output: {len(val_tokenized)}\")\n",
    "    \n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
    "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, data_collator\n",
    "\n",
    "print(\"✅ Data preparation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.391686Z",
     "iopub.status.busy": "2025-10-04T20:04:26.391270Z",
     "iopub.status.idle": "2025-10-04T20:04:26.399000Z",
     "shell.execute_reply": "2025-10-04T20:04:26.398296Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.391665Z"
    },
    "id": "bert_crf_model_creation"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT-CRF model and trainer creation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_bert_crf_model_and_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device):\n",
    "    \"\"\"\n",
    "    Create BERT-CRF model and trainer for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_num: Current fold number\n",
    "        train_dataset: Training dataset for this fold\n",
    "        val_dataset: Validation dataset for this fold\n",
    "        data_collator: Data collator\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "        device: Device to use (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, trainer, fold_output_dir)\n",
    "    \"\"\"\n",
    "    # Create fold-specific output directory\n",
    "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
    "    import os\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create BERT-CRF model with pretrained weights\n",
    "    model = BertCrfForTokenClassification(\n",
    "        MODEL_NAME,\n",
    "        ner_dataset.get_num_labels(),\n",
    "        ner_dataset.id_to_label,\n",
    "        ner_dataset.label_to_id,\n",
    "        use_crf=USE_CRF\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"🔧 Model created with USE_CRF={USE_CRF}\")\n",
    "    print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create training arguments for this fold\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        num_train_epochs=model_config['num_epochs'],\n",
    "        per_device_train_batch_size=model_config['batch_size'],\n",
    "        per_device_eval_batch_size=model_config['batch_size'],\n",
    "        learning_rate=model_config['learning_rate'],\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        report_to=\"none\",  # Disable wandb for cleaner output\n",
    "        run_name=f\"bertic_crf_fold_{fold_num}\",\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    \n",
    "    # Create metrics callback for comprehensive tracking\n",
    "    metrics_callback = PerClassMetricsCallback(id_to_label=ner_dataset.id_to_label)\n",
    "    \n",
    "    # Create trainer with custom compute_metrics for CRF\n",
    "    def compute_metrics_crf(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        # For CRF, we need to decode predictions differently\n",
    "        # This is a simplified version - you might need to adapt based on your evaluation needs\n",
    "        return {\"f1\": 0.0}  # Placeholder - will be computed in detailed_evaluation\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_crf,\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "    print(f\"BERT-CRF Trainer initialized for fold {fold_num} with comprehensive metrics tracking\")\n",
    "    return model, trainer, metrics_callback, fold_output_dir\n",
    "\n",
    "print(\"✅ BERT-CRF model and trainer creation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.400734Z",
     "iopub.status.busy": "2025-10-04T20:04:26.400366Z",
     "iopub.status.idle": "2025-10-04T20:04:26.411957Z",
     "shell.execute_reply": "2025-10-04T20:04:26.411215Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.400698Z"
    },
    "id": "bert_crf_evaluation"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT-CRF training and evaluation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_bert_crf_fold(fold_num, trainer, val_dataset, ner_dataset, fold_output_dir):\n",
    "    \"\"\"\n",
    "    Train and evaluate a BERT-CRF model for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_num: Current fold number\n",
    "        trainer: Trainer instance\n",
    "        val_dataset: Validation dataset for this fold\n",
    "        ner_dataset: NER dataset instance\n",
    "        fold_output_dir: Output directory for this fold\n",
    "    \n",
    "    Returns:\n",
    "        dict: Fold results including comprehensive metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n🏋️  Training BERT-CRF fold {fold_num}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"💾 Saving BERT-CRF model for fold {fold_num}...\")\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"📊 Evaluating BERT-CRF fold {fold_num}...\")\n",
    "    \n",
    "    # For BERT-CRF, we need custom evaluation since CRF decoding is different\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    \n",
    "    # Get device from trainer\n",
    "    device = trainer.args.device\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in trainer.get_eval_dataloader():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Get predictions (CRF decode or argmax)\n",
    "            predictions = model.predict(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Process labels\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # Handle different prediction formats\n",
    "            if USE_CRF:\n",
    "                # CRF returns list of lists\n",
    "                # Convert to lists and filter out special tokens\n",
    "                for pred_seq, label_seq, attention_seq in zip(predictions, labels, batch['attention_mask']):\n",
    "                    # Filter based on attention mask and ignore -100 labels\n",
    "                    valid_length = attention_seq.sum().item()\n",
    "                    pred_seq = pred_seq[:valid_length]\n",
    "                    label_seq = label_seq[:valid_length]\n",
    "                    \n",
    "                    # Filter out -100 labels\n",
    "                    valid_indices = label_seq != -100\n",
    "                    if valid_indices.any():\n",
    "                        pred_filtered = [pred_seq[i] for i in range(len(pred_seq)) if valid_indices[i]]\n",
    "                        label_filtered = [label_seq[i].item() for i in range(len(label_seq)) if valid_indices[i]]\n",
    "                        \n",
    "                        all_predictions.extend(pred_filtered)\n",
    "                        all_labels.extend(label_filtered)\n",
    "            else:\n",
    "                # Argmax returns numpy array\n",
    "                for pred_seq, label_seq in zip(predictions, labels.cpu().numpy()):\n",
    "                    # Filter out -100 labels\n",
    "                    valid_indices = label_seq != -100\n",
    "                    if valid_indices.any():\n",
    "                        pred_filtered = pred_seq[valid_indices].tolist()\n",
    "                        label_filtered = label_seq[valid_indices].tolist()\n",
    "                        \n",
    "                        all_predictions.extend(pred_filtered)\n",
    "                        all_labels.extend(label_filtered)\n",
    "    \n",
    "    # Convert to label names for evaluation\n",
    "    pred_labels = [ner_dataset.id_to_label[pred] for pred in all_predictions]\n",
    "    true_labels = [ner_dataset.id_to_label[label] for label in all_labels]\n",
    "    \n",
    "    # Calculate metrics using seqeval\n",
    "    from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "    \n",
    "    # Convert to sequence format for seqeval\n",
    "    pred_sequences = [pred_labels]\n",
    "    true_sequences = [true_labels]\n",
    "    \n",
    "    precision = precision_score(true_sequences, pred_sequences)\n",
    "    recall = recall_score(true_sequences, pred_sequences)\n",
    "    f1 = f1_score(true_sequences, pred_sequences)\n",
    "    accuracy = accuracy_score(true_sequences, pred_sequences)\n",
    "    \n",
    "    # Flatten for confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    flat_true = true_labels\n",
    "    flat_pred = pred_labels\n",
    "    all_labels = sorted(list(set(flat_true + flat_pred)))\n",
    "    cm = confusion_matrix(flat_true, flat_pred, labels=all_labels)\n",
    "    \n",
    "    # Generate classification report for this fold\n",
    "    # Convert to sequence format for generate_detailed_classification_report\n",
    "    true_sequences_for_report = [true_labels]\n",
    "    pred_sequences_for_report = [pred_labels]\n",
    "    per_class_metrics = generate_detailed_classification_report(\n",
    "        true_sequences_for_report, pred_sequences_for_report, fold_output_dir, fold_num, \"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    fold_result = {\n",
    "        'fold': fold_num,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'per_class_metrics': per_class_metrics,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBERT-CRF Fold {fold_num} completed successfully!\")\n",
    "    return fold_result\n",
    "\n",
    "print(\"✅ BERT-CRF training and evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_main_training"
   },
   "source": [
    "## 10. K-Fold Cross-Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T20:04:26.413677Z",
     "iopub.status.busy": "2025-10-04T20:04:26.413101Z",
     "iopub.status.idle": "2025-10-04T20:23:11.426770Z",
     "shell.execute_reply": "2025-10-04T20:23:11.425562Z",
     "shell.execute_reply.started": "2025-10-04T20:04:26.413655Z"
    },
    "id": "k_fold_main_loop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "STARTING 5-FOLD CROSS-VALIDATION - BERT-CRF\n",
      "================================================================================\n",
      "Total examples: 225\n",
      "Model: classla/bcms-bertic + CRF\n",
      "Device: cuda\n",
      "\n",
      "================================================================================\n",
      "BERT-CRF FOLD 1/5\n",
      "================================================================================\n",
      "Train indices: 180, Val indices: 45\n",
      "Training examples: 180\n",
      "Validation examples: 45\n",
      "\n",
      "📊 Analyzing entity distribution for fold 1...\n",
      "\n",
      "📊 Entity Distribution - Fold 1 - Training\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   7320          35.29%\n",
      "PROVISION_PROCEDURAL                 3210          15.48%\n",
      "CRIMINAL_ACT                         2252          10.86%\n",
      "COURT                                1641           7.91%\n",
      "DEFENDANT                            1445           6.97%\n",
      "SANCTION_VALUE                        956           4.61%\n",
      "JUDGE                                 734           3.54%\n",
      "REGISTRAR                             731           3.52%\n",
      "VERDICT                               573           2.76%\n",
      "PROSECUTOR                            510           2.46%\n",
      "SANCTION_TYPE                         448           2.16%\n",
      "PROCEDURE_COSTS                       391           1.89%\n",
      "DECISION_DATE                         348           1.68%\n",
      "CASE_NUMBER                           181           0.87%\n",
      "------------------------------------------------------------\n",
      "TOTAL                               20740         100.00%\n",
      "\n",
      "📊 Entity Distribution - Fold 1 - Validation\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   2284          37.58%\n",
      "PROVISION_PROCEDURAL                  958          15.76%\n",
      "CRIMINAL_ACT                          695          11.44%\n",
      "DEFENDANT                             458           7.54%\n",
      "COURT                                 405           6.66%\n",
      "SANCTION_VALUE                        242           3.98%\n",
      "REGISTRAR                             183           3.01%\n",
      "JUDGE                                 176           2.90%\n",
      "PROSECUTOR                            155           2.55%\n",
      "SANCTION_TYPE                         155           2.55%\n",
      "VERDICT                               148           2.44%\n",
      "DECISION_DATE                          87           1.43%\n",
      "PROCEDURE_COSTS                        86           1.42%\n",
      "CASE_NUMBER                            45           0.74%\n",
      "------------------------------------------------------------\n",
      "TOTAL                                6077         100.00%\n",
      "\n",
      "🔤 Preparing data for BERT-CRF fold 1...\n",
      "🔍 DEBUG - After tokenization:\n",
      "  train_examples input: 180\n",
      "  val_examples input: 45\n",
      "  train_tokenized output: 1845\n",
      "  val_tokenized output: 500\n",
      "📦 Created HuggingFace datasets:\n",
      "  Training: 1845 examples\n",
      "  Validation: 500 examples\n",
      "  Test: 500 examples\n",
      "📦 BERT-CRF Fold 1 datasets:\n",
      "  Training: 1845 examples\n",
      "  Validation: 500 examples\n",
      "\n",
      "🤖 Creating BERT-CRF model and trainer for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Model created with USE_CRF=False\n",
      "📊 Model parameters: 110,049,053\n",
      "BERT-CRF Trainer initialized for fold 1 with comprehensive metrics tracking\n",
      "\n",
      "🏋️  Training BERT-CRF fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1848' max='1848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1848/1848 13:54, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.913300</td>\n",
       "      <td>0.731893</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.305802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.196542</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.139997</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.101585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.084291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.072351</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.074791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.070873</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.076422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.075088</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.073913</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.077011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.077185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.078892</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving BERT-CRF model for fold 1...\n",
      "📊 Evaluating BERT-CRF fold 1...\n",
      "\n",
      "================================================================================\n",
      "Validation - Fold 1 Set - Detailed Classification Report:\n",
      "================================================================================\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         B-CASE_NUMBER     0.0000    0.0000    0.0000        45\n",
      "               B-COURT     0.9381    0.8689    0.9021       122\n",
      "        B-CRIMINAL_ACT     0.9117    0.9750    0.9423       519\n",
      "       B-DECISION_DATE     0.7171    0.9561    0.8195       114\n",
      "           B-DEFENDANT     0.5754    0.7435    0.6487       811\n",
      "               B-JUDGE     0.8500    0.4766    0.6108       107\n",
      "     B-PROCEDURE_COSTS     0.9930    1.0000    0.9965       141\n",
      "          B-PROSECUTOR     0.8182    0.0373    0.0714       241\n",
      "  B-PROVISION_MATERIAL     0.9095    0.9817    0.9442       819\n",
      "B-PROVISION_PROCEDURAL     0.9550    0.9475    0.9513       381\n",
      "           B-REGISTRAR     0.6071    0.7522    0.6719       113\n",
      "       B-SANCTION_TYPE     0.7336    0.9086    0.8118       197\n",
      "      B-SANCTION_VALUE     1.0000    0.6316    0.7742       190\n",
      "             B-VERDICT     0.9510    0.7907    0.8635       172\n",
      "               I-COURT     0.9126    0.9576    0.9346       425\n",
      "        I-CRIMINAL_ACT     0.9403    0.9910    0.9650      1557\n",
      "       I-DECISION_DATE     0.0000    0.0000    0.0000        27\n",
      "           I-DEFENDANT     0.5986    0.7030    0.6466       596\n",
      "               I-JUDGE     0.6512    0.5091    0.5714       110\n",
      "     I-PROCEDURE_COSTS     0.7000    0.0636    0.1167       110\n",
      "          I-PROSECUTOR     0.7143    0.1020    0.1786       196\n",
      "  I-PROVISION_MATERIAL     0.8948    0.9888    0.9394      6345\n",
      "I-PROVISION_PROCEDURAL     0.9320    0.8689    0.8993      2380\n",
      "           I-REGISTRAR     0.4477    0.6638    0.5347       116\n",
      "       I-SANCTION_TYPE     0.7904    0.7215    0.7544       298\n",
      "      I-SANCTION_VALUE     0.6135    0.8991    0.7293       535\n",
      "             I-VERDICT     0.9477    0.8831    0.9143       308\n",
      "                     O     0.9957    0.9912    0.9934    147192\n",
      "\n",
      "              accuracy                         0.9803    164167\n",
      "             macro avg     0.7535    0.6933    0.6852    164167\n",
      "          weighted avg     0.9813    0.9803    0.9794    164167\n",
      "\n",
      "✅ Classification report saved to: /storage/models/bertic_no_crf_5fold_cv/fold_1/classification_report_fold1.txt\n",
      "\n",
      "BERT-CRF Fold 1 completed successfully!\n",
      "\n",
      "✅ BERT-CRF Fold 1 completed!\n",
      "   Precision: 0.6764\n",
      "   Recall: 0.7578\n",
      "   F1-Score: 0.7148\n",
      "   Accuracy: 0.9803\n",
      "\n",
      "================================================================================\n",
      "BERT-CRF FOLD 2/5\n",
      "================================================================================\n",
      "Train indices: 180, Val indices: 45\n",
      "Training examples: 180\n",
      "Validation examples: 45\n",
      "\n",
      "📊 Analyzing entity distribution for fold 2...\n",
      "\n",
      "📊 Entity Distribution - Fold 2 - Training\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   7719          35.76%\n",
      "PROVISION_PROCEDURAL                 3372          15.62%\n",
      "CRIMINAL_ACT                         2289          10.60%\n",
      "COURT                                1640           7.60%\n",
      "DEFENDANT                            1564           7.25%\n",
      "SANCTION_VALUE                        964           4.47%\n",
      "REGISTRAR                             733           3.40%\n",
      "JUDGE                                 727           3.37%\n",
      "VERDICT                               601           2.78%\n",
      "PROSECUTOR                            544           2.52%\n",
      "SANCTION_TYPE                         507           2.35%\n",
      "PROCEDURE_COSTS                       394           1.83%\n",
      "DECISION_DATE                         351           1.63%\n",
      "CASE_NUMBER                           181           0.84%\n",
      "------------------------------------------------------------\n",
      "TOTAL                               21586         100.00%\n",
      "\n",
      "📊 Entity Distribution - Fold 2 - Validation\n",
      "============================================================\n",
      "Entity Type                         Count      Percentage\n",
      "------------------------------------------------------------\n",
      "PROVISION_MATERIAL                   1885          36.04%\n",
      "PROVISION_PROCEDURAL                  796          15.22%\n",
      "CRIMINAL_ACT                          658          12.58%\n",
      "COURT                                 406           7.76%\n",
      "DEFENDANT                             339           6.48%\n",
      "SANCTION_VALUE                        234           4.47%\n",
      "JUDGE                                 183           3.50%\n",
      "REGISTRAR                             181           3.46%\n",
      "PROSECUTOR                            121           2.31%\n",
      "VERDICT                               120           2.29%\n",
      "SANCTION_TYPE                          96           1.84%\n",
      "DECISION_DATE                          84           1.61%\n",
      "PROCEDURE_COSTS                        83           1.59%\n",
      "CASE_NUMBER                            45           0.86%\n",
      "------------------------------------------------------------\n",
      "TOTAL                                5231         100.00%\n",
      "\n",
      "🔤 Preparing data for BERT-CRF fold 2...\n",
      "🔍 DEBUG - After tokenization:\n",
      "  train_examples input: 180\n",
      "  val_examples input: 45\n",
      "  train_tokenized output: 1900\n",
      "  val_tokenized output: 445\n",
      "📦 Created HuggingFace datasets:\n",
      "  Training: 1900 examples\n",
      "  Validation: 445 examples\n",
      "  Test: 445 examples\n",
      "📦 BERT-CRF Fold 2 datasets:\n",
      "  Training: 1900 examples\n",
      "  Validation: 445 examples\n",
      "\n",
      "🤖 Creating BERT-CRF model and trainer for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Model created with USE_CRF=False\n",
      "📊 Model parameters: 110,049,053\n",
      "BERT-CRF Trainer initialized for fold 2 with comprehensive metrics tracking\n",
      "\n",
      "🏋️  Training BERT-CRF fold 2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='584' max='1904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 584/1904 04:13 < 09:35, 2.29 it/s, Epoch 2.45/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.170400</td>\n",
       "      <td>0.792598</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.310458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.165956</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.113186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.096055</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m model, trainer, metrics_callback, fold_output_dir \u001b[38;5;241m=\u001b[39m create_bert_crf_model_and_trainer(\n\u001b[1;32m     45\u001b[0m     fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train and evaluate this fold\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m fold_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_bert_crf_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_output_dir\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Store comprehensive data for aggregation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m fold_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistributions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dist, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: val_dist}\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mtrain_and_evaluate_bert_crf_fold\u001b[0;34m(fold_num, trainer, val_dataset, ner_dataset, fold_output_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🏋️  Training BERT-CRF fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💾 Saving BERT-CRF model for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Main K-Fold Cross-Validation Loop for BERT-CRF\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION - BERT-CRF\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total examples: {len(examples_array)}\")\n",
    "print(f\"Model: {MODEL_NAME} + CRF\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Execute K-Fold training\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BERT-CRF FOLD {fold_num}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
    "    \n",
    "    # Get fold data\n",
    "    train_examples = examples_array[train_idx].tolist()\n",
    "    val_examples = examples_array[val_idx].tolist()\n",
    "    \n",
    "    print(f\"Training examples: {len(train_examples)}\")\n",
    "    print(f\"Validation examples: {len(val_examples)}\")\n",
    "    \n",
    "    # Analyze entity distribution for this fold\n",
    "    print(f\"\\n📊 Analyzing entity distribution for fold {fold_num}...\")\n",
    "    train_dist = analyze_entity_distribution_per_fold(train_examples, f\"Fold {fold_num} - Training\")\n",
    "    val_dist = analyze_entity_distribution_per_fold(val_examples, f\"Fold {fold_num} - Validation\")\n",
    "    \n",
    "    # Prepare data for this fold\n",
    "    print(f\"\\n🔤 Preparing data for BERT-CRF fold {fold_num}...\")\n",
    "    train_dataset, val_dataset, data_collator = prepare_fold_data(\n",
    "        train_examples, val_examples, tokenizer, ner_dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"📦 BERT-CRF Fold {fold_num} datasets:\")\n",
    "    print(f\"  Training: {len(train_dataset)} examples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "    \n",
    "    # Create BERT-CRF model and trainer for this fold\n",
    "    print(f\"\\n🤖 Creating BERT-CRF model and trainer for fold {fold_num}...\")\n",
    "    model, trainer, metrics_callback, fold_output_dir = create_bert_crf_model_and_trainer(\n",
    "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate this fold\n",
    "    fold_result = train_and_evaluate_bert_crf_fold(\n",
    "        fold_num, trainer, val_dataset, ner_dataset, fold_output_dir\n",
    "    )\n",
    "    \n",
    "    # Store comprehensive data for aggregation\n",
    "    fold_result['distributions'] = {'train': train_dist, 'val': val_dist}\n",
    "    fold_result['training_history'] = metrics_callback.get_training_history()\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model, trainer, metrics_callback, train_dataset, val_dataset\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(f\"\\n✅ BERT-CRF Fold {fold_num} completed!\")\n",
    "    print(f\"   Precision: {fold_result['precision']:.4f}\")\n",
    "    print(f\"   Recall: {fold_result['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {fold_result['f1']:.4f}\")\n",
    "    print(f\"   Accuracy: {fold_result['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BERT-CRF K-FOLD CROSS-VALIDATION COMPLETED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_results_analysis"
   },
   "source": [
    "## 11. BERT-CRF Results Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T20:23:11.427569Z",
     "iopub.status.idle": "2025-10-04T20:23:11.428051Z",
     "shell.execute_reply": "2025-10-04T20:23:11.427940Z",
     "shell.execute_reply.started": "2025-10-04T20:23:11.427925Z"
    },
    "id": "k_fold_summary_stats"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BERT-CRF K-FOLD RESULTS WITH COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Create comprehensive aggregate report with all visualizations\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GENERATING COMPREHENSIVE AGGREGATE REPORT FOR BERT-CRF\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "aggregate_report = create_aggregate_report_across_folds(\n",
    "    fold_results=fold_results,\n",
    "    model_name=\"BERT-CRF (classla/bcms-bertic + CRF)\",\n",
    "    display=True\n",
    ")\n",
    "\n",
    "# Extract summary metrics\n",
    "precisions = [result['precision'] for result in fold_results]\n",
    "recalls = [result['recall'] for result in fold_results]\n",
    "f1_scores = [result['f1'] for result in fold_results]\n",
    "accuracies = [result['accuracy'] for result in fold_results]\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BERT-CRF K-FOLD CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"\\n📊 BERT-CRF PERFORMANCE METRICS ACROSS {N_FOLDS} FOLDS:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\n🎯 PRECISION:\")\n",
    "print(f\"  Mean: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
    "print(f\"  Min:  {np.min(precisions):.4f} (Fold {np.argmin(precisions) + 1})\")\n",
    "print(f\"  Max:  {np.max(precisions):.4f} (Fold {np.argmax(precisions) + 1})\")\n",
    "\n",
    "print(f\"\\n🎯 RECALL:\")\n",
    "print(f\"  Mean: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
    "print(f\"  Min:  {np.min(recalls):.4f} (Fold {np.argmin(recalls) + 1})\")\n",
    "print(f\"  Max:  {np.max(recalls):.4f} (Fold {np.argmax(recalls) + 1})\")\n",
    "\n",
    "print(f\"\\n🎯 F1-SCORE:\")\n",
    "print(f\"  Mean: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"  Min:  {np.min(f1_scores):.4f} (Fold {np.argmin(f1_scores) + 1})\")\n",
    "print(f\"  Max:  {np.max(f1_scores):.4f} (Fold {np.argmax(f1_scores) + 1})\")\n",
    "\n",
    "print(f\"\\n🎯 ACCURACY:\")\n",
    "print(f\"  Mean: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"  Min:  {np.min(accuracies):.4f} (Fold {np.argmin(accuracies) + 1})\")\n",
    "print(f\"  Max:  {np.max(accuracies):.4f} (Fold {np.argmax(accuracies) + 1})\")\n",
    "\n",
    "# Individual fold results\n",
    "print(f\"\\n📋 INDIVIDUAL BERT-CRF FOLD RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "for i, result in enumerate(fold_results, 1):\n",
    "    print(f\"Fold {i}: P={result['precision']:.4f}, R={result['recall']:.4f}, F1={result['f1']:.4f}, Acc={result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T20:23:11.429151Z",
     "iopub.status.idle": "2025-10-04T20:23:11.429396Z",
     "shell.execute_reply": "2025-10-04T20:23:11.429292Z",
     "shell.execute_reply.started": "2025-10-04T20:23:11.429269Z"
    },
    "id": "bert_crf_save_results"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE BERT-CRF RESULTS TO FILE\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    'experiment_info': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'architecture': 'BERT-CRF',\n",
    "        'n_folds': N_FOLDS,\n",
    "        'total_examples': len(prepared_examples),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'precision': {\n",
    "            'mean': float(np.mean(precisions)),\n",
    "            'std': float(np.std(precisions)),\n",
    "            'min': float(np.min(precisions)),\n",
    "            'max': float(np.max(precisions))\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': float(np.mean(recalls)),\n",
    "            'std': float(np.std(recalls)),\n",
    "            'min': float(np.min(recalls)),\n",
    "            'max': float(np.max(recalls))\n",
    "        },\n",
    "        'f1_score': {\n",
    "            'mean': float(np.mean(f1_scores)),\n",
    "            'std': float(np.std(f1_scores)),\n",
    "            'min': float(np.min(f1_scores)),\n",
    "            'max': float(np.max(f1_scores))\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'mean': float(np.mean(accuracies)),\n",
    "            'std': float(np.std(accuracies)),\n",
    "            'min': float(np.min(accuracies)),\n",
    "            'max': float(np.max(accuracies))\n",
    "        }\n",
    "    },\n",
    "    'fold_results': [\n",
    "        {\n",
    "            'fold': result['fold'],\n",
    "            'precision': float(result['precision']),\n",
    "            'recall': float(result['recall']),\n",
    "            'f1': float(result['f1']),\n",
    "            'accuracy': float(result['accuracy'])\n",
    "        }\n",
    "        for result in fold_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = f\"{OUTPUT_DIR}/bert_crf_5fold_cv_results.json\"\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ BERT-CRF Results saved to: {results_file}\")\n",
    "\n",
    "# Create CSV for easy analysis\n",
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        'Fold': result['fold'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1'],\n",
    "        'Accuracy': result['accuracy']\n",
    "    }\n",
    "    for result in fold_results\n",
    "])\n",
    "\n",
    "# Add summary row\n",
    "summary_row = {\n",
    "    'Fold': 'Mean ± Std',\n",
    "    'Precision': f\"{np.mean(precisions):.4f} ± {np.std(precisions):.4f}\",\n",
    "    'Recall': f\"{np.mean(recalls):.4f} ± {np.std(recalls):.4f}\",\n",
    "    'F1-Score': f\"{np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\",\n",
    "    'Accuracy': f\"{np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\"\n",
    "}\n",
    "\n",
    "df_results = pd.concat([df_results, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "csv_file = f\"{OUTPUT_DIR}/bert_crf_5fold_cv_results.csv\"\n",
    "df_results.to_csv(csv_file, index=False)\n",
    "print(f\"✅ BERT-CRF Results CSV saved to: {csv_file}\")\n",
    "\n",
    "# Display final summary table\n",
    "print(f\"\\n📊 BERT-CRF FINAL RESULTS TABLE:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "This notebook successfully implemented 5-fold cross-validation for the Serbian Legal NER pipeline using BERT-CRF architecture.\n",
    "\n",
    "### Key Achievements:\n",
    "- ✅ **BERT-CRF Implementation**: Combined BERT embeddings with CRF layer for better sequence modeling\n",
    "- ✅ **Robust Evaluation**: 5-fold cross-validation provides reliable performance estimates\n",
    "- ✅ **CRF Decoding**: Proper CRF decoding for optimal sequence labeling\n",
    "- ✅ **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracked across all folds\n",
    "- ✅ **Results Persistence**: JSON and CSV files saved for comparison with other models\n",
    "\n",
    "### BERT-CRF Advantages:\n",
    "- **Better Sequence Constraints**: CRF enforces valid BIO tag transitions\n",
    "- **Global Optimization**: Considers entire sequence for optimal labeling\n",
    "- **Improved Entity Boundaries**: More accurate entity span detection\n",
    "\n",
    "The BERT-CRF 5-fold cross-validation results can now be compared with the base BERT model to evaluate the impact of the CRF layer!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
