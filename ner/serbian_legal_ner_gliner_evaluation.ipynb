{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GLiNER Evaluation: Zero-Shot vs Few-Shot on Serbian Legal Documents\n",
        "\n",
        "This notebook evaluates GLiNER (Generalist and Lightweight Named Entity Recognition) on 225 Serbian legal documents.\n",
        "\n",
        "## ğŸ¯ **Evaluation Flow:**\n",
        "1. **Setup**: Define everything needed for GLiNER and few-shot\n",
        "2. **Zero-Shot Evaluation**: Run GLiNER without examples\n",
        "3. **Few-Shot Evaluation**: Run GLiNER with manual examples\n",
        "4. **Results Analysis**: Compare performance\n",
        "\n",
        "## ğŸ·ï¸ **Entity Types (Serbian):**\n",
        "- `sud ili tribunal` â†’ COURT\n",
        "- `sudija ili pravosudni sluÅ¾benik` â†’ JUDGE  \n",
        "- `optuÅ¾eni ili osoba na suÄ‘enju` â†’ DEFENDANT\n",
        "- `iznos ili trajanje kazne` â†’ SANCTION_VALUE\n",
        "- `broj predmeta ili identifikator sluÄaja` â†’ CASE_NUMBER\n",
        "- And 9 more entity types for comprehensive legal NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gliner seqeval scikit-learn matplotlib seaborn pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Define Everything Needed for GLiNER and Few-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GLiNER\n",
        "from gliner import GLiNER\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸš€ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU not available - will use CPU (slower)\")\n",
        "\n",
        "print(\"âœ… All dependencies loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "LABELSTUDIO_JSON_PATH = \"/content/drive/MyDrive/NER_Master/annotations.json\"  # Update path as needed\n",
        "JUDGMENTS_DIR = \"/content/drive/MyDrive/NER_Master/judgments\"  # Update path as needed\n",
        "FEW_SHOT_EXAMPLES_DIR = \"/content/drive/MyDrive/NER_Master/few_shot_examples\"  # Manual examples directory\n",
        "CONFIDENCE_THRESHOLD = 0.3  # GLiNER confidence threshold\n",
        "\n",
        "# Serbian Legal Entity Types for GLiNER\n",
        "LEGAL_ENTITY_TYPES = [\n",
        "    \"sud ili tribunal\",  # COURT\n",
        "    \"datum presude ili odluke\",  # DECISION_DATE\n",
        "    \"broj predmeta ili identifikator sluÄaja\",  # CASE_NUMBER\n",
        "    \"kriviÄno delo ili prestup\",  # CRIMINAL_ACT\n",
        "    \"tuÅ¾ilac ili javni tuÅ¾ilac\",  # PROSECUTOR\n",
        "    \"optuÅ¾eni ili osoba na suÄ‘enju\",  # DEFENDANT\n",
        "    \"sudija ili pravosudni sluÅ¾benik\",  # JUDGE\n",
        "    \"sudski zapisniÄar ili sluÅ¾benik\",  # REGISTRAR\n",
        "    \"sudska presuda ili odluka\",  # VERDICT\n",
        "    \"vrsta kazne ili sankcije\",  # SANCTION_TYPE\n",
        "    \"iznos ili trajanje kazne\",  # SANCTION_VALUE\n",
        "    \"materijalna pravna odredba ili Älan\",  # PROVISION_MATERIAL\n",
        "    \"procesna pravna odredba ili Älan\",  # PROVISION_PROCEDURAL\n",
        "    \"troskovi ili takse sudskog postupka\",  # PROCEDURE_COSTS\n",
        "]\n",
        "\n",
        "# Mapping from Serbian labels to ground truth labels\n",
        "GLINER_TO_GT_MAPPING = {\n",
        "    \"sud ili tribunal\": \"COURT\",\n",
        "    \"datum presude ili odluke\": \"DECISION_DATE\",\n",
        "    \"broj predmeta ili identifikator sluÄaja\": \"CASE_NUMBER\",\n",
        "    \"kriviÄno delo ili prestup\": \"CRIMINAL_ACT\",\n",
        "    \"tuÅ¾ilac ili javni tuÅ¾ilac\": \"PROSECUTOR\",\n",
        "    \"optuÅ¾eni ili osoba na suÄ‘enju\": \"DEFENDANT\",\n",
        "    \"sudija ili pravosudni sluÅ¾benik\": \"JUDGE\",\n",
        "    \"sudski zapisniÄar ili sluÅ¾benik\": \"REGISTRAR\",\n",
        "    \"sudska presuda ili odluka\": \"VERDICT\",\n",
        "    \"vrsta kazne ili sankcije\": \"SANCTION_TYPE\",\n",
        "    \"iznos ili trajanje kazne\": \"SANCTION_VALUE\",\n",
        "    \"materijalna pravna odredba ili Älan\": \"PROVISION_MATERIAL\",\n",
        "    \"procesna pravna odredba ili Älan\": \"PROVISION_PROCEDURAL\",\n",
        "    \"troskovi ili takse sudskog postupka\": \"PROCEDURE_COSTS\",\n",
        "}\n",
        "\n",
        "print(f\"ğŸ¯ Entity types: {len(LEGAL_ENTITY_TYPES)}\")\n",
        "print(f\"âš¡ Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import complete manual annotations with all 14 entity types\n",
        "import sys\n",
        "\n",
        "sys.path.append(\".\")\n",
        "\n",
        "\n",
        "# Use the complete annotations\n",
        "MANUAL_ANNOTATIONS = {\n",
        "    \"example_01_theft.txt\": {\n",
        "        \"sud ili tribunal\": [\"Osnovni sud u Herceg Novom\"],\n",
        "        \"datum presude ili odluke\": [\"30.12.2019.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 245/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"kraÄ‘e\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Dragana MilovanoviÄ‡a\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"M.P.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Marija NikoliÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Ane StojanoviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"USLOVNU OSUDU\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\"],\n",
        "        \"iznos ili trajanje kazne\": [\"6 mjeseci\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 344. stav 1. KriviÄnog zakonika\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\n",
        "            \"Älanu 434. Zakonika o kriviÄnom postupku\",\n",
        "            \"Älana 261. Zakonika o kriviÄnom postupku\",\n",
        "        ],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"40â‚¬\"],\n",
        "    },\n",
        "    \"example_02_assault.txt\": {\n",
        "        \"sud ili tribunal\": [\"ViÅ¡i sud u Podgorici\"],\n",
        "        \"datum presude ili odluke\": [\"16.05.2019.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"KÅ¾. 1567/22\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"nasilniÄko ponaÅ¡anje\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Milana ÄorÄ‘eviÄ‡a\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"S.M.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\n",
        "            \"Aleksandar JovanoviÄ‡\",\n",
        "            \"Milica RadoviÄ‡\",\n",
        "            \"Petar StankoviÄ‡\",\n",
        "        ],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Jovane MitroviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"OSLOBAÄA SE OD OPTUÅ½BE\"],\n",
        "        \"vrsta kazne ili sankcije\": [],\n",
        "        \"iznos ili trajanje kazne\": [],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 220. stav 1. KriviÄnog zakonika\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\n",
        "            \"Älanu 434. Zakonika o kriviÄnom postupku\"\n",
        "        ],\n",
        "        \"troskovi ili takse sudskog postupka\": [],\n",
        "    },\n",
        "    \"example_03_fraud.txt\": {\n",
        "        \"sud ili tribunal\": [\"Osnovni sud u NikÅ¡iÄ‡u\", \"OSNOVNI SUD U NIKÅ IÄ†U\"],\n",
        "        \"datum presude ili odluke\": [\"28. juna 2023. godine\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 567/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"prevare\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"SrÄ‘ana PetroviÄ‡a\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"A.S.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Jelena MilosavljeviÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\n",
        "            \"Nemanje StojanoviÄ‡a\",\n",
        "            \"Nemanja StojanoviÄ‡\",\n",
        "        ],\n",
        "        \"sudska presuda ili odluka\": [\"O S U Ä U J E\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\", \"uslovnu osudu\"],\n",
        "        \"iznos ili trajanje kazne\": [\"8 mjeseci\", \"sa rokom kuÅ¡nje od 2 godine\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 208. stav 1. KriviÄnog zakonika\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"120â‚¬\"],\n",
        "    },\n",
        "    \"example_04_traffic.txt\": {\n",
        "        \"sud ili tribunal\": [\n",
        "            \"PREKRÅ AJNI SUD U PODGORICI\",\n",
        "            \"PrekrÅ¡ajni sud u Podgorici\",\n",
        "        ],\n",
        "        \"datum presude ili odluke\": [\"10. maja 2023. godine\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"Pr. br. 3456/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"prekrÅ¡aj\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"V.M.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Marko ÄorÄ‘eviÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Tanje NikoliÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"P R E S U D U\"],\n",
        "        \"vrsta kazne ili sankcije\": [\n",
        "            \"novÄanom kaznom\",\n",
        "            \"ZAÅ TITNA MERA zabrane upravljanja motornim vozilom\",\n",
        "        ],\n",
        "        \"iznos ili trajanje kazne\": [\"80â‚¬\", \"u trajanju od 8 mjeseci\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 330. stav 1. taÄka 3. Zakona o bezbjednosti saobraÄ‡aja na putevima\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\"Älanu 175. Zakona o prekrÅ¡ajima\"],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"30â‚¬\"],\n",
        "    },\n",
        "    \"example_05_drug_possession.txt\": {\n",
        "        \"sud ili tribunal\": [\"Osnovni sud u Baru\"],\n",
        "        \"datum presude ili odluke\": [\"20.10.2023.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 567/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\n",
        "            \"neovlaÅ¡Ä‡ena proizvodnja i stavljanje u promet opojnih droga\"\n",
        "        ],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\n",
        "            \"Marije StankoviÄ‡\"\n",
        "        ],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"N.R.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Ana PopoviÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Milan JovanoviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"USLOVNU OSUDU\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\", \"Oduzima se predmet\"],\n",
        "        \"iznos ili trajanje kazne\": [\"6 mjeseci\", \"marihuana\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 246a stav 1. KriviÄnog zakonika\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\n",
        "            \"Älanu 423. Zakonika o kriviÄnom postupku\"\n",
        "        ],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"80â‚¬\"],\n",
        "    },\n",
        "    \"example_06_domestic_violence.txt\": {\n",
        "        \"sud ili tribunal\": [\"OSNOVNI SUD U PLJEVLJIMA\", \"Osnovni sud u Pljevljima\"],\n",
        "        \"datum presude ili odluke\": [\"25. 01. 2017.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 789/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"nasilje u porodici\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Dragice NikoliÄ‡\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"Z.M.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Gordana MiliÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Jovana PetroviÄ‡a\", \"Jovan PetroviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"O S U Ä U J E\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\", \"ZAÅ TITNA MERA zabrane prilaska\"],\n",
        "        \"iznos ili trajanje kazne\": [\"10 mjeseci\", \"u trajanju od 1 godine\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 194. stav 2. KriviÄnog zakonika CG\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"150â‚¬\"],\n",
        "    },\n",
        "    \"example_07_embezzlement.txt\": {\n",
        "        \"sud ili tribunal\": [\"VIÅ I SUD U PODGORICI\", \"ViÅ¡i sud u Podgorici\"],\n",
        "        \"datum presude ili odluke\": [\"09.12.2020\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 234/22\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"pronevjere\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Aleksandra MilovanoviÄ‡a\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"M.P.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\n",
        "            \"Milan StojanoviÄ‡\",\n",
        "        ],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Milice JovanoviÄ‡\", \"Milica JovanoviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"O S U Ä U J E\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\"],\n",
        "        \"iznos ili trajanje kazne\": [\"3 godine\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 364. stav 3. KriviÄnog zakonika\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\"Äl. 363 st. 1 taÄ. 3 ZKP-a\"],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"450â‚¬\"],\n",
        "    },\n",
        "    \"example_08_tax_evasion.txt\": {\n",
        "        \"sud ili tribunal\": [\"OSNOVNI SUD U ROÅ½AJAMA\", \"Osnovni sud u RoÅ¾ajama\"],\n",
        "        \"datum presude ili odluke\": [\"25. 08. 2023.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 445/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"poreska utaja\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Jovane PetroviÄ‡\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"R.Ä.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Milena StankoviÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Nemanje MilovanoviÄ‡a\", \"Nemanja MilovanoviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"USLOVNU OSUDU\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\"],\n",
        "        \"iznos ili trajanje kazne\": [\"1 godine i 6 mjeseci\", \"u roku od 3 godine\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 229. stav 2. KriviÄnog zakonika\",\n",
        "            \"Äl. 4 st. 2, Äl. 5, Äl. 13, Äl.15, Äl. 42 st. 1, Äl. 52 st. 2, Äl. 53 i Äl. 54 KriviÄnog zakonika Crne Gore\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\"Äl. 226, Äl. 229 i Äl. 374 Zakonika o kriviÄnom postupku\"],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"250â‚¬\"],\n",
        "    },\n",
        "    \"example_09_robbery.txt\": {\n",
        "        \"sud ili tribunal\": [\"OSNOVNI SUD U BIJELO POLJU\", \"Osnovni sud u Bijelo Polju\"],\n",
        "        \"datum presude ili odluke\": [\"06.09.2011.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 678/23\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"razbojniÅ¡tva\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Milice ÄorÄ‘eviÄ‡\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"M.J.\", \"S.N.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\"Bojan MarkoviÄ‡\"],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"Ane MilenkoviÄ‡\", \"Ana MilenkoviÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"O S U Ä U J E\"],\n",
        "        \"vrsta kazne ili sankcije\": [\"kaznu zatvora\"],\n",
        "        \"iznos ili trajanje kazne\": [\"2 godine\", \"1 godine i 8 mjeseci\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Älana 206. stav 1. KriviÄnog zakonika\",\n",
        "            \"Äl.3, 4, 5, 13, 16, 32, 42, 45, 46, 52, 53 i 54 KZ CG\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\"Äl. 226 st.3 i Äl.374 ZKP-a\"],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"300â‚¬\"],\n",
        "    },\n",
        "    \"example_10_corruption.txt\": {\n",
        "        \"sud ili tribunal\": [\"OSNOVNI SUD U BERANAMA\", \"Osnovni sud u Beranama\"],\n",
        "        \"datum presude ili odluke\": [\"01.04.2024.\"],\n",
        "        \"broj predmeta ili identifikator sluÄaja\": [\"K. br. 123/22\"],\n",
        "        \"kriviÄno delo ili prestup\": [\"primanja mita\"],\n",
        "        \"tuÅ¾ilac ili javni tuÅ¾ilac\": [\"Nikole SamardÅ¾iÄ‡a\"],\n",
        "        \"optuÅ¾eni ili osoba na suÄ‘enju\": [\"V.S.\"],\n",
        "        \"sudija ili pravosudni sluÅ¾benik\": [\n",
        "            \"Vesna GazdiÄ‡\"\n",
        "        ],\n",
        "        \"sudski zapisniÄar ili sluÅ¾benik\": [\"RistiÄ‡ Katarina\", \"Katarine RistiÄ‡\"],\n",
        "        \"sudska presuda ili odluka\": [\"O S U Ä U J E\"],\n",
        "        \"vrsta kazne ili sankcije\": [\n",
        "            \"kaznu zatvora\",\n",
        "            \"SPOREDNA KAZNA zabrane vrÅ¡enja javne funkcije\",\n",
        "        ],\n",
        "        \"iznos ili trajanje kazne\": [\"4 godine\", \"u trajanju od 3 godine\"],\n",
        "        \"materijalna pravna odredba ili Älan\": [\n",
        "            \"Äl.327 st.4 u vezi st.1 KriviÄnog Zakonika CG\",\n",
        "            \"Äl. 2, Äl. 3, Äl. 4 st. 2, Äl. 5, Äl. 13 st. 1, Äl. 15, Äl. 32, Äl. 36, Äl. 42, Äl. 51 st. 1 KriviÄnog zakonika Crne Gore\"\n",
        "        ],\n",
        "        \"procesna pravna odredba ili Älan\": [\"Äl. 226, 229 i 374 Zakonika o kriviÄnom postupku\"],\n",
        "        \"troskovi ili takse sudskog postupka\": [\"600â‚¬\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Display statistics about the annotations\n",
        "print(f\"ğŸ“š Manual annotations loaded for {len(MANUAL_ANNOTATIONS)} examples\")\n",
        "\n",
        "# Count entity types coverage\n",
        "entity_type_coverage = {}\n",
        "for example_name, annotations in MANUAL_ANNOTATIONS.items():\n",
        "    for entity_type, entities in annotations.items():\n",
        "        if entities:  # Only count non-empty entity list\n",
        "            if entity_type not in entity_type_coverage:\n",
        "                entity_type_coverage[entity_type] = 0\n",
        "            entity_type_coverage[entity_type] += 1\n",
        "\n",
        "print(f\"\\nğŸ·ï¸ Entity Type Coverage Across {len(MANUAL_ANNOTATIONS)} Examples:\")\n",
        "for entity_type in LEGAL_ENTITY_TYPES:\n",
        "    count = entity_type_coverage.get(entity_type, 0)\n",
        "    coverage_pct = (count / len(MANUAL_ANNOTATIONS)) * 100\n",
        "    print(\n",
        "        f\"  {entity_type}: {count}/{len(MANUAL_ANNOTATIONS)} examples ({coverage_pct:.0f}%)\"\n",
        "    )\n",
        "\n",
        "# Check if all entity types are covered\n",
        "missing_types = [\n",
        "    et\n",
        "    for et in LEGAL_ENTITY_TYPES\n",
        "    if et not in entity_type_coverage or entity_type_coverage[et] == 0\n",
        "]\n",
        "if missing_types:\n",
        "    print(f\"\\nâš ï¸ Missing entity types: {missing_types}\")\n",
        "else:\n",
        "    print(\n",
        "        f\"\\nâœ… All {len(LEGAL_ENTITY_TYPES)} entity types are covered in the manual examples!\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ground_truth_data():\n",
        "    \"\"\"Load ground truth data from LabelStudio annotations\"\"\"\n",
        "    print(\"ğŸ“‚ Loading LabelStudio annotations...\")\n",
        "    \n",
        "    # Create reverse mapping from BIO labels to Serbian labels\n",
        "    label_to_serbian = {label: serbian_label for serbian_label, label in GLINER_TO_GT_MAPPING.items()}\n",
        "    print(f\"ğŸ”„ Created mapping for {len(label_to_serbian)} entity types\")\n",
        "    \n",
        "    try:\n",
        "        with open(LABELSTUDIO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            labelstudio_data = json.load(f)\n",
        "        print(f\"âœ… Loaded {len(labelstudio_data)} annotated documents\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: {LABELSTUDIO_JSON_PATH} not found!\")\n",
        "        return []\n",
        "    \n",
        "    ground_truth_examples = []\n",
        "    entity_types = set()\n",
        "    bio_labels_found = set()\n",
        "    \n",
        "    for item in tqdm(labelstudio_data, desc=\"Loading ground truth\"):\n",
        "        file_path = item.get(\"file_upload\", \"\")\n",
        "        \n",
        "        # Load text file\n",
        "        if \"/\" in file_path:\n",
        "            filename = file_path.split(\"/\")[-1]\n",
        "        else:\n",
        "            filename = file_path\n",
        "        \n",
        "        full_path = Path(JUDGMENTS_DIR) / filename\n",
        "        \n",
        "        if not full_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                text_content = f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading {full_path}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Extract entities\n",
        "        annotations = item.get(\"annotations\", [])\n",
        "        for annotation in annotations:\n",
        "            entities = []\n",
        "            result = annotation.get(\"result\", [])\n",
        "            \n",
        "            for res in result:\n",
        "                if res.get(\"type\") == \"labels\":\n",
        "                    value = res[\"value\"]\n",
        "                    start = value[\"start\"]\n",
        "                    end = value[\"end\"]\n",
        "                    labels = value[\"labels\"]\n",
        "                    \n",
        "                    for bio_label in labels:\n",
        "                        bio_labels_found.add(bio_label)\n",
        "                        \n",
        "                        # Convert BIO label to Serbian label\n",
        "                        serbian_label = label_to_serbian.get(bio_label)\n",
        "                        if serbian_label:\n",
        "                            entity_types.add(serbian_label)\n",
        "                            entities.append({\n",
        "                                'text': text_content[start:end],\n",
        "                                'label': serbian_label,  # Use Serbian label\n",
        "                                'start': start,\n",
        "                                'end': end,\n",
        "                                'bio_label': bio_label  # Keep original for reference\n",
        "                            })\n",
        "                        else:\n",
        "                            print(f\"âš ï¸ Warning: Unknown BIO label '{bio_label}' - skipping\")\n",
        "            \n",
        "            if entities:\n",
        "                ground_truth_examples.append({\n",
        "                    'text': text_content,\n",
        "                    'entities': entities,\n",
        "                    'file_path': file_path\n",
        "                })\n",
        "    \n",
        "    print(f\"âœ… Loaded {len(ground_truth_examples)} examples with ground truth entities\")\n",
        "    print(f\"ğŸ·ï¸ Found Serbian entity types: {sorted(entity_types)}\")\n",
        "    print(f\"ğŸ”¤ Found BIO labels: {sorted(bio_labels_found)}\")\n",
        "    \n",
        "    # Show entity distribution\n",
        "    entity_counts = Counter()\n",
        "    for example in ground_truth_examples:\n",
        "        for entity in example['entities']:\n",
        "            entity_counts[entity['label']] += 1\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Ground Truth Statistics:\")\n",
        "    print(f\"  ğŸ“„ Total examples: {len(ground_truth_examples)}\")\n",
        "    print(f\"  ğŸ·ï¸ Entity types: {len(entity_types)}\")\n",
        "    print(f\"\\nğŸ“ˆ Entity Distribution:\")\n",
        "    for entity_type, count in entity_counts.most_common():\n",
        "        print(f\"  {entity_type}: {count}\")\n",
        "    \n",
        "    return ground_truth_examples\n",
        "\n",
        "# Load ground truth data\n",
        "ground_truth_examples = load_ground_truth_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_manual_few_shot_examples():\n",
        "    \"\"\"Load manual few-shot examples from files and annotations\"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    for filename, annotations in MANUAL_ANNOTATIONS.items():\n",
        "        # Load text file\n",
        "        file_path = Path(FEW_SHOT_EXAMPLES_DIR) / filename\n",
        "        \n",
        "        if not file_path.exists():\n",
        "            print(f\"âš ï¸ Warning: {file_path} not found, skipping...\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading {file_path}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Convert annotations to GLiNER format: {\"text\": text, \"entities\": [{\"start\": x, \"end\": y, \"label\": \"LABEL\"}]}\n",
        "        entities = []\n",
        "        \n",
        "        for label, entity_texts in annotations.items():\n",
        "            # Convert Serbian label to BIO label for GLiNER\n",
        "            serbian_label = GLINER_TO_GT_MAPPING.get(label)\n",
        "            \n",
        "            for entity_text in entity_texts:\n",
        "                if entity_text:  # Skip empty entities\n",
        "                    # Find entity position in text\n",
        "                    start_pos = text.find(entity_text)\n",
        "                    if start_pos != -1:\n",
        "                        end_pos = start_pos + len(entity_text)\n",
        "                        entities.append({\n",
        "                            \"start\": start_pos,\n",
        "                            \"end\": end_pos,\n",
        "                            \"label\": serbian_label\n",
        "                        })\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Warning: Entity '{entity_text}' not found in {filename}\")\n",
        "        \n",
        "        # Create GLiNER example format\n",
        "        gliner_example = {\n",
        "            \"text\": text,\n",
        "            \"entities\": entities\n",
        "        }\n",
        "        \n",
        "        examples.append(gliner_example)\n",
        "        print(f\"âœ… {filename}: {len(entities)} entities found\")\n",
        "    \n",
        "    print(f\"ğŸ“š Loaded {len(examples)} manual few-shot examples in GLiNER format\")\n",
        "    return examples\n",
        "\n",
        "# Load manual few-shot examples\n",
        "manual_examples = load_manual_few_shot_examples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GLiNEREvaluator:\n",
        "    \"\"\"GLiNER NER Evaluator for Serbian Legal Documents\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"knowledgator/gliner-bi-large-v1.0\", confidence_threshold: float = 0.3):\n",
        "        print(f\"ğŸŒŸ Initializing GLiNER Evaluator: {model_name}\")\n",
        "        \n",
        "        try:\n",
        "            # Load GLiNER model\n",
        "            self.model = GLiNER.from_pretrained(model_name)\n",
        "            \n",
        "            # Enable GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                self.model = self.model.cuda()\n",
        "                print(f\"ğŸš€ GPU enabled: {torch.cuda.get_device_name(0)}\")\n",
        "            else:\n",
        "                print(\"âš ï¸ GPU not available, using CPU\")\n",
        "            \n",
        "            self.model_name = model_name\n",
        "            self.confidence_threshold = confidence_threshold\n",
        "            \n",
        "            print(f\"âœ… GLiNER model loaded successfully\")\n",
        "            print(f\"ğŸ¯ Confidence threshold: {confidence_threshold}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading GLiNER model {model_name}: {e}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def predict_entities(self, text: str, entity_types: List[str], examples: Optional[List] = None) -> List[Dict]:\n",
        "        \"\"\"Predict entities using GLiNER zero-shot or few-shot approach\"\"\"\n",
        "        if self.model is None:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Determine method based on examples\n",
        "            method = \"gliner_few_shot\" if examples else \"gliner_zero_shot\"\n",
        "            \n",
        "            # Use correct GLiNER API\n",
        "            if examples:\n",
        "                # Few-shot prediction with examples in (text, {label: [entities]}) format\n",
        "                entities = self.model.predict_entities(\n",
        "                    text,\n",
        "                    labels=entity_types,\n",
        "                    threshold=self.confidence_threshold,\n",
        "                    few_shot_examples=examples\n",
        "                )\n",
        "            else:\n",
        "                # Zero-shot prediction\n",
        "                entities = self.model.predict_entities(\n",
        "                    text,\n",
        "                    labels=entity_types,\n",
        "                    threshold=self.confidence_threshold\n",
        "                )\n",
        "            \n",
        "            # Convert to our format\n",
        "            formatted_entities = []\n",
        "            for entity in entities:\n",
        "                formatted_entities.append({\n",
        "                    \"text\": entity[\"text\"],\n",
        "                    \"label\": entity[\"label\"],\n",
        "                    \"start\": entity[\"start\"],\n",
        "                    \"end\": entity[\"end\"],\n",
        "                    \"confidence\": entity[\"score\"],\n",
        "                    \"method\": method,\n",
        "                    \"model\": self.model_name\n",
        "                })\n",
        "            \n",
        "            return sorted(formatted_entities, key=lambda x: x[\"start\"])\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in GLiNER prediction: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize GLiNER evaluator\n",
        "gliner_evaluator = GLiNEREvaluator(confidence_threshold=CONFIDENCE_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Zero-Shot Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_gliner(examples, entity_types, few_shot_examples=None, method_name=\"GLiNER\"):\n",
        "    \"\"\"Evaluate GLiNER on the given examples\"\"\"\n",
        "    print(f\"\\nğŸ§ª Starting {method_name} Evaluation\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if gliner_evaluator.model is None:\n",
        "        return {\"error\": \"GLiNER model not loaded\"}\n",
        "    \n",
        "    detailed_results = []\n",
        "    prediction_counts = Counter()\n",
        "    confidence_scores = []\n",
        "    \n",
        "    print(f\"ğŸ“Š Evaluating on {len(examples)} examples...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i, example in enumerate(tqdm(examples, desc=f\"{method_name} Evaluation\")):\n",
        "        text = example[\"text\"]\n",
        "        true_entities = example[\"entities\"]\n",
        "        \n",
        "        # Get GLiNER predictions\n",
        "        pred_entities = gliner_evaluator.predict_entities(text, entity_types, examples=few_shot_examples)\n",
        "        \n",
        "        # Count predictions by type\n",
        "        for entity in pred_entities:\n",
        "            prediction_counts[entity[\"label\"]] += 1\n",
        "            confidence_scores.append(entity[\"confidence\"])\n",
        "        \n",
        "        # Store detailed results\n",
        "        detailed_results.append({\n",
        "            \"example_id\": i,\n",
        "            \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
        "            \"file_path\": example[\"file_path\"],\n",
        "            \"true_entities\": true_entities,\n",
        "            \"pred_entities\": pred_entities,\n",
        "            \"true_count\": len(true_entities),\n",
        "            \"pred_count\": len(pred_entities)\n",
        "        })\n",
        "    \n",
        "    end_time = time.time()\n",
        "    evaluation_time = end_time - start_time\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_true = sum(len(r[\"true_entities\"]) for r in detailed_results)\n",
        "    total_pred = sum(len(r[\"pred_entities\"]) for r in detailed_results)\n",
        "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0\n",
        "    \n",
        "    print(f\"\\nğŸ“Š {method_name} Prediction Statistics:\")\n",
        "    for label, count in prediction_counts.most_common():\n",
        "        print(f\"  {label}: {count}\")\n",
        "    \n",
        "    results = {\n",
        "        \"method\": method_name,\n",
        "        \"model_name\": gliner_evaluator.model_name,\n",
        "        \"confidence_threshold\": gliner_evaluator.confidence_threshold,\n",
        "        \"detailed_results\": detailed_results,\n",
        "        \"total_true_entities\": total_true,\n",
        "        \"total_pred_entities\": total_pred,\n",
        "        \"prediction_counts\": dict(prediction_counts),\n",
        "        \"examples_evaluated\": len(examples),\n",
        "        \"avg_confidence\": avg_confidence,\n",
        "        \"evaluation_time\": evaluation_time,\n",
        "        \"entities_per_second\": total_pred / evaluation_time if evaluation_time > 0 else 0\n",
        "    }\n",
        "    \n",
        "    print(f\"âœ… {method_name} evaluation complete!\")\n",
        "    print(f\"  ğŸ“Š True entities: {total_true}\")\n",
        "    print(f\"  ğŸ¤– Predicted entities: {total_pred}\")\n",
        "    print(f\"  âš¡ Average confidence: {avg_confidence:.3f}\")\n",
        "    print(f\"  â±ï¸ Evaluation time: {evaluation_time:.2f}s\")\n",
        "    print(f\"  ğŸš€ Entities/second: {results['entities_per_second']:.2f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run zero-shot evaluation\n",
        "zero_shot_results = evaluate_gliner(\n",
        "    ground_truth_examples, \n",
        "    LEGAL_ENTITY_TYPES, \n",
        "    few_shot_examples=None, \n",
        "    method_name=\"Zero-Shot GLiNER\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Few-Shot Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity check: Verify few-shot examples change predictions\n",
        "if len(manual_examples) > 0 and len(ground_truth_examples) > 0:\n",
        "    test_text = ground_truth_examples[0][\"text\"]\n",
        "    \n",
        "    # Zero-shot prediction\n",
        "    zero_shot_pred = gliner_evaluator.predict_entities(test_text, LEGAL_ENTITY_TYPES)\n",
        "    \n",
        "    # Few-shot prediction\n",
        "    few_shot_pred = gliner_evaluator.predict_entities(test_text, LEGAL_ENTITY_TYPES, examples=manual_examples)\n",
        "    \n",
        "    if zero_shot_pred != few_shot_pred:\n",
        "        print(\"âœ… Few-shot examples are working! Predictions differ.\")\n",
        "        print(f\"Zero-shot found {len(zero_shot_pred)} entities\")\n",
        "        print(f\"Few-shot found {len(few_shot_pred)} entities\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Warning: Few-shot predictions identical to zero-shot. Check examples format.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Cannot run sanity check - missing examples or ground truth data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run few-shot evaluation using manual examples\n",
        "few_shot_results = evaluate_gliner(\n",
        "    ground_truth_examples, \n",
        "    LEGAL_ENTITY_TYPES, \n",
        "    few_shot_examples=manual_examples,\n",
        "    method_name=\"Few-Shot GLiNER\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_results(zero_shot_results, few_shot_results):\n",
        "    \"\"\"Compare zero-shot and few-shot results\"\"\"\n",
        "    print(\"\\nğŸ“Š COMPARISON: Zero-Shot vs Few-Shot GLiNER\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Overall statistics\n",
        "    print(f\"\\nğŸ“ˆ Overall Performance:\")\n",
        "    print(f\"  Zero-Shot:\")\n",
        "    print(f\"    ğŸ¤– Predicted entities: {zero_shot_results['total_pred_entities']}\")\n",
        "    print(f\"    âš¡ Avg confidence: {zero_shot_results['avg_confidence']:.3f}\")\n",
        "    print(f\"    â±ï¸ Time: {zero_shot_results['evaluation_time']:.2f}s\")\n",
        "    \n",
        "    print(f\"  Few-Shot:\")\n",
        "    print(f\"    ğŸ¤– Predicted entities: {few_shot_results['total_pred_entities']}\")\n",
        "    print(f\"    âš¡ Avg confidence: {few_shot_results['avg_confidence']:.3f}\")\n",
        "    print(f\"    â±ï¸ Time: {few_shot_results['evaluation_time']:.2f}s\")\n",
        "    \n",
        "    # Entity type comparison\n",
        "    print(f\"\\nğŸ·ï¸ Entity Type Predictions:\")\n",
        "    all_labels = set(zero_shot_results['prediction_counts'].keys()) | set(few_shot_results['prediction_counts'].keys())\n",
        "    \n",
        "    for label in sorted(all_labels):\n",
        "        zero_count = zero_shot_results['prediction_counts'].get(label, 0)\n",
        "        few_count = few_shot_results['prediction_counts'].get(label, 0)\n",
        "        diff = few_count - zero_count\n",
        "        diff_str = f\"({diff:+d})\" if diff != 0 else \"\"\n",
        "        print(f\"  {label}: {zero_count} â†’ {few_count} {diff_str}\")\n",
        "    \n",
        "    # Performance improvement\n",
        "    total_improvement = few_shot_results['total_pred_entities'] - zero_shot_results['total_pred_entities']\n",
        "    confidence_improvement = few_shot_results['avg_confidence'] - zero_shot_results['avg_confidence']\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Summary:\")\n",
        "    print(f\"  ğŸ“ˆ Entity prediction change: {total_improvement:+d}\")\n",
        "    print(f\"  âš¡ Confidence change: {confidence_improvement:+.3f}\")\n",
        "    \n",
        "    if total_improvement > 0:\n",
        "        print(f\"  âœ… Few-shot found {total_improvement} more entities than zero-shot\")\n",
        "    elif total_improvement < 0:\n",
        "        print(f\"  âš ï¸ Few-shot found {abs(total_improvement)} fewer entities than zero-shot\")\n",
        "    else:\n",
        "        print(f\"  â¡ï¸ Few-shot and zero-shot found the same number of entities\")\n",
        "\n",
        "# Compare results\n",
        "if 'zero_shot_results' in locals() and 'few_shot_results' in locals():\n",
        "    compare_results(zero_shot_results, few_shot_results)\n",
        "else:\n",
        "    print(\"âš ï¸ Results not available for comparison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_results_visualization(zero_shot_results, few_shot_results):\n",
        "    \"\"\"Create visualizations comparing zero-shot and few-shot results\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('GLiNER Zero-Shot vs Few-Shot Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Entity count comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    methods = ['Zero-Shot', 'Few-Shot']\n",
        "    entity_counts = [zero_shot_results['total_pred_entities'], few_shot_results['total_pred_entities']]\n",
        "    colors = ['skyblue', 'lightcoral']\n",
        "    \n",
        "    bars1 = ax1.bar(methods, entity_counts, color=colors)\n",
        "    ax1.set_title('Total Predicted Entities')\n",
        "    ax1.set_ylabel('Number of Entities')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars1, entity_counts):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
        "                str(count), ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Confidence comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    confidence_scores = [zero_shot_results['avg_confidence'], few_shot_results['avg_confidence']]\n",
        "    \n",
        "    bars2 = ax2.bar(methods, confidence_scores, color=colors)\n",
        "    ax2.set_title('Average Confidence Score')\n",
        "    ax2.set_ylabel('Confidence')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, conf in zip(bars2, confidence_scores):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 3. Entity type comparison\n",
        "    ax3 = axes[1, 0]\n",
        "    all_labels = set(zero_shot_results['prediction_counts'].keys()) | set(few_shot_results['prediction_counts'].keys())\n",
        "    labels = sorted(all_labels)\n",
        "    \n",
        "    zero_counts = [zero_shot_results['prediction_counts'].get(label, 0) for label in labels]\n",
        "    few_counts = [few_shot_results['prediction_counts'].get(label, 0) for label in labels]\n",
        "    \n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax3.bar(x - width/2, zero_counts, width, label='Zero-Shot', color='skyblue')\n",
        "    ax3.bar(x + width/2, few_counts, width, label='Few-Shot', color='lightcoral')\n",
        "    \n",
        "    ax3.set_title('Predictions by Entity Type')\n",
        "    ax3.set_ylabel('Number of Predictions')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels([label.replace(' ili ', '\\n') for label in labels], rotation=45, ha='right')\n",
        "    ax3.legend()\n",
        "    \n",
        "    # 4. Performance metrics\n",
        "    ax4 = axes[1, 1]\n",
        "    metrics = ['Entities/sec', 'Time (s)']\n",
        "    zero_metrics = [zero_shot_results['entities_per_second'], zero_shot_results['evaluation_time']]\n",
        "    few_metrics = [few_shot_results['entities_per_second'], few_shot_results['evaluation_time']]\n",
        "    \n",
        "    x = np.arange(len(metrics))\n",
        "    ax4.bar(x - width/2, zero_metrics, width, label='Zero-Shot', color='skyblue')\n",
        "    ax4.bar(x + width/2, few_metrics, width, label='Few-Shot', color='lightcoral')\n",
        "    \n",
        "    ax4.set_title('Performance Metrics')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(metrics)\n",
        "    ax4.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "if 'zero_shot_results' in locals() and 'few_shot_results' in locals():\n",
        "    create_results_visualization(zero_shot_results, few_shot_results)\n",
        "else:\n",
        "    print(\"âš ï¸ Results not available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Debug: Few-Shot Examples Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check few-shot examples format\n",
        "print(\"ğŸ” DEBUG: Few-Shot Examples Format\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'manual_examples' in locals() and len(manual_examples) > 0:\n",
        "    print(f\"ğŸ“š Total few-shot examples: {len(manual_examples)}\")\n",
        "    print(f\"ğŸ“ Type of manual_examples: {type(manual_examples)}\")\n",
        "    \n",
        "    # Show first example in detail\n",
        "    print(f\"\\nğŸ” First example structure:\")\n",
        "    first_example = manual_examples[0]\n",
        "    print(f\"Type: {type(first_example)}\")\n",
        "    \n",
        "    if isinstance(first_example, tuple):\n",
        "        print(f\"Tuple length: {len(first_example)}\")\n",
        "        print(f\"First element type: {type(first_example[0])}\")\n",
        "        print(f\"Second element type: {type(first_example[1])}\")\n",
        "        \n",
        "        # Show text (truncated)\n",
        "        text = first_example[0]\n",
        "        print(f\"\\nğŸ“„ Text (first 200 chars): {text[:200]}...\")\n",
        "        \n",
        "        # Show entities structure\n",
        "        entities = first_example[1]\n",
        "        print(f\"\\nğŸ·ï¸ Entities structure:\")\n",
        "        print(f\"Type: {type(entities)}\")\n",
        "        \n",
        "        if isinstance(entities, dict):\n",
        "            print(f\"Keys: {list(entities.keys())}\")\n",
        "            for key, values in list(entities.items())[:3]:  # Show first 3 entity types\n",
        "                print(f\"  {key}: {values} (type: {type(values)})\")\n",
        "        \n",
        "        print(f\"\\nâœ… GLiNER Expected Format:\")\n",
        "        print(f\"   (text, {{label: [entity_texts]}})\")\n",
        "        print(f\"\\nâœ… Current Format:\")\n",
        "        print(f\"   {type(first_example)} with {type(first_example[1])}\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"âŒ Unexpected format: {first_example}\")\n",
        "        \n",
        "    # Test if GLiNER would accept this format\n",
        "    print(f\"\\nğŸ§ª Testing GLiNER compatibility...\")\n",
        "    try:\n",
        "        test_text = \"Test text for GLiNER.\"\n",
        "        test_pred = gliner_evaluator.predict_entities(\n",
        "            test_text, \n",
        "            [\"sud ili tribunal\"], \n",
        "            examples=manual_examples[:1]  # Use just first example\n",
        "        )\n",
        "        print(f\"âœ… GLiNER accepted the format!\")\n",
        "        print(f\"ğŸ” Test prediction: {test_pred}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GLiNER format error: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ No manual_examples found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Detailed Entity-Level Classification Report (No BIO Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate detailed classification report\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def generate_classification_report(results, method_name):\n",
        "    \"\"\"Generate detailed entity-level classification report for GLiNER results\"\"\"\n",
        "    print(f\"\\nğŸ“Š {method_name} - Detailed Entity-Level Classification Report:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if 'detailed_results' not in results:\n",
        "        print(\"âŒ No detailed results available\")\n",
        "        return\n",
        "    \n",
        "    # Collect all true and predicted labels\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "    \n",
        "    # Create BIO tags for evaluation\n",
        "    for example_result in results['detailed_results']:\n",
        "        text = example_result['text']\n",
        "        true_entities = example_result['true_entities']\n",
        "        pred_entities = example_result['pred_entities']\n",
        "        \n",
        "        # Convert to BIO format\n",
        "        text_length = len(text)\n",
        "        true_bio = ['O'] * text_length\n",
        "        pred_bio = ['O'] * text_length\n",
        "        \n",
        "        # Fill true BIO tags\n",
        "        for entity in true_entities:\n",
        "            start, end = entity['start'], entity['end']\n",
        "            bio_label = GLINER_TO_GT_MAPPING.get(entity['label'], entity['label'])\n",
        "            if start < text_length and end <= text_length:\n",
        "                true_bio[start] = f'B-{bio_label}'\n",
        "                for i in range(start + 1, min(end, text_length)):\n",
        "                    true_bio[i] = f'I-{bio_label}'\n",
        "        \n",
        "        # Fill predicted BIO tags\n",
        "        for entity in pred_entities:\n",
        "            start, end = entity['start'], entity['end']\n",
        "            bio_label = GLINER_TO_GT_MAPPING.get(entity['label'], entity['label'])\n",
        "            if start < text_length and end <= text_length:\n",
        "                pred_bio[start] = f'B-{bio_label}'\n",
        "                for i in range(start + 1, min(end, text_length)):\n",
        "                    pred_bio[i] = f'I-{bio_label}'\n",
        "        \n",
        "        all_true_labels.extend(true_bio)\n",
        "        all_pred_labels.extend(pred_bio)\n",
        "    \n",
        "    # Generate classification report\n",
        "    if len(all_true_labels) > 0 and len(all_pred_labels) > 0:\n",
        "        report = classification_report(\n",
        "            all_true_labels, \n",
        "            all_pred_labels, \n",
        "            zero_division=0,\n",
        "            digits=2\n",
        "        )\n",
        "        print(report)\n",
        "        \n",
        "        # Entity-level statistics\n",
        "        entity_stats = {}\n",
        "        for label in set(all_true_labels + all_pred_labels):\n",
        "            if label != 'O':\n",
        "                true_count = all_true_labels.count(label)\n",
        "                pred_count = all_pred_labels.count(label)\n",
        "                entity_stats[label] = {'true': true_count, 'pred': pred_count}\n",
        "        \n",
        "        print(f\"\\nğŸ“ˆ Entity-Level Statistics:\")\n",
        "        for label, stats in sorted(entity_stats.items()):\n",
        "            print(f\"  {label:20} True: {stats['true']:4d}  Pred: {stats['pred']:4d}\")\n",
        "    else:\n",
        "        print(\"âŒ No labels found for classification report\")\n",
        "\n",
        "# Generate reports for both methods\n",
        "if 'zero_shot_results' in locals():\n",
        "    generate_classification_report(zero_shot_results, \"Zero-Shot GLiNER\")\n",
        "\n",
        "if 'few_shot_results' in locals():\n",
        "    generate_classification_report(few_shot_results, \"Few-Shot GLiNER\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Proper Entity-Level Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_entity_classification_report(results, method_name):\n",
        "    \"\"\"Generate proper entity-level classification report (no BIO tagging)\"\"\"\n",
        "    print(f\"\\nğŸ“Š {method_name} - Entity-Level Classification Report:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if 'detailed_results' not in results:\n",
        "        print(\"âŒ No detailed results available\")\n",
        "        return\n",
        "    \n",
        "    # Entity-level evaluation\n",
        "    entity_stats = {}\n",
        "    total_true = 0\n",
        "    total_pred = 0\n",
        "    exact_matches = 0\n",
        "    \n",
        "    # Collect entity-level statistics\n",
        "    for example_result in results['detailed_results']:\n",
        "        true_entities = example_result['true_entities']\n",
        "        pred_entities = example_result['pred_entities']\n",
        "        \n",
        "        total_true += len(true_entities)\n",
        "        total_pred += len(pred_entities)\n",
        "        \n",
        "        # Count by entity type\n",
        "        for entity in true_entities:\n",
        "            label = entity['label']\n",
        "            if label not in entity_stats:\n",
        "                entity_stats[label] = {'true': 0, 'pred': 0, 'correct': 0}\n",
        "            entity_stats[label]['true'] += 1\n",
        "        \n",
        "        for entity in pred_entities:\n",
        "            label = entity['label']\n",
        "            if label not in entity_stats:\n",
        "                entity_stats[label] = {'true': 0, 'pred': 0, 'correct': 0}\n",
        "            entity_stats[label]['pred'] += 1\n",
        "        \n",
        "        # Check for exact matches (same span and label)\n",
        "        for true_entity in true_entities:\n",
        "            for pred_entity in pred_entities:\n",
        "                if (true_entity['start'] == pred_entity['start'] and \n",
        "                    true_entity['end'] == pred_entity['end'] and \n",
        "                    true_entity['label'] == pred_entity['label']):\n",
        "                    entity_stats[true_entity['label']]['correct'] += 1\n",
        "                    exact_matches += 1\n",
        "                    break\n",
        "    \n",
        "    # Calculate metrics for each entity type\n",
        "    print(f\"{'Entity Type':<30} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    num_types = 0\n",
        "    \n",
        "    for label in sorted(entity_stats.keys()):\n",
        "        stats = entity_stats[label]\n",
        "        \n",
        "        # Calculate precision, recall, F1\n",
        "        precision = stats['correct'] / stats['pred'] if stats['pred'] > 0 else 0.0\n",
        "        recall = stats['correct'] / stats['true'] if stats['true'] > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        print(f\"{label:<30} {precision:<10.2f} {recall:<10.2f} {f1:<10.2f} {stats['true']:<10d}\")\n",
        "        \n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "        num_types += 1\n",
        "    \n",
        "    # Overall metrics\n",
        "    print(\"=\" * 80)\n",
        "    overall_precision = exact_matches / total_pred if total_pred > 0 else 0.0\n",
        "    overall_recall = exact_matches / total_true if total_true > 0 else 0.0\n",
        "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "    \n",
        "    print(f\"{'Micro Avg':<30} {overall_precision:<10.2f} {overall_recall:<10.2f} {overall_f1:<10.2f} {total_true:<10d}\")\n",
        "    \n",
        "    if num_types > 0:\n",
        "        macro_precision = total_precision / num_types\n",
        "        macro_recall = total_recall / num_types\n",
        "        macro_f1 = total_f1 / num_types\n",
        "        print(f\"{'Macro Avg':<30} {macro_precision:<10.2f} {macro_recall:<10.2f} {macro_f1:<10.2f} {total_true:<10d}\")\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Summary:\")\n",
        "    print(f\"  Total True Entities: {total_true}\")\n",
        "    print(f\"  Total Predicted Entities: {total_pred}\")\n",
        "    print(f\"  Exact Matches: {exact_matches}\")\n",
        "    accuracy = exact_matches/max(total_true, total_pred) if max(total_true, total_pred) > 0 else 0.0\n",
        "    print(f\"  Overall Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Generate proper entity-level reports\n",
        "if 'zero_shot_results' in locals():\n",
        "    generate_entity_classification_report(zero_shot_results, \"Zero-Shot GLiNER\")\n",
        "\n",
        "if 'few_shot_results' in locals():\n",
        "    generate_entity_classification_report(few_shot_results, \"Few-Shot GLiNER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results for further analysis\n",
        "results_summary = {\n",
        "    \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"model_name\": gliner_evaluator.model_name,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLD,\n",
        "    \"entity_types\": LEGAL_ENTITY_TYPES,\n",
        "    \"manual_examples_count\": len(manual_examples),\n",
        "    \"ground_truth_examples_count\": len(ground_truth_examples),\n",
        "    \"zero_shot\": zero_shot_results if 'zero_shot_results' in locals() else None,\n",
        "    \"few_shot\": few_shot_results if 'few_shot_results' in locals() else None\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "output_file = \"gliner_evaluation_results.json\"\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Results saved to {output_file}\")\n",
        "print(f\"\\nğŸ‰ GLiNER evaluation complete!\")\n",
        "print(f\"ğŸ“Š Evaluated {len(ground_truth_examples)} documents\")\n",
        "print(f\"ğŸ·ï¸ Tested {len(LEGAL_ENTITY_TYPES)} entity types\")\n",
        "print(f\"ğŸ“š Used {len(manual_examples)} manual few-shot examples\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
