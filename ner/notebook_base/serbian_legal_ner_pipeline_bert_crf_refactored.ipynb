{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal NER Pipeline with BERT-CRF - Refactored\n",
        "\n",
        "This notebook demonstrates the BERT-CRF approach for Serbian Legal NER using shared modules.\n",
        "BERT-CRF combines BERT embeddings with a Conditional Random Field (CRF) layer for better sequence modeling.\n",
        "\n",
        "## Key Features:\n",
        "- **BERT Embeddings**: Contextual word representations\n",
        "- **CRF Layer**: Enforces valid BIO sequence constraints\n",
        "- **Improved Performance**: Better handling of sequence dependencies\n",
        "- **Entity Boundary Detection**: More accurate entity span detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages including pytorch-crf for CRF layer\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the shared modules to path\n",
        "sys.path.append('/content/drive/MyDrive/NER_Master/ner/')\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment, get_default_training_args,\n",
        "    \n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data, \n",
        "    analyze_labelstudio_data, validate_bio_examples,\n",
        "    \n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "    \n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "    \n",
        "    \n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification, Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Install pytorch-crf if not available\n",
        "try:\n",
        "    from torchcrf import CRF\n",
        "except ImportError:\n",
        "    !pip install pytorch-crf\n",
        "    from torchcrf import CRF\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)\n",
        "print(f\"üîß Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment and paths for Google Colab\n",
        "env_setup = setup_environment(use_local=False, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration for BERT-CRF\n",
        "MODEL_NAME = \"classla/bcms-bertic\"\n",
        "# BERT-CRF Configuration (notebook-specific)\n",
        "BERT_CRF_CONFIG = {\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"bert_lr\": 3e-5,\n",
        "    \"classifier_lr\": 1e-4,\n",
        "    \"crf_lr\": 1e-3,\n",
        "    \"max_length\": 512,\n",
        "    \"stride\": 128,\n",
        "    \"num_train_epochs\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"per_device_eval_batch_size\": 4,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"weight_decay\": 0.01\n",
        "}\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_bert_crf\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß BERT-CRF Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"  BIO labels: {len(BIO_LABELS)}\")\n",
        "print(f\"  Dropout rate: {BERT_CRF_CONFIG['dropout_rate']}\")\n",
        "print(f\"  BERT LR: {BERT_CRF_CONFIG['bert_lr']}\")\n",
        "print(f\"  CRF LR: {BERT_CRF_CONFIG['crf_lr']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze LabelStudio data\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "    \n",
        "    # Convert to BIO format\n",
        "    converter = LabelStudioToBIOConverter(\n",
        "        judgments_dir=paths['judgments_dir'],\n",
        "        labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        "    )\n",
        "    \n",
        "    bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "    print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "    \n",
        "    # Validate BIO examples\n",
        "    valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "    print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
        "    raise Exception(\"Data loading failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Preparation and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "# Split dataset\n",
        "train_examples, val_examples, test_examples = split_dataset(\n",
        "    prepared_examples, test_size=0.2, val_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset split:\")\n",
        "print(f\"  Training: {len(train_examples)} examples\")\n",
        "print(f\"  Validation: {len(val_examples)} examples\")\n",
        "print(f\"  Test: {len(test_examples)} examples\")\n",
        "print(f\"  Total labels: {ner_dataset.get_num_labels()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. BERT-CRF Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT-CRF Functions to Add to BERT-CRF Notebook\n",
        "# Copy this code into a new cell in the BERT-CRF notebook after the imports\n",
        "\n",
        "try:\n",
        "    from torchcrf import CRF\n",
        "except ImportError:\n",
        "    !pip install pytorch-crf\n",
        "    from torchcrf import CRF\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer\n",
        "\n",
        "# BERT-CRF Model Implementation (notebook-specific)\n",
        "class BertCrfForTokenClassification(nn.Module):\n",
        "    \"\"\"BERT model with CRF layer for token classification\"\"\"\n",
        "    \n",
        "    def __init__(self, bert_model, num_labels, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        if labels is not None:\n",
        "            # Create mask for CRF (exclude padding and special tokens)\n",
        "            # Use attention mask directly - CRF requires first timestep to be valid\n",
        "            mask = attention_mask == 1\n",
        "            \n",
        "            # Replace -100 with 0 for CRF computation\n",
        "            crf_labels = labels.clone()\n",
        "            crf_labels[labels == -100] = 0\n",
        "            \n",
        "            # Ensure first timestep is valid for CRF (required by torchcrf)\n",
        "            # The first token (CLS) should always be valid but ignored in loss\n",
        "            \n",
        "            # Compute CRF loss\n",
        "            loss = -self.crf(logits, crf_labels, mask=mask, reduction='mean')\n",
        "            return {'loss': loss, 'logits': logits}\n",
        "        else:\n",
        "            # Decode best path\n",
        "            mask = attention_mask == 1\n",
        "            predictions = self.crf.decode(logits, mask=mask)\n",
        "            return {'logits': logits, 'predictions': predictions}\n",
        "\n",
        "def create_bert_crf_model(model_name, num_labels, dropout_rate=0.1):\n",
        "    \"\"\"Create BERT-CRF model\"\"\"\n",
        "    from transformers import AutoModel\n",
        "    \n",
        "    # Load BERT model (without classification head)\n",
        "    bert_model = AutoModel.from_pretrained(model_name)\n",
        "    \n",
        "    # Create BERT-CRF model\n",
        "    model = BertCrfForTokenClassification(\n",
        "        bert_model=bert_model,\n",
        "        num_labels=num_labels,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",

        "print(\"‚úÖ BERT-CRF classes and functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create BERT-CRF model\n",
        "model = create_bert_crf_model(\n",
        "    model_name=MODEL_NAME,\n",
        "    num_labels=ner_dataset.get_num_labels(),\n",
        "    dropout_rate=BERT_CRF_CONFIG['dropout_rate']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"‚úÖ BERT-CRF model created successfully\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize datasets with sliding window\n",
        "print(\"üî§ Tokenizing datasets...\")\n",
        "\n",
        "train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    train_examples, tokenizer, ner_dataset.label_to_id, \n",
        "    max_length=BERT_CRF_CONFIG['max_length'], \n",
        "    stride=BERT_CRF_CONFIG['stride']\n",
        ")\n",
        "\n",
        "val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=BERT_CRF_CONFIG['max_length'], \n",
        "    stride=BERT_CRF_CONFIG['stride']\n",
        ")\n",
        "\n",
        "test_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "    test_examples, tokenizer, ner_dataset.label_to_id,\n",
        "    max_length=BERT_CRF_CONFIG['max_length'], \n",
        "    stride=BERT_CRF_CONFIG['stride']\n",
        ")\n",
        "\n",
        "# Create HuggingFace datasets\n",
        "train_dataset, val_dataset, test_dataset = create_huggingface_datasets(\n",
        "    train_tokenized, val_tokenized, test_tokenized\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. BERT-CRF Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training arguments for BERT-CRF\n",
        "training_args = create_training_arguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=BERT_CRF_CONFIG['num_train_epochs'],\n",
        "    per_device_train_batch_size=BERT_CRF_CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=BERT_CRF_CONFIG['per_device_eval_batch_size'],\n",
        "    learning_rate=BERT_CRF_CONFIG['bert_lr'],  # Use BERT learning rate\n",
        "    warmup_steps=BERT_CRF_CONFIG['warmup_steps'],\n",
        "    weight_decay=BERT_CRF_CONFIG['weight_decay'],\n",
        "    logging_steps=50,\n",
        "    eval_steps=100,\n",
        "    save_steps=500,\n",
        "    early_stopping_patience=3\n",
        ")\n",
        "\n",
        "# Create BERT-CRF trainer using shared function\n",
        "trainer = create_trainer(\n",
        "    model=model,\n",
        "    training_args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    id_to_label=ner_dataset.id_to_label,\n",
        "    early_stopping_patience=3\n",
        ")\n",
        "\n",
        "print(\"üèãÔ∏è  BERT-CRF trainer created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start BERT-CRF training\n",
        "print(\"üöÄ Starting BERT-CRF training...\")\n",
        "print(\"‚ö° This may take longer than standard BERT due to CRF layer\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"üíæ Saving BERT-CRF model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save model info with CRF-specific details\n",
        "save_model_info(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    model_name=MODEL_NAME,\n",
        "    model_type=\"bert_crf\",\n",
        "    num_labels=ner_dataset.get_num_labels(),\n",
        "    id_to_label=ner_dataset.id_to_label,\n",
        "    label_to_id=ner_dataset.label_to_id,\n",
        "    training_args=training_args,\n",
        "    additional_info={\n",
        "        \"dropout_rate\": BERT_CRF_CONFIG['dropout_rate'],\n",
        "        \"bert_lr\": BERT_CRF_CONFIG['bert_lr'],\n",
        "        \"crf_lr\": BERT_CRF_CONFIG['crf_lr'],\n",
        "        \"uses_crf\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ BERT-CRF training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate BERT-CRF model on test set using shared function\n",
        "print(\"üìä Evaluating BERT-CRF model on test set...\")\n",
        "\n",
        "test_results = detailed_evaluation(\n",
        "    trainer=trainer,\n",
        "    dataset=test_dataset,\n",
        "    dataset_name=\"Test (BERT-CRF)\",\n",
        "    id_to_label=ner_dataset.id_to_label\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training History and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(trainer)\n",
        "\n",
        "# Plot entity distribution\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "plot_entity_distribution(label_stats['entity_counts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéØ BERT-CRF FINAL SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {MODEL_NAME} + CRF\")\n",
        "print(f\"Training examples: {len(train_examples)}\")\n",
        "print(f\"Validation examples: {len(val_examples)}\")\n",
        "print(f\"Test examples: {len(test_examples)}\")\n",
        "print(f\"Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"BIO labels: {len(BIO_LABELS)}\")\n",
        "print(f\"\\nBERT-CRF Configuration:\")\n",
        "print(f\"  Dropout rate: {BERT_CRF_CONFIG['dropout_rate']}\")\n",
        "print(f\"  BERT learning rate: {BERT_CRF_CONFIG['bert_lr']}\")\n",
        "print(f\"  CRF learning rate: {BERT_CRF_CONFIG['crf_lr']}\")\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
        "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\n‚úÖ BERT-CRF pipeline completed successfully!\")\n",
        "print(\"\\nüí° CRF layer helps with:\")\n",
        "print(\"   ‚Ä¢ Valid BIO sequence constraints\")\n",
        "print(\"   ‚Ä¢ Better entity boundary detection\")\n",
        "print(\"   ‚Ä¢ Improved sequence modeling\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
