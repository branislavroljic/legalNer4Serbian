{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal Named Entity Recognition (NER) Pipeline - Class Weights 5-Fold Cross-Validation\n",
        "\n",
        "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using class weights to handle class imbalance.\n",
        "Class weights help address the imbalanced distribution of entity types in the dataset by giving more importance to underrepresented classes.\n",
        "\n",
        "## Key Features\n",
        "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
        "- **Class Weights**: Automatic calculation and application of class weights\n",
        "- **Imbalance Handling**: Better performance on minority entity classes\n",
        "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
        "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
        "- **Statistical Analysis**: Mean and standard deviation across folds\n",
        "\n",
        "## Class Weights Advantages\n",
        "- **Balanced Learning**: Addresses class imbalance in entity distribution\n",
        "- **Improved Minority Class Performance**: Better recognition of rare entity types\n",
        "- **Automatic Calculation**: Weights computed based on class frequencies\n",
        "- **No Architecture Changes**: Uses standard BERT with weighted loss\n",
        "\n",
        "## Entity Types\n",
        "- **COURT**: Court institutions\n",
        "- **DECISION_DATE**: Dates of legal decisions\n",
        "- **CASE_NUMBER**: Case identifiers\n",
        "- **CRIMINAL_ACT**: Criminal acts/charges\n",
        "- **PROSECUTOR**: Prosecutor entities\n",
        "- **DEFENDANT**: Defendant entities\n",
        "- **JUDGE**: Judge names\n",
        "- **REGISTRAR**: Court registrar\n",
        "- **SANCTION**: Sanctions/penalties\n",
        "- **SANCTION_TYPE**: Type of sanction\n",
        "- **SANCTION_VALUE**: Value/duration of sanction\n",
        "- **PROVISION**: Legal provisions\n",
        "- **PROCEDURE_COSTS**: Legal procedure costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    USE_COLAB = True\n",
        "except ImportError:\n",
        "    USE_COLAB = False\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHG6Rs8jTOo"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11kIuCNPjTOo"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ig3oNUjTOo"
      },
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the shared modules to path\n",
        "if USE_COLAB:\n",
        "    sys.path.append('/content/drive/MyDrive/NER_Master/ner/')\n",
        "else:\n",
        "    sys.path.append('../shared')\n",
        "\n",
        "import importlib\n",
        "import shared\n",
        "import shared.model_utils\n",
        "import shared.data_processing\n",
        "import shared.dataset\n",
        "import shared.evaluation\n",
        "import shared.config\n",
        "importlib.reload(shared.config)\n",
        "importlib.reload(shared.data_processing)\n",
        "importlib.reload(shared.dataset)\n",
        "importlib.reload(shared.model_utils)\n",
        "importlib.reload(shared.evaluation)\n",
        "importlib.reload(shared)\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment,\n",
        "\n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
        "    analyze_labelstudio_data, validate_bio_examples,\n",
        "\n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "\n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "\n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution\n",
        ")\n",
        "\n",
        "# Standard imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DataCollatorForTokenClassification, AutoTokenizer, Trainer\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "class_weights_imports"
      },
      "source": [
        "## 2. Class Weights Specific Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "class_weights_imports_code"
      },
      "outputs": [],
      "source": [
        "# Class weights specific imports\n",
        "from collections import Counter\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "print(\"‚úÖ Class weights specific imports loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R56QtmR7fIY2"
      },
      "source": [
        "## 3. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i3CBXt2fIY3"
      },
      "outputs": [],
      "source": [
        "# Setup environment and paths\n",
        "env_setup = setup_environment(use_local=not USE_COLAB, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"classla/bcms-bertic\"\n",
        "model_config = get_default_model_config()\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_class_weights_5fold_cv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Architecture: BERT + Class Weights\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVSwtVS1fIY3"
      },
      "source": [
        "## 4. Data Loading and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmq8DjhyfIY4"
      },
      "outputs": [],
      "source": [
        "# Load LabelStudio data\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "# Analyze the data\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAyV4gQfIY4"
      },
      "source": [
        "## 5. Data Preprocessing and BIO Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOe5xEDnfIY5"
      },
      "outputs": [],
      "source": [
        "# Convert LabelStudio data to BIO format\n",
        "converter = LabelStudioToBIOConverter(\n",
        "    judgments_dir=paths['judgments_dir'],\n",
        "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        ")\n",
        "\n",
        "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "\n",
        "# Validate BIO examples\n",
        "valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HcyAbInfIY5"
      },
      "source": [
        "## 6. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cX7CQA3fIY5"
      },
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "print(f\"üìä Dataset statistics:\")\n",
        "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
        "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
        "\n",
        "# Get label statistics\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
        "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "class_weights_calculation"
      },
      "source": [
        "## 7. Class Weights Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "class_weights_calc_code"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(examples, label_to_id):\n",
        "    \"\"\"\n",
        "    Calculate class weights based on label frequency in the training data.\n",
        "    \n",
        "    Args:\n",
        "        examples: List of training examples\n",
        "        label_to_id: Dictionary mapping labels to IDs\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Class weights tensor\n",
        "    \"\"\"\n",
        "    # Collect all labels from training examples\n",
        "    all_labels = []\n",
        "    for example in examples:\n",
        "        all_labels.extend(example['labels'])\n",
        "    \n",
        "    # Count label frequencies\n",
        "    label_counts = Counter(all_labels)\n",
        "    \n",
        "    # Get unique classes and their counts\n",
        "    classes = list(range(len(label_to_id)))\n",
        "    class_counts = [label_counts.get(label, 1) for label in classes]  # Use 1 for unseen labels\n",
        "    \n",
        "    # Calculate class weights using sklearn's balanced approach\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.array(classes),\n",
        "        y=all_labels\n",
        "    )\n",
        "    \n",
        "    # Convert to tensor\n",
        "    class_weights_tensor = torch.FloatTensor(class_weights)\n",
        "    \n",
        "    print(f\"üìä Class weights calculated:\")\n",
        "    print(f\"  Number of classes: {len(classes)}\")\n",
        "    print(f\"  Weight range: {class_weights.min():.4f} - {class_weights.max():.4f}\")\n",
        "    print(f\"  Mean weight: {class_weights.mean():.4f}\")\n",
        "    \n",
        "    return class_weights_tensor\n",
        "\n",
        "print(\"‚úÖ Class weights calculation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weighted_trainer"
      },
      "source": [
        "## 8. Weighted Loss Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weighted_trainer_code"
      },
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom Trainer that uses weighted CrossEntropyLoss for handling class imbalance.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "        \n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"\n",
        "        Compute weighted loss for token classification.\n",
        "        \"\"\"\n",
        "        labels = inputs.get(\"labels\")\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        \n",
        "        if labels is not None:\n",
        "            # Move class weights to the same device as logits\n",
        "            if self.class_weights is not None:\n",
        "                class_weights = self.class_weights.to(logits.device)\n",
        "            else:\n",
        "                class_weights = None\n",
        "            \n",
        "            # Create weighted loss function\n",
        "            loss_fct = CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
        "            \n",
        "            # Flatten for loss calculation\n",
        "            active_loss = labels.view(-1) != -100\n",
        "            active_logits = logits.view(-1, logits.shape[-1])\n",
        "            active_labels = torch.where(\n",
        "                active_loss,\n",
        "                labels.view(-1),\n",
        "                torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            \n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "print(\"‚úÖ Weighted Trainer class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtKaFXofIY5"
      },
      "source": [
        "## 9. K-Fold Cross-Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOZoMYBNfIY6"
      },
      "outputs": [],
      "source": [
        "# Set up 5-fold cross-validation\n",
        "N_FOLDS = 5\n",
        "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert to numpy array for easier indexing\n",
        "examples_array = np.array(prepared_examples, dtype=object)\n",
        "\n",
        "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
        "print(f\"Total examples: {len(prepared_examples)}\")\n",
        "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
        "\n",
        "# Load tokenizer (will be used across all folds)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Store results from all folds\n",
        "fold_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_helper_functions"
      },
      "source": [
        "## 10. K-Fold Cross-Validation Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "class_weights_helper_functions"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CLASS WEIGHTS K-FOLD CROSS-VALIDATION HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_fold_data_with_weights(train_examples, val_examples, tokenizer, ner_dataset):\n",
        "    \"\"\"\n",
        "    Prepare training and validation datasets for a specific fold, including class weights calculation.\n",
        "    \n",
        "    Args:\n",
        "        train_examples: Training examples for this fold\n",
        "        val_examples: Validation examples for this fold\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset, data_collator, class_weights)\n",
        "    \"\"\"\n",
        "    # Tokenize datasets with sliding window\n",
        "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    # Calculate class weights from training data\n",
        "    class_weights = calculate_class_weights(train_tokenized, ner_dataset.label_to_id)\n",
        "    \n",
        "    # Create HuggingFace datasets\n",
        "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
        "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    return train_dataset, val_dataset, data_collator, class_weights\n",
        "\n",
        "print(\"‚úÖ Class weights data preparation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "class_weights_model_creation"
      },
      "outputs": [],
      "source": [
        "def create_class_weights_model_and_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device):\n",
        "    \"\"\"\n",
        "    Create model and weighted trainer for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        fold_num: Current fold number\n",
        "        train_dataset: Training dataset for this fold\n",
        "        val_dataset: Validation dataset for this fold\n",
        "        data_collator: Data collator\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "        class_weights: Class weights tensor\n",
        "        device: Device to use (cuda/cpu)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, trainer, fold_output_dir)\n",
        "    \"\"\"\n",
        "    # Create fold-specific output directory\n",
        "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
        "    import os\n",
        "    os.makedirs(fold_output_dir, exist_ok=True)\n",
        "    \n",
        "    # Load fresh model for this fold\n",
        "    model, _ = load_model_and_tokenizer(\n",
        "        MODEL_NAME,\n",
        "        ner_dataset.get_num_labels(),\n",
        "        ner_dataset.id_to_label,\n",
        "        ner_dataset.label_to_id\n",
        "    )\n",
        "    \n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    \n",
        "    # Create training arguments for this fold\n",
        "    training_args = create_training_arguments(\n",
        "        output_dir=fold_output_dir,\n",
        "        num_epochs=model_config['num_epochs'],\n",
        "        batch_size=model_config['batch_size'],\n",
        "        learning_rate=model_config['learning_rate'],\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_steps=100,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        report_to=\"none\",  # Disable wandb for cleaner output\n",
        "        run_name=f\"bertic_class_weights_fold_{fold_num}\"\n",
        "    )\n",
        "    \n",
        "    # Create weighted trainer\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    \n",
        "    print(f\"Class Weights Trainer initialized for fold {fold_num}\")\n",
        "    print(f\"  Class weights shape: {class_weights.shape}\")\n",
        "    print(f\"  Weight range: {class_weights.min():.4f} - {class_weights.max():.4f}\")\n",
        "    \n",
        "    return model, trainer, fold_output_dir\n",
        "\n",
        "print(\"‚úÖ Class weights model and trainer creation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_main_training"
      },
      "source": [
        "## 11. K-Fold Cross-Validation Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_main_loop"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Main K-Fold Cross-Validation Loop for Class Weights\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION - CLASS WEIGHTS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total examples: {len(examples_array)}\")\n",
        "print(f\"Model: {MODEL_NAME} + Class Weights\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Execute K-Fold training\n",
        "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CLASS WEIGHTS FOLD {fold_num}/{N_FOLDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
        "    \n",
        "    # Get fold data\n",
        "    train_examples = examples_array[train_idx].tolist()\n",
        "    val_examples = examples_array[val_idx].tolist()\n",
        "    \n",
        "    print(f\"Training examples: {len(train_examples)}\")\n",
        "    print(f\"Validation examples: {len(val_examples)}\")\n",
        "    \n",
        "    # Prepare data for this fold (including class weights calculation)\n",
        "    print(f\"\\nüî§ Preparing data for class weights fold {fold_num}...\")\n",
        "    train_dataset, val_dataset, data_collator, class_weights = prepare_fold_data_with_weights(\n",
        "        train_examples, val_examples, tokenizer, ner_dataset\n",
        "    )\n",
        "    \n",
        "    print(f\"üì¶ Class Weights Fold {fold_num} datasets:\")\n",
        "    print(f\"  Training: {len(train_dataset)} examples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
        "    \n",
        "    # Create model and weighted trainer for this fold\n",
        "    print(f\"\\nü§ñ Creating class weights model and trainer for fold {fold_num}...\")\n",
        "    model, trainer, fold_output_dir = create_class_weights_model_and_trainer(\n",
        "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device\n",
        "    )\n",
        "    \n",
        "    # Train and evaluate this fold\n",
        "    print(f\"\\nüèãÔ∏è  Training class weights fold {fold_num}...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    print(f\"üíæ Saving class weights model for fold {fold_num}...\")\n",
        "    trainer.save_model()\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(f\"üìä Evaluating class weights fold {fold_num}...\")\n",
        "    eval_results = detailed_evaluation(\n",
        "        trainer, val_dataset, f\"Class Weights Fold {fold_num} Validation\", ner_dataset.id_to_label\n",
        "    )\n",
        "    \n",
        "    # Extract metrics\n",
        "    fold_result = {\n",
        "        'fold': fold_num,\n",
        "        'precision': eval_results['precision'],\n",
        "        'recall': eval_results['recall'],\n",
        "        'f1': eval_results['f1'],\n",
        "        'accuracy': eval_results['accuracy'],\n",
        "        'true_predictions': eval_results['true_predictions'],\n",
        "        'true_labels': eval_results['true_labels']\n",
        "    }\n",
        "    \n",
        "    fold_results.append(fold_result)\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del model, trainer, train_dataset, val_dataset\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    print(f\"\\n‚úÖ Class Weights Fold {fold_num} completed!\")\n",
        "    print(f\"   Precision: {fold_result['precision']:.4f}\")\n",
        "    print(f\"   Recall: {fold_result['recall']:.4f}\")\n",
        "    print(f\"   F1-Score: {fold_result['f1']:.4f}\")\n",
        "    print(f\"   Accuracy: {fold_result['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"CLASS WEIGHTS K-FOLD CROSS-VALIDATION COMPLETED!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggregate_results"
      },
      "source": [
        "## 12. Aggregate Results Across Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggregate_results_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AGGREGATE RESULTS ACROSS FOLDS\n",
        "# ============================================================================\n",
        "\n",
        "# Extract metrics from all folds\n",
        "precisions = [result['precision'] for result in fold_results]\n",
        "recalls = [result['recall'] for result in fold_results]\n",
        "f1_scores = [result['f1'] for result in fold_results]\n",
        "accuracies = [result['accuracy'] for result in fold_results]\n",
        "\n",
        "# Calculate statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"CLASS WEIGHTS FINAL RESULTS ACROSS {N_FOLDS} FOLDS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
        "print(f\"Accuracy:  {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")\n",
        "\n",
        "# Individual fold results\n",
        "print(f\"\\nüìã INDIVIDUAL FOLD RESULTS:\")\n",
        "print(f\"{'='*50}\")\n",
        "for i, result in enumerate(fold_results, 1):\n",
        "    print(f\"Fold {i}: P={result['precision']:.4f}, R={result['recall']:.4f}, F1={result['f1']:.4f}, Acc={result['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 13. Visualization of K-Fold Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION OF K-FOLD RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create visualization of fold results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle(f'{N_FOLDS}-Fold Cross-Validation Results - Class Weights Model', fontsize=16, fontweight='bold')\n",
        "\n",
        "fold_numbers = list(range(1, N_FOLDS + 1))\n",
        "\n",
        "# Plot precision\n",
        "ax1.plot(fold_numbers, precisions, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
        "ax1.axhline(y=np.mean(precisions), color='r', linestyle='--', label=f'Mean: {np.mean(precisions):.4f}')\n",
        "ax1.set_xlabel('Fold Number', fontsize=12)\n",
        "ax1.set_ylabel('Precision', fontsize=12)\n",
        "ax1.set_title('Precision Across Folds', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot recall\n",
        "ax2.plot(fold_numbers, recalls, marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
        "ax2.axhline(y=np.mean(recalls), color='r', linestyle='--', label=f'Mean: {np.mean(recalls):.4f}')\n",
        "ax2.set_xlabel('Fold Number', fontsize=12)\n",
        "ax2.set_ylabel('Recall', fontsize=12)\n",
        "ax2.set_title('Recall Across Folds', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot F1-score\n",
        "ax3.plot(fold_numbers, f1_scores, marker='^', linewidth=2, markersize=8, color='#F18F01')\n",
        "ax3.axhline(y=np.mean(f1_scores), color='r', linestyle='--', label=f'Mean: {np.mean(f1_scores):.4f}')\n",
        "ax3.set_xlabel('Fold Number', fontsize=12)\n",
        "ax3.set_ylabel('F1-Score', fontsize=12)\n",
        "ax3.set_title('F1-Score Across Folds', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.legend()\n",
        "ax3.set_xticks(fold_numbers)\n",
        "\n",
        "# Plot accuracy\n",
        "ax4.plot(fold_numbers, accuracies, marker='D', linewidth=2, markersize=8, color='#6A994E')\n",
        "ax4.axhline(y=np.mean(accuracies), color='r', linestyle='--', label=f'Mean: {np.mean(accuracies):.4f}')\n",
        "ax4.set_xlabel('Fold Number', fontsize=12)\n",
        "ax4.set_ylabel('Accuracy', fontsize=12)\n",
        "ax4.set_title('Accuracy Across Folds', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.legend()\n",
        "ax4.set_xticks(fold_numbers)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/class_weights_5fold_cv_results.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Visualization saved to: {OUTPUT_DIR}/class_weights_5fold_cv_results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## 14. Save Results to File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAVE RESULTS TO FILE\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': MODEL_NAME,\n",
        "        'architecture': 'BERT + Class Weights',\n",
        "        'n_folds': N_FOLDS,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'overall_metrics': {\n",
        "        'precision_mean': float(np.mean(precisions)),\n",
        "        'precision_std': float(np.std(precisions)),\n",
        "        'recall_mean': float(np.mean(recalls)),\n",
        "        'recall_std': float(np.std(recalls)),\n",
        "        'f1_mean': float(np.mean(f1_scores)),\n",
        "        'f1_std': float(np.std(f1_scores)),\n",
        "        'accuracy_mean': float(np.mean(accuracies)),\n",
        "        'accuracy_std': float(np.std(accuracies))\n",
        "    },\n",
        "    'fold_results': [\n",
        "        {\n",
        "            'fold': result['fold'],\n",
        "            'precision': float(result['precision']),\n",
        "            'recall': float(result['recall']),\n",
        "            'f1': float(result['f1']),\n",
        "            'accuracy': float(result['accuracy'])\n",
        "        }\n",
        "        for result in fold_results\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "results_file = f\"{OUTPUT_DIR}/5fold_cv_results.json\"\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Results saved to: {results_file}\")\n",
        "\n",
        "# Create CSV for easy analysis\n",
        "df_results = pd.DataFrame([\n",
        "    {\n",
        "        'Fold': result['fold'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1'],\n",
        "        'Accuracy': result['accuracy']\n",
        "    }\n",
        "    for result in fold_results\n",
        "])\n",
        "\n",
        "# Add summary row\n",
        "summary_row = {\n",
        "    'Fold': 'Mean ¬± Std',\n",
        "    'Precision': f\"{np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\",\n",
        "    'Recall': f\"{np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\",\n",
        "    'F1-Score': f\"{np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\",\n",
        "    'Accuracy': f\"{np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\"\n",
        "}\n",
        "\n",
        "df_results = pd.concat([df_results, pd.DataFrame([summary_row])], ignore_index=True)\n",
        "\n",
        "csv_file = f\"{OUTPUT_DIR}/5fold_cv_results.csv\"\n",
        "df_results.to_csv(csv_file, index=False)\n",
        "print(f\"‚úÖ Results CSV saved to: {csv_file}\")\n",
        "\n",
        "# Display final summary table\n",
        "print(f\"\\nüìä FINAL RESULTS TABLE:\")\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## 15. Conclusion\n",
        "\n",
        "This notebook successfully implemented 5-fold cross-validation for the Serbian Legal NER pipeline using class weights to handle class imbalance.\n",
        "\n",
        "### Key Achievements:\n",
        "- ‚úÖ **Class Imbalance Handling**: Automatic calculation and application of class weights\n",
        "- ‚úÖ **Robust Evaluation**: 5-fold cross-validation provides reliable performance estimates\n",
        "- ‚úÖ **Improved Minority Class Performance**: Better recognition of rare entity types\n",
        "- ‚úÖ **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracked across all folds\n",
        "- ‚úÖ **Statistical Analysis**: Mean and standard deviation calculated for all metrics\n",
        "- ‚úÖ **Visualization**: Clear charts showing performance across folds\n",
        "- ‚úÖ **Results Persistence**: JSON and CSV files saved for further analysis\n",
        "\n",
        "### Class Weights Advantages:\n",
        "- **Balanced Learning**: Addresses class imbalance in entity distribution\n",
        "- **No Architecture Changes**: Uses standard BERT with weighted loss\n",
        "- **Automatic Calculation**: Weights computed based on class frequencies\n",
        "- **Better Minority Performance**: Improved recognition of underrepresented entity types\n",
        "\n",
        "### Next Steps:\n",
        "1. **Compare with Other Models**: Analyze performance differences with base BERT, BERT-CRF, and XLM-R-BERTiƒá\n",
        "2. **Error Analysis**: Examine misclassified entities, especially minority classes\n",
        "3. **Hyperparameter Tuning**: Optimize learning rate, batch size, and weight calculation method\n",
        "4. **Ensemble Methods**: Combine predictions from multiple folds for better performance\n",
        "\n",
        "The 5-fold cross-validation framework successfully evaluated class weights approach for Serbian Legal NER!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
