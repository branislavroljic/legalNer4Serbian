{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GLiNER Multi-Model Evaluation: Zero-Shot on Serbian Legal Documents\n",
        "\n",
        "This notebook evaluates multiple GLiNER (Generalist and Lightweight Named Entity Recognition) models on 225 Serbian legal documents using English entity type descriptions for zero-shot NER.\n",
        "\n",
        "## ü§ñ **Models Tested:**\n",
        "- urchade/gliner_multiv2.1\n",
        "- knowledgator/gliner-x-large\n",
        "- urchade/gliner_large-v2\n",
        "- modern-gliner-bi-large-v1.0\n",
        "\n",
        "## üéØ **Evaluation Flow:**\n",
        "1. **Setup**: Define everything needed for GLiNER\n",
        "2. **Multi-Model Zero-Shot Evaluation**: Test all models without examples\n",
        "3. **Results Analysis**: Compare performance across models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gliner seqeval scikit-learn matplotlib seaborn pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Define Everything Needed for GLiNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GLiNER\n",
        "from gliner import GLiNER\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU not available - will use CPU (slower)\")\n",
        "\n",
        "print(\"‚úÖ All dependencies loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "LABELSTUDIO_JSON_PATH = \"/content/drive/MyDrive/NER_Master/annotations.json\"  # Update path as needed\n",
        "JUDGMENTS_DIR = \"/content/drive/MyDrive/NER_Master/judgments\"  # Path to judgment text files\n",
        "CONFIDENCE_THRESHOLD = 0.3  # GLiNER confidence threshold\n",
        "\n",
        "# GLiNER Models to Test\n",
        "GLINER_MODELS = [\n",
        "    \"urchade/gliner_multiv2.1\",\n",
        "    \"knowledgator/gliner-x-large\", \n",
        "    \"urchade/gliner_large-v2\",\n",
        "    \"modern-gliner-bi-large-v1.0\"\n",
        "]\n",
        "\n",
        "# English Legal Entity Types for GLiNER Zero-Shot\n",
        "LEGAL_ENTITY_TYPES = [\n",
        "    \"court or tribunal\",  # COURT\n",
        "    \"judgment date or decision date\",  # DECISION_DATE\n",
        "    \"case number or case identifier\",  # CASE_NUMBER\n",
        "    \"criminal act or offense\",  # CRIMINAL_ACT\n",
        "    \"prosecutor or public prosecutor\",  # PROSECUTOR\n",
        "    \"defendant or accused person\",  # DEFENDANT\n",
        "    \"judge or judicial officer\",  # JUDGE\n",
        "    \"court registrar or clerk\",  # REGISTRAR\n",
        "    \"court verdict or judgment\",  # VERDICT\n",
        "    \"type of penalty or sanction\",  # SANCTION_TYPE\n",
        "    \"amount or duration of penalty\",  # SANCTION_VALUE\n",
        "    \"material legal provision or article\",  # PROVISION_MATERIAL\n",
        "    \"procedural legal provision or article\",  # PROVISION_PROCEDURAL\n",
        "    \"court procedure costs or fees\",  # PROCEDURE_COSTS\n",
        "]\n",
        "\n",
        "# Mapping from English GLiNER labels to ground truth labels\n",
        "GLINER_TO_GT_MAPPING = {\n",
        "    \"court or tribunal\": \"COURT\",\n",
        "    \"judgment date or decision date\": \"DECISION_DATE\",\n",
        "    \"case number or case identifier\": \"CASE_NUMBER\",\n",
        "    \"criminal act or offense\": \"CRIMINAL_ACT\",\n",
        "    \"prosecutor or public prosecutor\": \"PROSECUTOR\",\n",
        "    \"defendant or accused person\": \"DEFENDANT\",\n",
        "    \"judge or judicial officer\": \"JUDGE\",\n",
        "    \"court registrar or clerk\": \"REGISTRAR\",\n",
        "    \"court verdict or judgment\": \"VERDICT\",\n",
        "    \"type of penalty or sanction\": \"SANCTION_TYPE\",\n",
        "    \"amount or duration of penalty\": \"SANCTION_VALUE\",\n",
        "    \"material legal provision or article\": \"PROVISION_MATERIAL\",\n",
        "    \"procedural legal provision or article\": \"PROVISION_PROCEDURAL\",\n",
        "    \"court procedure costs or fees\": \"PROCEDURE_COSTS\",\n",
        "}\n",
        "\n",
        "print(f\"üéØ Entity types: {len(LEGAL_ENTITY_TYPES)}\")\n",
        "print(f\"ü§ñ Models to test: {len(GLINER_MODELS)}\")\n",
        "print(f\"‚ö° Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
        "print(f\"\\nüìã Models:\")\n",
        "for i, model in enumerate(GLINER_MODELS, 1):\n",
        "    print(f\"  {i}. {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ground_truth_data():\n",
        "    \"\"\"Load ground truth data from LabelStudio annotations\"\"\"\n",
        "    print(\"üìÇ Loading LabelStudio annotations...\")\n",
        "    \n",
        "    # Create reverse mapping from BIO labels to English GLiNER labels\n",
        "    label_to_english = {label: english_label for english_label, label in GLINER_TO_GT_MAPPING.items()}\n",
        "    print(f\"üîÑ Created mapping for {len(label_to_english)} entity types\")\n",
        "    \n",
        "    try:\n",
        "        with open(LABELSTUDIO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            labelstudio_data = json.load(f)\n",
        "        print(f\"‚úÖ Loaded {len(labelstudio_data)} annotated documents\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {LABELSTUDIO_JSON_PATH} not found!\")\n",
        "        return []\n",
        "    \n",
        "    ground_truth_examples = []\n",
        "    entity_types = set()\n",
        "    bio_labels_found = set()\n",
        "    \n",
        "    for item in tqdm(labelstudio_data, desc=\"Loading ground truth\"):\n",
        "        file_path = item.get(\"file_upload\", \"\")\n",
        "        \n",
        "        # Load text file\n",
        "        if \"/\" in file_path:\n",
        "            filename = file_path.split(\"/\")[-1]\n",
        "        else:\n",
        "            filename = file_path\n",
        "        \n",
        "        full_path = Path(JUDGMENTS_DIR) / filename\n",
        "        \n",
        "        if not full_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                text_content = f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading {full_path}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Extract entities\n",
        "        annotations = item.get(\"annotations\", [])\n",
        "        for annotation in annotations:\n",
        "            entities = []\n",
        "            result = annotation.get(\"result\", [])\n",
        "            \n",
        "            for res in result:\n",
        "                if res.get(\"type\") == \"labels\":\n",
        "                    value = res[\"value\"]\n",
        "                    start = value[\"start\"]\n",
        "                    end = value[\"end\"]\n",
        "                    labels = value[\"labels\"]\n",
        "                    \n",
        "                    for bio_label in labels:\n",
        "                        bio_labels_found.add(bio_label)\n",
        "                        \n",
        "                        # Convert BIO label to English GLiNER label\n",
        "                        english_label = label_to_english.get(bio_label)\n",
        "                        if english_label:\n",
        "                            entity_types.add(english_label)\n",
        "                            entities.append({\n",
        "                                'text': text_content[start:end],\n",
        "                                'label': english_label,  # Use English GLiNER label\n",
        "                                'start': start,\n",
        "                                'end': end,\n",
        "                                'bio_label': bio_label  # Keep original for reference\n",
        "                            })\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è Warning: Unknown BIO label '{bio_label}' - skipping\")\n",
        "            \n",
        "            if entities:\n",
        "                ground_truth_examples.append({\n",
        "                    'text': text_content,\n",
        "                    'entities': entities,\n",
        "                    'file_path': file_path\n",
        "                })\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(ground_truth_examples)} examples with ground truth entities\")\n",
        "    print(f\"üè∑Ô∏è Found English GLiNER entity types: {sorted(entity_types)}\")\n",
        "    print(f\"üî§ Found BIO labels: {sorted(bio_labels_found)}\")\n",
        "    \n",
        "    # Show entity distribution\n",
        "    entity_counts = Counter()\n",
        "    for example in ground_truth_examples:\n",
        "        for entity in example['entities']:\n",
        "            entity_counts[entity['label']] += 1\n",
        "    \n",
        "    print(f\"\\nüìä Ground Truth Statistics:\")\n",
        "    print(f\"  üìÑ Total examples: {len(ground_truth_examples)}\")\n",
        "    print(f\"  üè∑Ô∏è Entity types: {len(entity_types)}\")\n",
        "    print(f\"\\nüìà Entity Distribution:\")\n",
        "    for entity_type, count in entity_counts.most_common():\n",
        "        print(f\"  {entity_type}: {count}\")\n",
        "    \n",
        "    return ground_truth_examples\n",
        "\n",
        "# Load ground truth data\n",
        "ground_truth_examples = load_ground_truth_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GLiNEREvaluator:\n",
        "    \"\"\"GLiNER NER Evaluator for Serbian Legal Documents\"\"\"\n",
        "    \n",
        "    def __init__(self, confidence_threshold: float = 0.3):\n",
        "        self.model = None\n",
        "        self.model_name = None\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        print(f\"üåü GLiNER Evaluator initialized with confidence threshold: {confidence_threshold}\")\n",
        "    \n",
        "    def load_model(self, model_name: str):\n",
        "        \"\"\"Load a specific GLiNER model\"\"\"\n",
        "        print(f\"\\nüîÑ Loading GLiNER model: {model_name}\")\n",
        "        \n",
        "        # Clean up previous model if exists\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        try:\n",
        "            # Load GLiNER model\n",
        "            self.model = GLiNER.from_pretrained(model_name)\n",
        "            \n",
        "            # Enable GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                self.model = self.model.cuda()\n",
        "                print(f\"üöÄ GPU enabled: {torch.cuda.get_device_name(0)}\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è GPU not available, using CPU\")\n",
        "            \n",
        "            self.model_name = model_name\n",
        "            print(f\"‚úÖ GLiNER model {model_name} loaded successfully\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading GLiNER model {model_name}: {e}\")\n",
        "            self.model = None\n",
        "            self.model_name = None\n",
        "            return False\n",
        "    \n",
        "    def predict_entities(self, text: str, entity_types: List[str]) -> List[Dict]:\n",
        "        \"\"\"Predict entities using GLiNER zero-shot approach\"\"\"\n",
        "        if self.model is None:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            method =  \"gliner_zero_shot\"\n",
        "            \n",
        "            # Zero-shot prediction\n",
        "            entities = self.model.predict_entities(\n",
        "                text,\n",
        "                labels=entity_types,\n",
        "                threshold=self.confidence_threshold\n",
        "            )\n",
        "            \n",
        "            # Convert to our format\n",
        "            formatted_entities = []\n",
        "            for entity in entities:\n",
        "                formatted_entities.append({\n",
        "                    \"text\": entity[\"text\"],\n",
        "                    \"label\": entity[\"label\"],\n",
        "                    \"start\": entity[\"start\"],\n",
        "                    \"end\": entity[\"end\"],\n",
        "                    \"confidence\": entity[\"score\"],\n",
        "                    \"method\": method,\n",
        "                    \"model\": self.model_name\n",
        "                })\n",
        "            \n",
        "            return sorted(formatted_entities, key=lambda x: x[\"start\"])\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in GLiNER prediction: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize GLiNER evaluator (models will be loaded individually)\n",
        "gliner_evaluator = GLiNEREvaluator(confidence_threshold=CONFIDENCE_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Zero-Shot Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_gliner(examples, entity_types, evaluator, method_name=\"GLiNER\"):\n",
        "    \"\"\"Evaluate GLiNER on the given examples\"\"\"\n",
        "    print(f\"\\nüß™ Starting {method_name} Evaluation\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if evaluator.model is None:\n",
        "        return {\"error\": \"GLiNER model not loaded\"}\n",
        "    \n",
        "    detailed_results = []\n",
        "    prediction_counts = Counter()\n",
        "    confidence_scores = []\n",
        "    \n",
        "    print(f\"üìä Evaluating on {len(examples)} examples...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i, example in enumerate(tqdm(examples, desc=f\"{method_name} Evaluation\")):\n",
        "        text = example[\"text\"]\n",
        "        true_entities = example[\"entities\"]\n",
        "        \n",
        "        # Get GLiNER predictions\n",
        "        pred_entities = evaluator.predict_entities(text, entity_types)\n",
        "        \n",
        "        # Count predictions by type\n",
        "        for entity in pred_entities:\n",
        "            prediction_counts[entity[\"label\"]] += 1\n",
        "            confidence_scores.append(entity[\"confidence\"])\n",
        "        \n",
        "        # Store detailed results\n",
        "        detailed_results.append({\n",
        "            \"example_id\": i,\n",
        "            \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
        "            \"file_path\": example[\"file_path\"],\n",
        "            \"true_entities\": true_entities,\n",
        "            \"pred_entities\": pred_entities,\n",
        "            \"true_count\": len(true_entities),\n",
        "            \"pred_count\": len(pred_entities)\n",
        "        })\n",
        "    \n",
        "    end_time = time.time()\n",
        "    evaluation_time = end_time - start_time\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_true = sum(len(r[\"true_entities\"]) for r in detailed_results)\n",
        "    total_pred = sum(len(r[\"pred_entities\"]) for r in detailed_results)\n",
        "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0\n",
        "    \n",
        "    print(f\"\\nüìä {method_name} Prediction Statistics:\")\n",
        "    for label, count in prediction_counts.most_common():\n",
        "        print(f\"  {label}: {count}\")\n",
        "    \n",
        "    results = {\n",
        "        \"method\": method_name,\n",
        "        \"model_name\": evaluator.model_name,\n",
        "        \"confidence_threshold\": evaluator.confidence_threshold,\n",
        "        \"detailed_results\": detailed_results,\n",
        "        \"total_true_entities\": total_true,\n",
        "        \"total_pred_entities\": total_pred,\n",
        "        \"prediction_counts\": dict(prediction_counts),\n",
        "        \"examples_evaluated\": len(examples),\n",
        "        \"avg_confidence\": avg_confidence,\n",
        "        \"evaluation_time\": evaluation_time,\n",
        "        \"entities_per_second\": total_pred / evaluation_time if evaluation_time > 0 else 0\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ {method_name} evaluation complete!\")\n",
        "    print(f\"  üìä True entities: {total_true}\")\n",
        "    print(f\"  ü§ñ Predicted entities: {total_pred}\")\n",
        "    print(f\"  ‚ö° Average confidence: {avg_confidence:.3f}\")\n",
        "    print(f\"  ‚è±Ô∏è Evaluation time: {evaluation_time:.2f}s\")\n",
        "    print(f\"  üöÄ Entities/second: {results['entities_per_second']:.2f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run zero-shot evaluation for all models\n",
        "all_model_results = {}\n",
        "failed_models = []\n",
        "\n",
        "print(f\"\\nüöÄ Starting evaluation of {len(GLINER_MODELS)} GLiNER models...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, model_name in enumerate(GLINER_MODELS, 1):\n",
        "    print(f\"\\nüìä Model {i}/{len(GLINER_MODELS)}: {model_name}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Load the model\n",
        "    if gliner_evaluator.load_model(model_name):\n",
        "        # Run evaluation\n",
        "        model_results = evaluate_gliner(\n",
        "            ground_truth_examples, \n",
        "            LEGAL_ENTITY_TYPES, \n",
        "            gliner_evaluator,\n",
        "            method_name=f\"Zero-Shot {model_name}\"\n",
        "        )\n",
        "        \n",
        "        if \"error\" not in model_results:\n",
        "            all_model_results[model_name] = model_results\n",
        "            print(f\"‚úÖ {model_name} evaluation completed successfully\")\n",
        "        else:\n",
        "            failed_models.append(model_name)\n",
        "            print(f\"‚ùå {model_name} evaluation failed: {model_results['error']}\")\n",
        "    else:\n",
        "        failed_models.append(model_name)\n",
        "        print(f\"‚ùå Failed to load {model_name}\")\n",
        "\n",
        "print(f\"\\nüéâ Evaluation complete!\")\n",
        "print(f\"‚úÖ Successfully evaluated: {len(all_model_results)} models\")\n",
        "print(f\"‚ùå Failed models: {len(failed_models)}\")\n",
        "if failed_models:\n",
        "    print(f\"   Failed: {', '.join(failed_models)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_results_visualization(all_model_results):\n",
        "    \"\"\"Create visualizations comparing results across multiple models\"\"\"\n",
        "    if not all_model_results:\n",
        "        print(\"‚ö†Ô∏è No results to visualize\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "    fig.suptitle('GLiNER Multi-Model Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Entity count comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    model_names = [name.split('/')[-1] for name in all_model_results.keys()]  # Short names\n",
        "    entity_counts = [results['total_pred_entities'] for results in all_model_results.values()]\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
        "    \n",
        "    bars1 = ax1.bar(model_names, entity_counts, color=colors)\n",
        "    ax1.set_title('Total Predicted Entities by Model')\n",
        "    ax1.set_ylabel('Number of Entities')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars1, entity_counts):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
        "                str(count), ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Confidence comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    confidence_scores = [results['avg_confidence'] for results in all_model_results.values()]\n",
        "    \n",
        "    bars2 = ax2.bar(model_names, confidence_scores, color=colors)\n",
        "    ax2.set_title('Average Confidence Score by Model')\n",
        "    ax2.set_ylabel('Confidence')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, conf in zip(bars2, confidence_scores):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 3. Performance comparison (F1 scores would go here if calculated)\n",
        "    ax3 = axes[1, 0]\n",
        "    eval_times = [results['evaluation_time'] for results in all_model_results.values()]\n",
        "    \n",
        "    bars3 = ax3.bar(model_names, eval_times, color=colors)\n",
        "    ax3.set_title('Evaluation Time by Model')\n",
        "    ax3.set_ylabel('Time (seconds)')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, time_val in zip(bars3, eval_times):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(eval_times)*0.01, \n",
        "                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 4. Entities per second comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    entities_per_sec = [results['entities_per_second'] for results in all_model_results.values()]\n",
        "    \n",
        "    bars4 = ax4.bar(model_names, entities_per_sec, color=colors)\n",
        "    ax4.set_title('Processing Speed by Model')\n",
        "    ax4.set_ylabel('Entities/second')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, speed in zip(bars4, entities_per_sec):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(entities_per_sec)*0.01, \n",
        "                f'{speed:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "if 'all_model_results' in locals() and all_model_results:\n",
        "    create_results_visualization(all_model_results)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Results not available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Proper Entity-Level Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_entity_classification_report(results, method_name):\n",
        "    \"\"\"Generate proper entity-level classification report (no BIO tagging)\"\"\"\n",
        "    print(f\"\\nüìä {method_name} - Entity-Level Classification Report:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if 'detailed_results' not in results:\n",
        "        print(\"‚ùå No detailed results available\")\n",
        "        return\n",
        "    \n",
        "    # Entity-level evaluation\n",
        "    entity_stats = {}\n",
        "    total_true = 0\n",
        "    total_pred = 0\n",
        "    exact_matches = 0\n",
        "    \n",
        "    # Collect entity-level statistics\n",
        "    for example_result in results['detailed_results']:\n",
        "        true_entities = example_result['true_entities']\n",
        "        pred_entities = example_result['pred_entities']\n",
        "        \n",
        "        total_true += len(true_entities)\n",
        "        total_pred += len(pred_entities)\n",
        "        \n",
        "        # Count by entity type\n",
        "        for entity in true_entities:\n",
        "            label = entity['label']\n",
        "            if label not in entity_stats:\n",
        "                entity_stats[label] = {'true': 0, 'pred': 0, 'correct': 0}\n",
        "            entity_stats[label]['true'] += 1\n",
        "        \n",
        "        for entity in pred_entities:\n",
        "            label = entity['label']\n",
        "            if label not in entity_stats:\n",
        "                entity_stats[label] = {'true': 0, 'pred': 0, 'correct': 0}\n",
        "            entity_stats[label]['pred'] += 1\n",
        "        \n",
        "        # Check for exact matches (same span and label)\n",
        "        for true_entity in true_entities:\n",
        "            for pred_entity in pred_entities:\n",
        "                if (true_entity['start'] == pred_entity['start'] and \n",
        "                    true_entity['end'] == pred_entity['end'] and \n",
        "                    true_entity['label'] == pred_entity['label']):\n",
        "                    entity_stats[true_entity['label']]['correct'] += 1\n",
        "                    exact_matches += 1\n",
        "                    break\n",
        "    \n",
        "    # Calculate metrics for each entity type\n",
        "    print(f\"{'Entity Type':<30} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    num_types = 0\n",
        "    \n",
        "    for label in sorted(entity_stats.keys()):\n",
        "        stats = entity_stats[label]\n",
        "        \n",
        "        # Calculate precision, recall, F1\n",
        "        precision = stats['correct'] / stats['pred'] if stats['pred'] > 0 else 0.0\n",
        "        recall = stats['correct'] / stats['true'] if stats['true'] > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        print(f\"{label:<30} {precision:<10.2f} {recall:<10.2f} {f1:<10.2f} {stats['true']:<10d}\")\n",
        "        \n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "        num_types += 1\n",
        "    \n",
        "    # Overall metrics\n",
        "    print(\"=\" * 80)\n",
        "    overall_precision = exact_matches / total_pred if total_pred > 0 else 0.0\n",
        "    overall_recall = exact_matches / total_true if total_true > 0 else 0.0\n",
        "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "    \n",
        "    print(f\"{'Micro Avg':<30} {overall_precision:<10.2f} {overall_recall:<10.2f} {overall_f1:<10.2f} {total_true:<10d}\")\n",
        "    \n",
        "    if num_types > 0:\n",
        "        macro_precision = total_precision / num_types\n",
        "        macro_recall = total_recall / num_types\n",
        "        macro_f1 = total_f1 / num_types\n",
        "        print(f\"{'Macro Avg':<30} {macro_precision:<10.2f} {macro_recall:<10.2f} {macro_f1:<10.2f} {total_true:<10d}\")\n",
        "    \n",
        "    print(f\"\\nüìä Summary:\")\n",
        "    print(f\"  Total True Entities: {total_true}\")\n",
        "    print(f\"  Total Predicted Entities: {total_pred}\")\n",
        "    print(f\"  Exact Matches: {exact_matches}\")\n",
        "    accuracy = exact_matches/max(total_true, total_pred) if max(total_true, total_pred) > 0 else 0.0\n",
        "    print(f\"  Overall Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Generate proper entity-level reports for all models\n",
        "if 'all_model_results' in locals() and all_model_results:\n",
        "    for model_name, results in all_model_results.items():\n",
        "        generate_entity_classification_report(results, f\"Zero-Shot {model_name}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results available for classification reports\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results for further analysis\n",
        "if 'all_model_results' in locals() and all_model_results:\n",
        "    results_summary = {\n",
        "        \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"models_tested\": list(all_model_results.keys()),\n",
        "        \"models_failed\": failed_models if 'failed_models' in locals() else [],\n",
        "        \"confidence_threshold\": CONFIDENCE_THRESHOLD,\n",
        "        \"entity_types\": LEGAL_ENTITY_TYPES,\n",
        "        \"ground_truth_examples_count\": len(ground_truth_examples),\n",
        "        \"model_results\": all_model_results,\n",
        "    }\n",
        "    \n",
        "    # Save to JSON file\n",
        "    output_file = \"gliner_multi_model_evaluation_results.json\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"\\nüíæ Results saved to {output_file}\")\n",
        "    print(f\"\\nüéâ GLiNER multi-model evaluation complete!\")\n",
        "    print(f\"üìä Evaluated {len(ground_truth_examples)} documents\")\n",
        "    print(f\"ü§ñ Tested {len(all_model_results)} models successfully\")\n",
        "    print(f\"üè∑Ô∏è Tested {len(LEGAL_ENTITY_TYPES)} entity types\")\n",
        "    \n",
        "    # Print summary comparison\n",
        "    print(f\"\\nüìà Model Performance Summary:\")\n",
        "    print(f\"{'Model':<30} {'Entities':<10} {'Avg Conf':<10} {'Time (s)':<10} {'Speed':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "    for model_name, results in all_model_results.items():\n",
        "        short_name = model_name.split('/')[-1][:28]\n",
        "        print(f\"{short_name:<30} {results['total_pred_entities']:<10} {results['avg_confidence']:<10.3f} {results['evaluation_time']:<10.1f} {results['entities_per_second']:<12.1f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to save\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
