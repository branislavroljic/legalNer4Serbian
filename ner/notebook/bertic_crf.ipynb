{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxJ3lfIjTOm"
      },
      "source": [
        "# Serbian Legal Named Entity Recognition (NER) Pipeline - BERT-CRF 5-Fold Cross-Validation\n",
        "\n",
        "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using BERT-CRF architecture.\n",
        "BERT-CRF combines BERT embeddings with a Conditional Random Field (CRF) layer for better sequence modeling.\n",
        "\n",
        "## Key Features\n",
        "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
        "- **BERT-CRF Architecture**: BERT embeddings + CRF layer for sequence constraints\n",
        "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
        "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
        "- **Statistical Analysis**: Mean and standard deviation across folds\n",
        "\n",
        "## BERT-CRF Advantages\n",
        "- **Better Sequence Modeling**: CRF enforces valid BIO sequence constraints\n",
        "- **Improved Entity Boundaries**: More accurate entity span detection\n",
        "- **Global Optimization**: Considers entire sequence for optimal labeling\n",
        "\n",
        "## Entity Types\n",
        "- **COURT**: Court institutions\n",
        "- **DECISION_DATE**: Dates of legal decisions\n",
        "- **CASE_NUMBER**: Case identifiers\n",
        "- **CRIMINAL_ACT**: Criminal acts/charges\n",
        "- **PROSECUTOR**: Prosecutor entities\n",
        "- **DEFENDANT**: Defendant entities\n",
        "- **JUDGE**: Judge names\n",
        "- **REGISTRAR**: Court registrar\n",
        "- **SANCTION**: Sanctions/penalties\n",
        "- **SANCTION_TYPE**: Type of sanction\n",
        "- **SANCTION_VALUE**: Value/duration of sanction\n",
        "- **PROVISION**: Legal provisions\n",
        "- **PROCEDURE_COSTS**: Legal procedure costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTyacPF64Th"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    USE_COLAB = True\n",
        "except ImportError:\n",
        "    USE_COLAB = False\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHG6Rs8jTOo"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11kIuCNPjTOo"
      },
      "outputs": [],
      "source": [
        "# Install required packages including pytorch-crf for CRF layer\n",
        "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ig3oNUjTOo"
      },
      "outputs": [],
      "source": [
        "# Import shared modules\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the shared modules to path\n",
        "if USE_COLAB:\n",
        "    sys.path.append('/content/drive/MyDrive/NER_Master/ner/')\n",
        "else:\n",
        "    sys.path.append('../shared')\n",
        "\n",
        "import importlib\n",
        "import shared\n",
        "import shared.model_utils\n",
        "import shared.data_processing\n",
        "import shared.dataset\n",
        "import shared.evaluation\n",
        "import shared.config\n",
        "importlib.reload(shared.config)\n",
        "importlib.reload(shared.data_processing)\n",
        "importlib.reload(shared.dataset)\n",
        "importlib.reload(shared.model_utils)\n",
        "importlib.reload(shared.evaluation)\n",
        "importlib.reload(shared)\n",
        "\n",
        "# Import from shared modules\n",
        "from shared import (\n",
        "    # Configuration\n",
        "    ENTITY_TYPES, BIO_LABELS, DEFAULT_TRAINING_ARGS,\n",
        "    get_default_model_config, get_paths, setup_environment,\n",
        "\n",
        "    # Data processing\n",
        "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
        "    analyze_labelstudio_data, validate_bio_examples,\n",
        "\n",
        "    # Dataset\n",
        "    NERDataset, split_dataset, tokenize_and_align_labels_with_sliding_window,\n",
        "    print_sequence_analysis, create_huggingface_datasets,\n",
        "\n",
        "    # Model utilities\n",
        "    load_model_and_tokenizer, create_training_arguments, create_trainer,\n",
        "    detailed_evaluation, save_model_info, setup_device_and_seed,\n",
        "\n",
        "    # Evaluation\n",
        "    generate_evaluation_report, plot_training_history, plot_entity_distribution\n",
        ")\n",
        "\n",
        "# Standard imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "from transformers import DataCollatorForTokenClassification, AutoTokenizer\n",
        "\n",
        "# Setup device and random seed\n",
        "device = setup_device_and_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bert_crf_imports"
      },
      "source": [
        "## 2. BERT-CRF Specific Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_crf_imports_code"
      },
      "outputs": [],
      "source": [
        "# BERT-CRF specific imports\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "\n",
        "print(\"‚úÖ BERT-CRF specific imports loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R56QtmR7fIY2"
      },
      "source": [
        "## 3. Configuration and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i3CBXt2fIY3"
      },
      "outputs": [],
      "source": [
        "# Setup environment and paths\n",
        "env_setup = setup_environment(use_local=not USE_COLAB, create_dirs=True)\n",
        "paths = env_setup['paths']\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"classla/bcms-bertic\"\n",
        "model_config = get_default_model_config()\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_crf_5fold_cv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üîß Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Architecture: BERT-CRF\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
        "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVSwtVS1fIY3"
      },
      "source": [
        "## 4. Data Loading and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmq8DjhyfIY4"
      },
      "outputs": [],
      "source": [
        "# Load LabelStudio data\n",
        "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
        "\n",
        "# Analyze the data\n",
        "if labelstudio_data:\n",
        "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAyV4gQfIY4"
      },
      "source": [
        "## 5. Data Preprocessing and BIO Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOe5xEDnfIY5"
      },
      "outputs": [],
      "source": [
        "# Convert LabelStudio data to BIO format\n",
        "converter = LabelStudioToBIOConverter(\n",
        "    judgments_dir=paths['judgments_dir'],\n",
        "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
        ")\n",
        "\n",
        "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
        "print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
        "\n",
        "# Validate BIO examples\n",
        "valid_examples, stats = validate_bio_examples(bio_examples)\n",
        "print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HcyAbInfIY5"
      },
      "source": [
        "## 6. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cX7CQA3fIY5"
      },
      "outputs": [],
      "source": [
        "# Create NER dataset\n",
        "ner_dataset = NERDataset(valid_examples)\n",
        "prepared_examples = ner_dataset.prepare_for_training()\n",
        "\n",
        "print(f\"üìä Dataset statistics:\")\n",
        "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
        "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
        "\n",
        "# Get label statistics\n",
        "label_stats = ner_dataset.get_label_statistics()\n",
        "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
        "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bert_crf_model_definition"
      },
      "source": [
        "## 7. BERT-CRF Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_crf_model_code"
      },
      "outputs": [],
      "source": [
        "class BertCrfForTokenClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model with CRF layer for token classification.\n",
        "    Combines BERT embeddings with CRF for better sequence modeling.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.config = config\n",
        "        \n",
        "        # BERT backbone\n",
        "        self.bert = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
        "        \n",
        "        # Dropout and classifier\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        \n",
        "        # CRF layer\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        \n",
        "        # Apply dropout and classifier\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # CRF loss calculation\n",
        "            # Convert labels to mask (ignore -100 labels)\n",
        "            mask = labels != -100\n",
        "            # Replace -100 with 0 for CRF (will be masked anyway)\n",
        "            labels_masked = labels.clone()\n",
        "            labels_masked[labels == -100] = 0\n",
        "            \n",
        "            # Calculate CRF loss\n",
        "            log_likelihood = self.crf(logits, labels_masked, mask=mask, reduction='mean')\n",
        "            loss = -log_likelihood\n",
        "        \n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "    \n",
        "    def predict(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Predict using CRF decoding\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "            sequence_output = self.dropout(outputs.last_hidden_state)\n",
        "            logits = self.classifier(sequence_output)\n",
        "            \n",
        "            # CRF decoding\n",
        "            mask = attention_mask.bool() if attention_mask is not None else None\n",
        "            predictions = self.crf.decode(logits, mask=mask)\n",
        "            return predictions\n",
        "\n",
        "print(\"‚úÖ BERT-CRF model class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtKaFXofIY5"
      },
      "source": [
        "## 8. K-Fold Cross-Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOZoMYBNfIY6"
      },
      "outputs": [],
      "source": [
        "# Set up 5-fold cross-validation\n",
        "N_FOLDS = 5\n",
        "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert to numpy array for easier indexing\n",
        "examples_array = np.array(prepared_examples, dtype=object)\n",
        "\n",
        "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
        "print(f\"Total examples: {len(prepared_examples)}\")\n",
        "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
        "\n",
        "# Load tokenizer (will be used across all folds)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Store results from all folds\n",
        "fold_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_helper_functions"
      },
      "source": [
        "## 9. K-Fold Cross-Validation Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_helper_functions_code"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BERT-CRF K-FOLD CROSS-VALIDATION HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_fold_data(train_examples, val_examples, tokenizer, ner_dataset):\n",
        "    \"\"\"\n",
        "    Prepare training and validation datasets for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        train_examples: Training examples for this fold\n",
        "        val_examples: Validation examples for this fold\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset, data_collator)\n",
        "    \"\"\"\n",
        "    # Tokenize datasets with sliding window\n",
        "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
        "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
        "        max_length=model_config['max_length'], stride=model_config['stride']\n",
        "    )\n",
        "    \n",
        "    # Create HuggingFace datasets\n",
        "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
        "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    return train_dataset, val_dataset, data_collator\n",
        "\n",
        "print(\"‚úÖ Data preparation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_crf_model_creation"
      },
      "outputs": [],
      "source": [
        "def create_bert_crf_model_and_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device):\n",
        "    \"\"\"\n",
        "    Create BERT-CRF model and trainer for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        fold_num: Current fold number\n",
        "        train_dataset: Training dataset for this fold\n",
        "        val_dataset: Validation dataset for this fold\n",
        "        data_collator: Data collator\n",
        "        tokenizer: Tokenizer instance\n",
        "        ner_dataset: NER dataset instance\n",
        "        device: Device to use (cuda/cpu)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, trainer, fold_output_dir)\n",
        "    \"\"\"\n",
        "    # Create fold-specific output directory\n",
        "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
        "    import os\n",
        "    os.makedirs(fold_output_dir, exist_ok=True)\n",
        "    \n",
        "    # Load config and create BERT-CRF model\n",
        "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    model = BertCrfForTokenClassification(config, ner_dataset.get_num_labels())\n",
        "    \n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    \n",
        "    # Create training arguments for this fold\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=fold_output_dir,\n",
        "        num_train_epochs=model_config['num_epochs'],\n",
        "        per_device_train_batch_size=model_config['batch_size'],\n",
        "        per_device_eval_batch_size=model_config['batch_size'],\n",
        "        learning_rate=model_config['learning_rate'],\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_steps=100,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        report_to=\"none\",  # Disable wandb for cleaner output\n",
        "        run_name=f\"bertic_crf_fold_{fold_num}\",\n",
        "        dataloader_pin_memory=False,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "    \n",
        "    # Create trainer with custom compute_metrics for CRF\n",
        "    def compute_metrics_crf(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        # For CRF, we need to decode predictions differently\n",
        "        # This is a simplified version - you might need to adapt based on your evaluation needs\n",
        "        return {\"f1\": 0.0}  # Placeholder - will be computed in detailed_evaluation\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_crf\n",
        "    )\n",
        "    \n",
        "    print(f\"BERT-CRF Trainer initialized for fold {fold_num}\")\n",
        "    return model, trainer, fold_output_dir\n",
        "\n",
        "print(\"‚úÖ BERT-CRF model and trainer creation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_crf_evaluation"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_bert_crf_fold(fold_num, trainer, val_dataset, ner_dataset):\n",
        "    \"\"\"\n",
        "    Train and evaluate a BERT-CRF model for a specific fold.\n",
        "    \n",
        "    Args:\n",
        "        fold_num: Current fold number\n",
        "        trainer: Trainer instance\n",
        "        val_dataset: Validation dataset for this fold\n",
        "        ner_dataset: NER dataset instance\n",
        "    \n",
        "    Returns:\n",
        "        dict: Fold results including metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\nüèãÔ∏è  Training BERT-CRF fold {fold_num}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    \n",
        "    print(f\"üíæ Saving BERT-CRF model for fold {fold_num}...\")\n",
        "    trainer.save_model()\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(f\"üìä Evaluating BERT-CRF fold {fold_num}...\")\n",
        "    \n",
        "    # For BERT-CRF, we need custom evaluation since CRF decoding is different\n",
        "    model = trainer.model\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in trainer.get_eval_dataloader():\n",
        "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "            \n",
        "            # Get CRF predictions\n",
        "            predictions = model.predict(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            \n",
        "            # Process labels\n",
        "            labels = batch['labels']\n",
        "            \n",
        "            # Convert to lists and filter out special tokens\n",
        "            for pred_seq, label_seq, attention_seq in zip(predictions, labels, batch['attention_mask']):\n",
        "                # Filter based on attention mask and ignore -100 labels\n",
        "                valid_length = attention_seq.sum().item()\n",
        "                pred_seq = pred_seq[:valid_length]\n",
        "                label_seq = label_seq[:valid_length]\n",
        "                \n",
        "                # Filter out -100 labels\n",
        "                valid_indices = label_seq != -100\n",
        "                if valid_indices.any():\n",
        "                    pred_filtered = [pred_seq[i] for i in range(len(pred_seq)) if valid_indices[i]]\n",
        "                    label_filtered = [label_seq[i].item() for i in range(len(label_seq)) if valid_indices[i]]\n",
        "                    \n",
        "                    all_predictions.extend(pred_filtered)\n",
        "                    all_labels.extend(label_filtered)\n",
        "    \n",
        "    # Convert to label names for evaluation\n",
        "    pred_labels = [ner_dataset.id_to_label[pred] for pred in all_predictions]\n",
        "    true_labels = [ner_dataset.id_to_label[label] for label in all_labels]\n",
        "    \n",
        "    # Calculate metrics using seqeval\n",
        "    from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "    \n",
        "    # Convert to sequence format for seqeval\n",
        "    pred_sequences = [pred_labels]\n",
        "    true_sequences = [true_labels]\n",
        "    \n",
        "    precision = precision_score(true_sequences, pred_sequences)\n",
        "    recall = recall_score(true_sequences, pred_sequences)\n",
        "    f1 = f1_score(true_sequences, pred_sequences)\n",
        "    accuracy = accuracy_score(true_sequences, pred_sequences)\n",
        "    \n",
        "    # Extract metrics\n",
        "    fold_result = {\n",
        "        'fold': fold_num,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'accuracy': accuracy,\n",
        "        'true_predictions': pred_labels,\n",
        "        'true_labels': true_labels\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nBERT-CRF Fold {fold_num} completed successfully!\")\n",
        "    return fold_result\n",
        "\n",
        "print(\"‚úÖ BERT-CRF training and evaluation function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_main_training"
      },
      "source": [
        "## 10. K-Fold Cross-Validation Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_main_loop"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Main K-Fold Cross-Validation Loop for BERT-CRF\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION - BERT-CRF\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total examples: {len(examples_array)}\")\n",
        "print(f\"Model: {MODEL_NAME} + CRF\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Execute K-Fold training\n",
        "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"BERT-CRF FOLD {fold_num}/{N_FOLDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
        "    \n",
        "    # Get fold data\n",
        "    train_examples = examples_array[train_idx].tolist()\n",
        "    val_examples = examples_array[val_idx].tolist()\n",
        "    \n",
        "    print(f\"Training examples: {len(train_examples)}\")\n",
        "    print(f\"Validation examples: {len(val_examples)}\")\n",
        "    \n",
        "    # Prepare data for this fold\n",
        "    print(f\"\\nüî§ Preparing data for BERT-CRF fold {fold_num}...\")\n",
        "    train_dataset, val_dataset, data_collator = prepare_fold_data(\n",
        "        train_examples, val_examples, tokenizer, ner_dataset\n",
        "    )\n",
        "    \n",
        "    print(f\"üì¶ BERT-CRF Fold {fold_num} datasets:\")\n",
        "    print(f\"  Training: {len(train_dataset)} examples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
        "    \n",
        "    # Create BERT-CRF model and trainer for this fold\n",
        "    print(f\"\\nü§ñ Creating BERT-CRF model and trainer for fold {fold_num}...\")\n",
        "    model, trainer, fold_output_dir = create_bert_crf_model_and_trainer(\n",
        "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, device\n",
        "    )\n",
        "    \n",
        "    # Train and evaluate this fold\n",
        "    fold_result = train_and_evaluate_bert_crf_fold(fold_num, trainer, val_dataset, ner_dataset)\n",
        "    fold_results.append(fold_result)\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del model, trainer, train_dataset, val_dataset\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    print(f\"\\n‚úÖ BERT-CRF Fold {fold_num} completed!\")\n",
        "    print(f\"   Precision: {fold_result['precision']:.4f}\")\n",
        "    print(f\"   Recall: {fold_result['recall']:.4f}\")\n",
        "    print(f\"   F1-Score: {fold_result['f1']:.4f}\")\n",
        "    print(f\"   Accuracy: {fold_result['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"BERT-CRF K-FOLD CROSS-VALIDATION COMPLETED!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fold_results_analysis"
      },
      "source": [
        "## 11. BERT-CRF Results Analysis and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fold_summary_stats"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BERT-CRF K-FOLD RESULTS SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"BERT-CRF K-FOLD CROSS-VALIDATION RESULTS SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Extract metrics from all folds\n",
        "precisions = [result['precision'] for result in fold_results]\n",
        "recalls = [result['recall'] for result in fold_results]\n",
        "f1_scores = [result['f1'] for result in fold_results]\n",
        "accuracies = [result['accuracy'] for result in fold_results]\n",
        "\n",
        "# Calculate statistics\n",
        "print(f\"\\nüìä BERT-CRF PERFORMANCE METRICS ACROSS {N_FOLDS} FOLDS:\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "print(f\"\\nüéØ PRECISION:\")\n",
        "print(f\"  Mean: {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\")\n",
        "print(f\"  Min:  {np.min(precisions):.4f} (Fold {np.argmin(precisions) + 1})\")\n",
        "print(f\"  Max:  {np.max(precisions):.4f} (Fold {np.argmax(precisions) + 1})\")\n",
        "\n",
        "print(f\"\\nüéØ RECALL:\")\n",
        "print(f\"  Mean: {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\")\n",
        "print(f\"  Min:  {np.min(recalls):.4f} (Fold {np.argmin(recalls) + 1})\")\n",
        "print(f\"  Max:  {np.max(recalls):.4f} (Fold {np.argmax(recalls) + 1})\")\n",
        "\n",
        "print(f\"\\nüéØ F1-SCORE:\")\n",
        "print(f\"  Mean: {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
        "print(f\"  Min:  {np.min(f1_scores):.4f} (Fold {np.argmin(f1_scores) + 1})\")\n",
        "print(f\"  Max:  {np.max(f1_scores):.4f} (Fold {np.argmax(f1_scores) + 1})\")\n",
        "\n",
        "print(f\"\\nüéØ ACCURACY:\")\n",
        "print(f\"  Mean: {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")\n",
        "print(f\"  Min:  {np.min(accuracies):.4f} (Fold {np.argmin(accuracies) + 1})\")\n",
        "print(f\"  Max:  {np.max(accuracies):.4f} (Fold {np.argmax(accuracies) + 1})\")\n",
        "\n",
        "# Individual fold results\n",
        "print(f\"\\nüìã INDIVIDUAL BERT-CRF FOLD RESULTS:\")\n",
        "print(f\"{'='*50}\")\n",
        "for i, result in enumerate(fold_results, 1):\n",
        "    print(f\"Fold {i}: P={result['precision']:.4f}, R={result['recall']:.4f}, F1={result['f1']:.4f}, Acc={result['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_crf_save_results"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAVE BERT-CRF RESULTS TO FILE\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'architecture': 'BERT-CRF',\n",
        "        'n_folds': N_FOLDS,\n",
        "        'total_examples': len(prepared_examples),\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'device': str(device)\n",
        "    },\n",
        "    'overall_metrics': {\n",
        "        'precision': {\n",
        "            'mean': float(np.mean(precisions)),\n",
        "            'std': float(np.std(precisions)),\n",
        "            'min': float(np.min(precisions)),\n",
        "            'max': float(np.max(precisions))\n",
        "        },\n",
        "        'recall': {\n",
        "            'mean': float(np.mean(recalls)),\n",
        "            'std': float(np.std(recalls)),\n",
        "            'min': float(np.min(recalls)),\n",
        "            'max': float(np.max(recalls))\n",
        "        },\n",
        "        'f1_score': {\n",
        "            'mean': float(np.mean(f1_scores)),\n",
        "            'std': float(np.std(f1_scores)),\n",
        "            'min': float(np.min(f1_scores)),\n",
        "            'max': float(np.max(f1_scores))\n",
        "        },\n",
        "        'accuracy': {\n",
        "            'mean': float(np.mean(accuracies)),\n",
        "            'std': float(np.std(accuracies)),\n",
        "            'min': float(np.min(accuracies)),\n",
        "            'max': float(np.max(accuracies))\n",
        "        }\n",
        "    },\n",
        "    'fold_results': [\n",
        "        {\n",
        "            'fold': result['fold'],\n",
        "            'precision': float(result['precision']),\n",
        "            'recall': float(result['recall']),\n",
        "            'f1': float(result['f1']),\n",
        "            'accuracy': float(result['accuracy'])\n",
        "        }\n",
        "        for result in fold_results\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "results_file = f\"{OUTPUT_DIR}/bert_crf_5fold_cv_results.json\"\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ BERT-CRF Results saved to: {results_file}\")\n",
        "\n",
        "# Create CSV for easy analysis\n",
        "df_results = pd.DataFrame([\n",
        "    {\n",
        "        'Fold': result['fold'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1'],\n",
        "        'Accuracy': result['accuracy']\n",
        "    }\n",
        "    for result in fold_results\n",
        "])\n",
        "\n",
        "# Add summary row\n",
        "summary_row = {\n",
        "    'Fold': 'Mean ¬± Std',\n",
        "    'Precision': f\"{np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\",\n",
        "    'Recall': f\"{np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\",\n",
        "    'F1-Score': f\"{np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\",\n",
        "    'Accuracy': f\"{np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\"\n",
        "}\n",
        "\n",
        "df_results = pd.concat([df_results, pd.DataFrame([summary_row])], ignore_index=True)\n",
        "\n",
        "csv_file = f\"{OUTPUT_DIR}/bert_crf_5fold_cv_results.csv\"\n",
        "df_results.to_csv(csv_file, index=False)\n",
        "print(f\"‚úÖ BERT-CRF Results CSV saved to: {csv_file}\")\n",
        "\n",
        "# Display final summary table\n",
        "print(f\"\\nüìä BERT-CRF FINAL RESULTS TABLE:\")\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## 12. Conclusion\n",
        "\n",
        "This notebook successfully implemented 5-fold cross-validation for the Serbian Legal NER pipeline using BERT-CRF architecture.\n",
        "\n",
        "### Key Achievements:\n",
        "- ‚úÖ **BERT-CRF Implementation**: Combined BERT embeddings with CRF layer for better sequence modeling\n",
        "- ‚úÖ **Robust Evaluation**: 5-fold cross-validation provides reliable performance estimates\n",
        "- ‚úÖ **CRF Decoding**: Proper CRF decoding for optimal sequence labeling\n",
        "- ‚úÖ **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracked across all folds\n",
        "- ‚úÖ **Results Persistence**: JSON and CSV files saved for comparison with other models\n",
        "\n",
        "### BERT-CRF Advantages:\n",
        "- **Better Sequence Constraints**: CRF enforces valid BIO tag transitions\n",
        "- **Global Optimization**: Considers entire sequence for optimal labeling\n",
        "- **Improved Entity Boundaries**: More accurate entity span detection\n",
        "\n",
        "The BERT-CRF 5-fold cross-validation results can now be compared with the base BERT model to evaluate the impact of the CRF layer!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
