{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCxJ3lfIjTOm"
   },
   "source": [
    "# Serbian Legal Named Entity Recognition (NER) Pipeline - 5-Fold Cross-Validation with Class Weights\n",
    "\n",
    "This notebook implements 5-fold cross-validation for the Serbian Legal NER pipeline using the base BERT model (classla/bcms-bertic) **with class weights** to handle class imbalance.\n",
    "\n",
    "## Key Features\n",
    "- **5-Fold Cross-Validation**: Robust evaluation across different data splits\n",
    "- **Base BERT Architecture**: Uses classla/bcms-bertic for token classification\n",
    "- **Class Weights**: Weighted loss function to handle imbalanced entity distribution\n",
    "- **Sliding Window Tokenization**: Handles long sequences without truncation\n",
    "- **Comprehensive Metrics**: Precision, recall, F1-score, and accuracy tracking\n",
    "- **Statistical Analysis**: Mean and standard deviation across folds\n",
    "\n",
    "## Entity Types\n",
    "- **COURT**: Court institutions\n",
    "- **DECISION_DATE**: Dates of legal decisions\n",
    "- **CASE_NUMBER**: Case identifiers\n",
    "- **CRIMINAL_ACT**: Criminal acts/charges\n",
    "- **PROSECUTOR**: Prosecutor entities\n",
    "- **DEFENDANT**: Defendant entities\n",
    "- **JUDGE**: Judge names\n",
    "- **REGISTRAR**: Court registrar\n",
    "- **SANCTION**: Sanctions/penalties\n",
    "- **SANCTION_TYPE**: Type of sanction\n",
    "- **SANCTION_VALUE**: Value/duration of sanction\n",
    "- **PROVISION**: Legal provisions\n",
    "- **PROCEDURE_COSTS**: Legal procedure costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxHG6Rs8jTOo"
   },
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11kIuCNPjTOo"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch datasets tokenizers scikit-learn seqeval pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3ig3oNUjTOo"
   },
   "outputs": [],
   "source": [
    "# Import shared modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/shared/')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import shared\n",
    "import shared.model_utils\n",
    "import shared.data_processing\n",
    "import shared.dataset\n",
    "import shared.evaluation\n",
    "import shared.config\n",
    "importlib.reload(shared.config)\n",
    "importlib.reload(shared.data_processing)\n",
    "importlib.reload(shared.dataset)\n",
    "importlib.reload(shared.model_utils)\n",
    "importlib.reload(shared.evaluation)\n",
    "importlib.reload(shared)\n",
    "\n",
    "# Import from shared modules\n",
    "from shared import (\n",
    "    # Configuration\n",
    "    ENTITY_TYPES, BIO_LABELS,\n",
    "    get_default_model_config, setup_environment,\n",
    "\n",
    "    # Data processing\n",
    "    LabelStudioToBIOConverter, load_labelstudio_data,\n",
    "    analyze_labelstudio_data, validate_bio_examples,\n",
    "\n",
    "    # Dataset\n",
    "    NERDataset, tokenize_and_align_labels_with_sliding_window,\n",
    "    create_huggingface_datasets,\n",
    "\n",
    "    # Model utilities\n",
    "    load_model_and_tokenizer, create_training_arguments,\n",
    "    detailed_evaluation, setup_device_and_seed,\n",
    "    PerClassMetricsCallback,\n",
    "\n",
    "    # Comprehensive tracking\n",
    "    analyze_entity_distribution_per_fold,\n",
    "    generate_detailed_classification_report,\n",
    "    # Aggregate functions\n",
    "    create_aggregate_report_across_folds\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DataCollatorForTokenClassification, AutoTokenizer, Trainer\n",
    "from collections import Counter\n",
    "\n",
    "# Setup device and random seed\n",
    "device = setup_device_and_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R56QtmR7fIY2"
   },
   "source": [
    "## 2. Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i3CBXt2fIY3"
   },
   "outputs": [],
   "source": [
    "# Setup environment and paths\n",
    "env_setup = setup_environment(use_local=False, create_dirs=False)\n",
    "paths = env_setup['paths']\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"classla/bcms-bertic\"\n",
    "model_config = get_default_model_config()\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{paths['models_dir']}/bertic_base_class_weights_5fold_cv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"  BIO labels: {len(BIO_LABELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVSwtVS1fIY3"
   },
   "source": [
    "## 3. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vmq8DjhyfIY4"
   },
   "outputs": [],
   "source": [
    "# Load LabelStudio data\n",
    "labelstudio_data = load_labelstudio_data(paths['labelstudio_json'])\n",
    "\n",
    "# Analyze the data\n",
    "if labelstudio_data:\n",
    "    analysis = analyze_labelstudio_data(labelstudio_data)\n",
    "else:\n",
    "    print(\"‚ùå No data loaded. Please check your paths.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAyV4gQfIY4"
   },
   "source": [
    "## 4. Data Preprocessing and BIO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOe5xEDnfIY5"
   },
   "outputs": [],
   "source": [
    "# Convert LabelStudio data to BIO format\n",
    "converter = LabelStudioToBIOConverter(\n",
    "    judgments_dir=paths['judgments_dir'],\n",
    "    labelstudio_files_dir=paths.get('labelstudio_files_dir')\n",
    ")\n",
    "\n",
    "bio_examples = converter.convert_to_bio(labelstudio_data)\n",
    "print(f\"‚úÖ Converted {len(bio_examples)} examples to BIO format\")\n",
    "\n",
    "# Validate BIO examples\n",
    "valid_examples, stats = validate_bio_examples(bio_examples)\n",
    "print(f\"üìä Validation complete: {stats['valid_examples']} valid examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HcyAbInfIY5"
   },
   "source": [
    "## 5. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cX7CQA3fIY5"
   },
   "outputs": [],
   "source": [
    "# Create NER dataset\n",
    "ner_dataset = NERDataset(valid_examples)\n",
    "prepared_examples = ner_dataset.prepare_for_training()\n",
    "\n",
    "print(f\"üìä Dataset statistics:\")\n",
    "print(f\"  Number of unique labels: {ner_dataset.get_num_labels()}\")\n",
    "print(f\"  Prepared examples: {len(prepared_examples)}\")\n",
    "\n",
    "# Get label statistics\n",
    "label_stats = ner_dataset.get_label_statistics()\n",
    "print(f\"  Total tokens: {label_stats['total_tokens']}\")\n",
    "print(f\"  Entity types found: {len(label_stats['entity_counts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "class_weights_section"
   },
   "source": [
    "## 6. Class Weights Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "class_weights_functions"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLASS WEIGHTS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_class_weights_from_tokenized(tokenized_examples, label_to_id):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on label frequency in tokenized training data.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_examples: List of tokenized training examples (with integer label IDs)\n",
    "        label_to_id: Dictionary mapping labels to IDs\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Class weights tensor\n",
    "    \"\"\"\n",
    "    # Collect all label IDs from training examples, filtering out -100 (ignore index)\n",
    "    all_label_ids = []\n",
    "    for example in tokenized_examples:\n",
    "        # Filter out -100 values (used for padding/subword tokens)\n",
    "        valid_labels = [label for label in example['labels'] if label != -100]\n",
    "        all_label_ids.extend(valid_labels)\n",
    "    \n",
    "    # Get unique classes that actually appear in the training data\n",
    "    unique_labels_in_data = np.array(sorted(list(set(all_label_ids))))\n",
    "    \n",
    "    # Calculate class weights using sklearn's balanced approach for labels that appear\n",
    "    class_weights_for_present = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_labels_in_data,\n",
    "        y=np.array(all_label_ids)\n",
    "    )\n",
    "    \n",
    "    # Create full weight array for all possible labels\n",
    "    num_labels = len(label_to_id)\n",
    "    class_weights = np.ones(num_labels)  # Default weight of 1.0 for unseen labels\n",
    "    \n",
    "    # Fill in calculated weights for labels that appear in training data\n",
    "    for label_id, weight in zip(unique_labels_in_data, class_weights_for_present):\n",
    "        class_weights[label_id] = weight\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "    \n",
    "    print(f\"üìä Class weights calculated:\")\n",
    "    print(f\"  Total label types: {num_labels}\")\n",
    "    print(f\"  Labels present in training: {len(unique_labels_in_data)}\")\n",
    "    print(f\"  Labels absent from training: {num_labels - len(unique_labels_in_data)}\")\n",
    "    print(f\"  Total valid tokens: {len(all_label_ids)}\")\n",
    "    print(f\"  Weight range: {class_weights.min():.4f} - {class_weights.max():.4f}\")\n",
    "    print(f\"  Mean weight: {class_weights.mean():.4f}\")\n",
    "    \n",
    "    return class_weights_tensor\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that uses weighted CrossEntropyLoss for handling class imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute weighted loss for token classification.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Move class weights to the same device as logits\n",
    "            if self.class_weights is not None:\n",
    "                class_weights = self.class_weights.to(logits.device)\n",
    "            else:\n",
    "                class_weights = None\n",
    "            \n",
    "            # Create weighted loss function\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "            \n",
    "            # Flatten for loss calculation\n",
    "            active_loss = labels.view(-1) != -100\n",
    "            active_logits = logits.view(-1, logits.shape[-1])\n",
    "            active_labels = torch.where(\n",
    "                active_loss,\n",
    "                labels.view(-1),\n",
    "                torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "            \n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"‚úÖ Class weights functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEtKaFXofIY5"
   },
   "source": [
    "## 7. K-Fold Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOZoMYBNfIY6"
   },
   "outputs": [],
   "source": [
    "# Set up 5-fold cross-validation\n",
    "N_FOLDS = 5\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to numpy array for easier indexing\n",
    "examples_array = np.array(prepared_examples, dtype=object)\n",
    "\n",
    "print(f\"Setting up {N_FOLDS}-fold cross-validation\")\n",
    "print(f\"Total examples: {len(prepared_examples)}\")\n",
    "print(f\"Examples per fold (approx): {len(prepared_examples) // N_FOLDS}\")\n",
    "\n",
    "# Load tokenizer (will be used across all folds)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"\\nLoaded tokenizer for {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Store results from all folds\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_helper_functions"
   },
   "source": [
    "## 8. K-Fold Cross-Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_fold_helper_functions_code"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# K-FOLD CROSS-VALIDATION HELPER FUNCTIONS WITH CLASS WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_fold_data_with_class_weights(train_examples, val_examples, tokenizer, ner_dataset):\n",
    "    \"\"\"\n",
    "    Prepare training and validation datasets for a specific fold with class weights.\n",
    "\n",
    "    Args:\n",
    "        train_examples: Training examples for this fold\n",
    "        val_examples: Validation examples for this fold\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, data_collator, class_weights)\n",
    "    \"\"\"\n",
    "    # Tokenize datasets with sliding window\n",
    "    train_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        train_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "\n",
    "    val_tokenized = tokenize_and_align_labels_with_sliding_window(\n",
    "        val_examples, tokenizer, ner_dataset.label_to_id,\n",
    "        max_length=model_config['max_length'], stride=model_config['stride']\n",
    "    )\n",
    "\n",
    "    # Calculate class weights from training data\n",
    "    print(\"‚öñÔ∏è  Calculating class weights from training data...\")\n",
    "    class_weights = calculate_class_weights_from_tokenized(train_tokenized, ner_dataset.label_to_id)\n",
    "\n",
    "    # Create HuggingFace datasets\n",
    "    train_dataset, val_dataset, _ = create_huggingface_datasets(\n",
    "        train_tokenized, val_tokenized, val_tokenized  # Using val as placeholder for test\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, data_collator, class_weights\n",
    "\n",
    "print(\"‚úÖ K-fold helper functions with class weights defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_fold_helper_functions_2"
   },
   "outputs": [],
   "source": [
    "def create_model_and_weighted_trainer(fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device):\n",
    "    \"\"\"\n",
    "    Create model and weighted trainer for a specific fold with comprehensive metrics tracking.\n",
    "\n",
    "    Args:\n",
    "        fold_num: Current fold number\n",
    "        train_dataset: Training dataset for this fold\n",
    "        val_dataset: Validation dataset for this fold\n",
    "        data_collator: Data collator\n",
    "        tokenizer: Tokenizer instance\n",
    "        ner_dataset: NER dataset instance\n",
    "        class_weights: Class weights tensor\n",
    "        device: Device to use (cuda/cpu)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, trainer, metrics_callback, fold_output_dir)\n",
    "    \"\"\"\n",
    "    # Create fold-specific output directory\n",
    "    fold_output_dir = f\"{OUTPUT_DIR}/fold_{fold_num}\"\n",
    "    import os\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "\n",
    "    # Load fresh model for this fold\n",
    "    model, _ = load_model_and_tokenizer(\n",
    "        MODEL_NAME,\n",
    "        ner_dataset.get_num_labels(),\n",
    "        ner_dataset.id_to_label,\n",
    "        ner_dataset.label_to_id\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Create training arguments for this fold\n",
    "    training_args = create_training_arguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        num_epochs=model_config['num_epochs'],\n",
    "        batch_size=model_config['batch_size'],\n",
    "        learning_rate=model_config['learning_rate'],\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_steps=100,\n",
    "        save_steps=500,\n",
    "        early_stopping_patience=3\n",
    "    )\n",
    "\n",
    "    # Create metrics callback for comprehensive tracking\n",
    "    metrics_callback = PerClassMetricsCallback(id_to_label=ner_dataset.id_to_label)\n",
    "\n",
    "    # Import compute_metrics from model_utils (not exported in __init__.py)\n",
    "    from shared.model_utils import compute_metrics\n",
    "    from transformers import EarlyStoppingCallback\n",
    "    \n",
    "    # Create compute_metrics function with id_to_label bound\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        return compute_metrics(eval_pred, ner_dataset.id_to_label)\n",
    "\n",
    "    # Build callbacks list\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3), metrics_callback]\n",
    "\n",
    "    # Create weighted trainer with class weights\n",
    "    print(f\"‚öñÔ∏è  Creating WeightedTrainer with class weights for fold {fold_num}\")\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    print(f\"Weighted trainer initialized for fold {fold_num} with comprehensive metrics tracking\")\n",
    "    return model, trainer, metrics_callback, fold_output_dir\n",
    "\n",
    "print(\"‚úÖ Model and weighted trainer creation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_fold_helper_functions_3"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_fold(fold_num, trainer, val_dataset, ner_dataset):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model for a specific fold.\n",
    "\n",
    "    Args:\n",
    "        fold_num: Current fold number\n",
    "        trainer: Trainer instance\n",
    "        val_dataset: Validation dataset for this fold\n",
    "        ner_dataset: NER dataset instance\n",
    "\n",
    "    Returns:\n",
    "        dict: Fold results including metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüèãÔ∏è  Training fold {fold_num} with class weights...\")\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"üíæ Saving model for fold {fold_num}...\")\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print(f\"üìä Evaluating fold {fold_num}...\")\n",
    "    eval_results = detailed_evaluation(\n",
    "        trainer, val_dataset, f\"Fold {fold_num} Validation\", ner_dataset.id_to_label\n",
    "    )\n",
    "\n",
    "    # Extract metrics\n",
    "    fold_result = {\n",
    "        'fold': fold_num,\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f1': eval_results['f1'],\n",
    "        'accuracy': eval_results['accuracy'],\n",
    "        'true_predictions': eval_results['true_predictions'],\n",
    "        'true_labels': eval_results['true_labels']\n",
    "    }\n",
    "\n",
    "    print(f\"\\nFold {fold_num} completed successfully!\")\n",
    "    return fold_result\n",
    "\n",
    "print(\"‚úÖ Training and evaluation helper function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fold_main_training"
   },
   "source": [
    "## 9. K-Fold Cross-Validation Training Loop with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_fold_main_loop"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN K-FOLD CROSS-VALIDATION LOOP WITH CLASS WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING 5-FOLD CROSS-VALIDATION WITH CLASS WEIGHTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total examples: {len(prepared_examples)}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(examples_array), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold_num}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    train_examples = examples_array[train_idx].tolist()\n",
    "    val_examples = examples_array[val_idx].tolist()\n",
    "    \n",
    "    print(f\"Training examples: {len(train_examples)}\")\n",
    "    print(f\"Validation examples: {len(val_examples)}\")\n",
    "    \n",
    "    # Analyze entity distributions for this fold\n",
    "    print(f\"\\nüìä Analyzing entity distributions...\")\n",
    "    train_dist = analyze_entity_distribution_per_fold(train_examples, f\"Fold {fold_num} - Training\")\n",
    "    val_dist = analyze_entity_distribution_per_fold(val_examples, f\"Fold {fold_num} - Validation\")\n",
    "    \n",
    "    # Prepare data for this fold with class weights\n",
    "    print(f\"\\nüî§ Preparing data for fold {fold_num} with class weights...\")\n",
    "    train_dataset, val_dataset, data_collator, class_weights = prepare_fold_data_with_class_weights(\n",
    "        train_examples, val_examples, tokenizer, ner_dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"üì¶ Fold {fold_num} datasets:\")\n",
    "    print(f\"  Training: {len(train_dataset)} examples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "    \n",
    "    # Create model and weighted trainer for this fold\n",
    "    print(f\"\\nü§ñ Creating model and weighted trainer for fold {fold_num}...\")\n",
    "    model, trainer, metrics_callback, fold_output_dir = create_model_and_weighted_trainer(\n",
    "        fold_num, train_dataset, val_dataset, data_collator, tokenizer, ner_dataset, class_weights, device\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate this fold\n",
    "    fold_result = train_and_evaluate_fold(fold_num, trainer, val_dataset, ner_dataset)\n",
    "    \n",
    "    # Get predictions and labels for aggregation\n",
    "    print(f\"\\nüìä Getting predictions for fold {fold_num}...\")\n",
    "    predictions, labels, _ = trainer.predict(val_dataset)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert to label names\n",
    "    true_labels = [[ner_dataset.id_to_label[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[ner_dataset.id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                   for prediction, label in zip(predictions, labels)]\n",
    "    \n",
    "    # Generate per-class metrics and confusion matrix for this fold\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    flat_true = [label for seq in true_labels for label in seq]\n",
    "    flat_pred = [label for seq in pred_labels for label in seq]\n",
    "    all_labels = sorted(list(set(flat_true + flat_pred)))\n",
    "    cm = confusion_matrix(flat_true, flat_pred, labels=all_labels)\n",
    "    \n",
    "    # Generate classification report for this fold\n",
    "    per_class_metrics = generate_detailed_classification_report(\n",
    "        true_labels, pred_labels, fold_output_dir, fold_num, \"Class Weights Validation\"\n",
    "    )\n",
    "    \n",
    "    # Store comprehensive data for aggregation\n",
    "    fold_result['distributions'] = {'train': train_dist, 'val': val_dist}\n",
    "    fold_result['per_class_metrics'] = per_class_metrics\n",
    "    fold_result['confusion_matrix'] = cm\n",
    "    fold_result['labels'] = all_labels\n",
    "    fold_result['training_history'] = metrics_callback.get_training_history()\n",
    "    fold_results.append(fold_result)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model, trainer, train_dataset, val_dataset, metrics_callback\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"5-FOLD CROSS-VALIDATION WITH CLASS WEIGHTS COMPLETED\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aggregate_results"
   },
   "source": [
    "## 10. Aggregate Results Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aggregate_results_code"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS ACROSS ALL FOLDS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GENERATING AGGREGATE REPORT ACROSS ALL {N_FOLDS} FOLDS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create aggregate report with all visualizations displayed in notebook\n",
    "aggregate_report = create_aggregate_report_across_folds(\n",
    "    fold_results=fold_results,\n",
    "    model_name=\"BERTiƒá Base with Class Weights\",\n",
    "    display=True\n",
    ")\n",
    "\n",
    "# Calculate overall metrics from fold results\n",
    "precisions = [fold['precision'] for fold in fold_results]\n",
    "recalls = [fold['recall'] for fold in fold_results]\n",
    "f1_scores = [fold['f1'] for fold in fold_results]\n",
    "accuracies = [fold['accuracy'] for fold in fold_results]\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL RESULTS - BERTiƒá Base with Class Weights ({N_FOLDS}-Fold CV)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOverall Metrics (Mean ¬± Std):\")\n",
    "print(f\"  Precision: {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f}\")\n",
    "print(f\"  Recall:    {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f}\")\n",
    "print(f\"  F1-score:  {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
    "print(f\"  Accuracy:  {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")\n",
    "\n",
    "# Save aggregate report\n",
    "import json\n",
    "aggregate_report_path = f\"{OUTPUT_DIR}/aggregate_report.json\"\n",
    "with open(aggregate_report_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'model_name': 'BERTiƒá Base with Class Weights',\n",
    "        'n_folds': N_FOLDS,\n",
    "        'overall_metrics': {\n",
    "            'precision_mean': float(np.mean(precisions)),\n",
    "            'precision_std': float(np.std(precisions)),\n",
    "            'recall_mean': float(np.mean(recalls)),\n",
    "            'recall_std': float(np.std(recalls)),\n",
    "            'f1_mean': float(np.mean(f1_scores)),\n",
    "            'f1_std': float(np.std(f1_scores)),\n",
    "            'accuracy_mean': float(np.mean(accuracies)),\n",
    "            'accuracy_std': float(np.std(accuracies))\n",
    "        },\n",
    "        'fold_results': fold_results\n",
    "    }, f, indent=2, default=str)\n",
    "print(f\"\\nüíæ Saved aggregate report to {aggregate_report_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìä All visualizations displayed in notebook above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
