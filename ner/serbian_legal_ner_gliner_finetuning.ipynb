{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLiNER Fine-tuning: Serbian Legal NER with gliner_multiv2.1\n",
    "\n",
    "This notebook fine-tunes GLiNER multiv2.1 on 225 Serbian legal documents for Named Entity Recognition.\n",
    "\n",
    "## 🎯 **Fine-tuning Strategy:**\n",
    "Based on the zero-shot evaluation results:\n",
    "- **gliner_multiv2.1**: 1945 entities, 0.688 confidence, 32.4s, 60.0 entities/sec ✅ **SELECTED**\n",
    "- gliner-x-large: 18710 entities, 0.466 confidence, 230.2s, 81.3 entities/sec (too many false positives)\n",
    "- gliner_large-v2: 2112 entities, 0.517 confidence, 76.2s, 27.7 entities/sec\n",
    "- modern-gliner-bi-large-v1.0: 13 entities, 0.321 confidence, 272.3s, 0.0 entities/sec (poor performance)\n",
    "\n",
    "## 🏷️ **Entity Types (14 types):**\n",
    "COURT, DECISION_DATE, CASE_NUMBER, CRIMINAL_ACT, PROSECUTOR, DEFENDANT, JUDGE, REGISTRAR, VERDICT, SANCTION_TYPE, SANCTION_VALUE, PROVISION_MATERIAL, PROVISION_PROCEDURAL, PROCEDURE_COSTS\n",
    "\n",
    "## 📊 **Dataset:**\n",
    "- 225 annotated Serbian legal documents\n",
    "- LabelStudio JSON format with character-level annotations\n",
    "- Small dataset → Early stopping essential\n",
    "\n",
    "## 🔧 **Approach:**\n",
    "1. Convert LabelStudio annotations to GLiNER training format\n",
    "2. Split data with stratification (80/20 train/val)\n",
    "3. Fine-tune gliner_multiv2.1 with early stopping\n",
    "4. Evaluate against zero-shot baseline\n",
    "5. Save fine-tuned model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gliner torch transformers datasets scikit-learn seqeval matplotlib seaborn pandas tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "from gliner import GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🚀 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"⚠️ GPU not available - will use CPU (slower)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"✅ All dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LABELSTUDIO_JSON_PATH = \"annotations.json\"  # Path to LabelStudio annotations\n",
    "LABELSTUDIO_FILES_DIR = \"labelstudio_files\"  # Path to judgment text files\n",
    "MODEL_NAME = \"urchade/gliner_multiv2.1\"  # Base GLiNER model to fine-tune\n",
    "OUTPUT_DIR = \"./gliner_finetuned_serbian_legal\"  # Output directory for fine-tuned model\n",
    "CONFIDENCE_THRESHOLD = 0.3  # GLiNER confidence threshold\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAIN_BATCH_SIZE = 8  # Small batch size for limited GPU memory\n",
    "EVAL_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5  # Conservative learning rate for fine-tuning\n",
    "NUM_EPOCHS = 10  # Will use early stopping\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "EVAL_STEPS = 50  # Evaluate every 50 steps\n",
    "SAVE_STEPS = 100\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "# Data split\n",
    "TEST_SIZE = 0.2  # 80/20 train/validation split\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"🎯 Base model: {MODEL_NAME}\")\n",
    "print(f\"📂 Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"🔧 Training config: {NUM_EPOCHS} epochs, LR={LEARNING_RATE}, batch_size={TRAIN_BATCH_SIZE}\")\n",
    "print(f\"📊 Data split: {int((1-TEST_SIZE)*100)}% train, {int(TEST_SIZE*100)}% validation\")\n",
    "print(f\"⚡ Early stopping patience: {EARLY_STOPPING_PATIENCE} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity type mapping (from your existing setup)\n",
    "ENTITY_TYPES = [\n",
    "    \"COURT\",\n",
    "    \"DECISION_DATE\", \n",
    "    \"CASE_NUMBER\",\n",
    "    \"CRIMINAL_ACT\",\n",
    "    \"PROSECUTOR\",\n",
    "    \"DEFENDANT\",\n",
    "    \"JUDGE\",\n",
    "    \"REGISTRAR\",\n",
    "    \"VERDICT\",\n",
    "    \"SANCTION_TYPE\",\n",
    "    \"SANCTION_VALUE\",\n",
    "    \"PROVISION_MATERIAL\",\n",
    "    \"PROVISION_PROCEDURAL\",\n",
    "    \"PROCEDURE_COSTS\"\n",
    "]\n",
    "\n",
    "print(f\"🏷️ Entity types ({len(ENTITY_TYPES)}): {', '.join(ENTITY_TYPES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labelstudio_data() -> List[Dict]:\n",
    "    \"\"\"Load and parse LabelStudio annotations\"\"\"\n",
    "    print(\"📂 Loading LabelStudio annotations...\")\n",
    "    \n",
    "    try:\n",
    "        with open(LABELSTUDIO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            labelstudio_data = json.load(f)\n",
    "        print(f\"✅ Loaded {len(labelstudio_data)} annotated documents\")\n",
    "        return labelstudio_data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: {LABELSTUDIO_JSON_PATH} not found!\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ Error parsing JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_text_file(file_path: str) -> str:\n",
    "    \"\"\"Load text content from file\"\"\"\n",
    "    # Extract filename from path\n",
    "    if \"/\" in file_path:\n",
    "        filename = file_path.split(\"/\")[-1]\n",
    "    else:\n",
    "        filename = file_path\n",
    "    \n",
    "    full_path = Path(LABELSTUDIO_FILES_DIR) / filename\n",
    "    \n",
    "    if not full_path.exists():\n",
    "        print(f\"⚠️ Warning: File {full_path} not found\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {full_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load the data\n",
    "labelstudio_data = load_labelstudio_data()\n",
    "print(f\"📊 Loaded {len(labelstudio_data)} documents for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text using regex (same as GLiNER expects)\"\"\"\n",
    "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
    "\n",
    "def find_token_positions(tokens: List[str], entity_tokens: List[str], text: str, start_char: int, end_char: int) -> Tuple[int, int]:\n",
    "    \"\"\"Find token start and end positions for an entity\"\"\"\n",
    "    entity_text = text[start_char:end_char].lower()\n",
    "    \n",
    "    # Try to find exact match first\n",
    "    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "        token_text = \" \".join(tokens[i:i + len(entity_tokens)]).lower()\n",
    "        if token_text == entity_text:\n",
    "            return i, i + len(entity_tokens) - 1\n",
    "    \n",
    "    # Fallback: find partial matches\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i].lower() in entity_text:\n",
    "            for j in range(i, min(i + len(entity_tokens) + 2, len(tokens))):\n",
    "                partial_text = \" \".join(tokens[i:j+1]).lower()\n",
    "                if entity_text in partial_text:\n",
    "                    return i, j\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def convert_to_gliner_format(labelstudio_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert LabelStudio annotations to GLiNER training format\"\"\"\n",
    "    print(\"🔄 Converting annotations to GLiNER training format...\")\n",
    "    \n",
    "    gliner_examples = []\n",
    "    entity_counts = Counter()\n",
    "    skipped_files = 0\n",
    "    \n",
    "    for item in tqdm(labelstudio_data, desc=\"Converting annotations\"):\n",
    "        file_path = item.get(\"file_upload\", \"\")\n",
    "        \n",
    "        # Load text content\n",
    "        text_content = load_text_file(file_path)\n",
    "        if not text_content:\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokens = tokenize_text(text_content)\n",
    "        \n",
    "        # Extract entities from annotations\n",
    "        annotations = item.get(\"annotations\", [])\n",
    "        if not annotations:\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            spans = []\n",
    "            result = annotation.get(\"result\", [])\n",
    "            \n",
    "            for res in result:\n",
    "                if res.get(\"type\") == \"labels\":\n",
    "                    value = res[\"value\"]\n",
    "                    start_char = value[\"start\"]\n",
    "                    end_char = value[\"end\"]\n",
    "                    labels = value[\"labels\"]\n",
    "                    \n",
    "                    # Get the entity text\n",
    "                    entity_text = text_content[start_char:end_char]\n",
    "                    entity_tokens = tokenize_text(entity_text)\n",
    "                    \n",
    "                    # Find token positions\n",
    "                    token_start, token_end = find_token_positions(tokens, entity_tokens, text_content, start_char, end_char)\n",
    "                    \n",
    "                    if token_start is not None and token_end is not None:\n",
    "                        for label in labels:\n",
    "                            if label in ENTITY_TYPES:\n",
    "                                spans.append((token_start, token_end, label))\n",
    "                                entity_counts[label] += 1\n",
    "            \n",
    "            # Create GLiNER example in correct format\n",
    "            if spans:  # Only include examples with entities\n",
    "                gliner_example = {\n",
    "                    \"tokenized_text\": tokens,\n",
    "                    \"ner\": spans\n",
    "                }\n",
    "                gliner_examples.append(gliner_example)\n",
    "    \n",
    "    print(f\"✅ Converted {len(gliner_examples)} examples\")\n",
    "    print(f\"⚠️ Skipped {skipped_files} files (missing text or annotations)\")\n",
    "    \n",
    "    # Print entity distribution\n",
    "    print(f\"\\n📊 Entity distribution:\")\n",
    "    for entity_type in ENTITY_TYPES:\n",
    "        count = entity_counts.get(entity_type, 0)\n",
    "        print(f\"  {entity_type}: {count}\")\n",
    "    \n",
    "    total_entities = sum(entity_counts.values())\n",
    "    print(f\"\\n🎯 Total entities: {total_entities}\")\n",
    "    print(f\"📄 Average entities per document: {total_entities/len(gliner_examples):.1f}\")\n",
    "    \n",
    "    return gliner_examples\n",
    "\n",
    "# Convert data to GLiNER format\n",
    "gliner_examples = convert_to_gliner_format(labelstudio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_by_entities(examples: List[Dict], test_size: float = 0.2) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Split data ensuring entity type representation in both train and validation sets\"\"\"\n",
    "    print(f\"📊 Splitting data: {int((1-test_size)*100)}% train, {int(test_size*100)}% validation\")\n",
    "    \n",
    "    # Create entity type vectors for stratification\n",
    "    entity_vectors = []\n",
    "    for example in examples:\n",
    "        # Create binary vector for entity types present in this example\n",
    "        entity_vector = [0] * len(ENTITY_TYPES)\n",
    "        present_types = set(entity['label'] for entity in example['entities'])\n",
    "        \n",
    "        for i, entity_type in enumerate(ENTITY_TYPES):\n",
    "            if entity_type in present_types:\n",
    "                entity_vector[i] = 1\n",
    "        \n",
    "        entity_vectors.append(tuple(entity_vector))  # Convert to tuple for hashing\n",
    "    \n",
    "    # Use stratify parameter to maintain entity distribution\n",
    "    try:\n",
    "        train_examples, val_examples = train_test_split(\n",
    "            examples, \n",
    "            test_size=test_size, \n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=entity_vectors\n",
    "        )\n",
    "    except ValueError:\n",
    "        # If stratification fails (too few samples), use random split\n",
    "        print(\"⚠️ Stratification failed, using random split\")\n",
    "        train_examples, val_examples = train_test_split(\n",
    "            examples, \n",
    "            test_size=test_size, \n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Split complete: {len(train_examples)} train, {len(val_examples)} validation\")\n",
    "    \n",
    "    # Analyze entity distribution in splits\n",
    "    train_entities = Counter()\n",
    "    val_entities = Counter()\n",
    "    \n",
    "    for example in train_examples:\n",
    "        for entity in example['entities']:\n",
    "            train_entities[entity['label']] += 1\n",
    "    \n",
    "    for example in val_examples:\n",
    "        for entity in example['entities']:\n",
    "            val_entities[entity['label']] += 1\n",
    "    \n",
    "    print(f\"\\n📊 Entity distribution in splits:\")\n",
    "    print(f\"{'Entity Type':<20} {'Train':<8} {'Val':<8} {'Total':<8} {'Val %':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for entity_type in ENTITY_TYPES:\n",
    "        train_count = train_entities.get(entity_type, 0)\n",
    "        val_count = val_entities.get(entity_type, 0)\n",
    "        total_count = train_count + val_count\n",
    "        val_percentage = (val_count / total_count * 100) if total_count > 0 else 0\n",
    "        \n",
    "        print(f\"{entity_type:<20} {train_count:<8} {val_count:<8} {total_count:<8} {val_percentage:<8.1f}\")\n",
    "    \n",
    "    return train_examples, val_examples\n",
    "\n",
# Data will be split in the training setup section
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GLiNER Dataset and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "random.shuffle(gliner_examples)\n",
    "split_idx = int(len(gliner_examples) * 0.8)\n",
    "\n",
    "train_dataset = gliner_examples[:split_idx]\n",
    "val_dataset = gliner_examples[split_idx:]\n",
    "\n",
    "print(f\"📊 Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"📊 Validation dataset: {len(val_dataset)} examples\")\n",
    "\n",
    "# Calculate training parameters\n",
    "num_steps = 500\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "data_size = len(train_dataset)\n",
    "num_batches = data_size // batch_size\n",
    "num_epochs = max(1, min(NUM_EPOCHS, num_steps // num_batches))\n",
    "\n",
    "print(f\"🔧 Training configuration:\")\n",
    "print(f\"  - Epochs: {num_epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=False,\n",
    "    report_to=\"none\",  # Disable wandb\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✅ Training arguments created\")\n",
    "print(f\"🔧 Key settings: LR={training_args.learning_rate}, Batch={training_args.per_device_train_batch_size}, Epochs={training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model() -> GLiNER:\n",
    "    \"\"\"Load the base GLiNER model for fine-tuning\"\"\"\n",
    "    print(f\"🤖 Loading base model: {MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        model = GLiNER.from_pretrained(MODEL_NAME)\n",
    "        print(f\"✅ Model loaded successfully\")\n",
    "        \n",
    "        # Move to device\n",
    "        model = model.to(device)\n",
    "        print(f\"📱 Model moved to {device}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the base model\n",
    "model = load_base_model()\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(\"Failed to load base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback:\n",
    "    \"\"\"Early stopping callback for training\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 3, min_delta: float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, eval_loss: float) -> bool:\n",
    "        if eval_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = eval_loss\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        if self.patience_counter >= self.patience:\n",
    "            self.should_stop = True\n",
    "            print(f\"\\n🛑 Early stopping triggered after {self.patience} evaluations without improvement\")\n",
    "            print(f\"📊 Best validation loss: {self.best_loss:.4f}\")\n",
    "        \n",
    "        return self.should_stop\n",
    "\n",
    "def create_trainer(model: GLiNER, training_args: TrainingArguments) -> Trainer:\n",
    "    \"\"\"Create GLiNER trainer with early stopping\"\"\"\n",
    "    print(\"🏋️ Creating trainer...\")\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=model.data_processor.transformer_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Trainer created successfully\")\n",
    "    return trainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = create_trainer(model, training_args)\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "print(f\"🎯 Ready for training with early stopping (patience={EARLY_STOPPING_PATIENCE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(trainer: Trainer, early_stopping: EarlyStoppingCallback) -> Dict:\n",
    "    \"\"\"Train the model with early stopping\"\"\"\n",
    "    print(\"🚀 Starting fine-tuning...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Store training metrics\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'eval_loss': [],\n",
    "        'learning_rate': [],\n",
    "        'epoch': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(f\"\\n✅ Training completed!\")\n",
    "        print(f\"⏱️ Training time: {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")\n",
    "        print(f\"📊 Final training loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        print(f\"\\n💾 Saving fine-tuned model to {OUTPUT_DIR}\")\n",
    "        trainer.save_model()\n",
    "        \n",
    "        # Save training metrics\n",
    "        training_history['final_loss'] = train_result.training_loss\n",
    "        training_history['training_time'] = training_time\n",
    "        training_history['total_steps'] = train_result.global_step\n",
    "        \n",
    "        return training_history\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        return training_history\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed: {e}\")\n",
    "        return training_history\n",
    "\n",
    "# Execute training\n",
    "training_history = train_model_with_early_stopping(trainer, early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetuned_model(model_path: str, val_examples: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate the fine-tuned model\"\"\"\n",
    "    print(f\"🧪 Evaluating fine-tuned model from {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load fine-tuned model\n",
    "        finetuned_model = GLiNER.from_pretrained(model_path)\n",
    "        finetuned_model = finetuned_model.to(device)\n",
    "        print(f\"✅ Fine-tuned model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    print(f\"📊 Evaluating on {len(val_examples)} validation examples...\")\n",
    "    \n",
    "    for example in tqdm(val_examples, desc=\"Evaluating\"):\n",
    "        text = example['text']\n",
    "        true_entities = example['entities']\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_entities = finetuned_model.predict_entities(text, ENTITY_TYPES, threshold=CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        # Store results\n",
    "        predictions.append(pred_entities)\n",
    "        ground_truth.append(true_entities)\n",
    "        \n",
    "        # Collect confidence scores\n",
    "        for entity in pred_entities:\n",
    "            confidence_scores.append(entity.get('score', 0.0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_pred = sum(len(preds) for preds in predictions)\n",
    "    total_true = sum(len(true) for true in ground_truth)\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'model_type': 'Fine-tuned GLiNER',\n",
    "        'model_path': model_path,\n",
    "        'total_pred_entities': total_pred,\n",
    "        'total_true_entities': total_true,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'examples_evaluated': len(val_examples)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Evaluation complete!\")\n",
    "    print(f\"  📊 True entities: {total_true}\")\n",
    "    print(f\"  🤖 Predicted entities: {total_pred}\")\n",
    "    print(f\"  ⚡ Average confidence: {avg_confidence:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "finetuned_results = evaluate_finetuned_model(OUTPUT_DIR, val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot_baseline(val_examples: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate zero-shot baseline for comparison\"\"\"\n",
    "    print(f\"🧪 Evaluating zero-shot baseline: {MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        # Load original model\n",
    "        baseline_model = GLiNER.from_pretrained(MODEL_NAME)\n",
    "        baseline_model = baseline_model.to(device)\n",
    "        print(f\"✅ Baseline model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading baseline model: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Use English entity descriptions for zero-shot (as in your original evaluation)\n",
    "    english_entity_types = [\n",
    "        \"court or tribunal\",  # COURT\n",
    "        \"judgment date or decision date\",  # DECISION_DATE\n",
    "        \"case number or case identifier\",  # CASE_NUMBER\n",
    "        \"criminal act or offense\",  # CRIMINAL_ACT\n",
    "        \"prosecutor or public prosecutor\",  # PROSECUTOR\n",
    "        \"defendant or accused person\",  # DEFENDANT\n",
    "        \"judge or judicial officer\",  # JUDGE\n",
    "        \"court registrar or clerk\",  # REGISTRAR\n",
    "        \"court verdict or judgment\",  # VERDICT\n",
    "        \"type of penalty or sanction\",  # SANCTION_TYPE\n",
    "        \"amount or duration of penalty\",  # SANCTION_VALUE\n",
    "        \"material legal provision or article\",  # PROVISION_MATERIAL\n",
    "        \"procedural legal provision or article\",  # PROVISION_PROCEDURAL\n",
    "        \"court procedure costs or fees\",  # PROCEDURE_COSTS\n",
    "    ]\n",
    "    \n",
    "    # Mapping from English to standard labels\n",
    "    english_to_standard = {\n",
    "        \"court or tribunal\": \"COURT\",\n",
    "        \"judgment date or decision date\": \"DECISION_DATE\",\n",
    "        \"case number or case identifier\": \"CASE_NUMBER\",\n",
    "        \"criminal act or offense\": \"CRIMINAL_ACT\",\n",
    "        \"prosecutor or public prosecutor\": \"PROSECUTOR\",\n",
    "        \"defendant or accused person\": \"DEFENDANT\",\n",
    "        \"judge or judicial officer\": \"JUDGE\",\n",
    "        \"court registrar or clerk\": \"REGISTRAR\",\n",
    "        \"court verdict or judgment\": \"VERDICT\",\n",
    "        \"type of penalty or sanction\": \"SANCTION_TYPE\",\n",
    "        \"amount or duration of penalty\": \"SANCTION_VALUE\",\n",
    "        \"material legal provision or article\": \"PROVISION_MATERIAL\",\n",
    "        \"procedural legal provision or article\": \"PROVISION_PROCEDURAL\",\n",
    "        \"court procedure costs or fees\": \"PROCEDURE_COSTS\",\n",
    "    }\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    print(f\"📊 Evaluating on {len(val_examples)} validation examples...\")\n",
    "    \n",
    "    for example in tqdm(val_examples, desc=\"Zero-shot evaluation\"):\n",
    "        text = example['text']\n",
    "        true_entities = example['entities']\n",
    "        \n",
    "        # Get predictions with English entity types\n",
    "        pred_entities_raw = baseline_model.predict_entities(text, english_entity_types, threshold=CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        # Convert English labels to standard labels\n",
    "        pred_entities = []\n",
    "        for entity in pred_entities_raw:\n",
    "            standard_label = english_to_standard.get(entity['label'], entity['label'])\n",
    "            pred_entities.append({\n",
    "                'start': entity['start'],\n",
    "                'end': entity['end'],\n",
    "                'label': standard_label,\n",
    "                'score': entity.get('score', 0.0)\n",
    "            })\n",
    "        \n",
    "        predictions.append(pred_entities)\n",
    "        ground_truth.append(true_entities)\n",
    "        \n",
    "        # Collect confidence scores\n",
    "        for entity in pred_entities:\n",
    "            confidence_scores.append(entity.get('score', 0.0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_pred = sum(len(preds) for preds in predictions)\n",
    "    total_true = sum(len(true) for true in ground_truth)\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'model_type': 'Zero-shot GLiNER',\n",
    "        'model_path': MODEL_NAME,\n",
    "        'total_pred_entities': total_pred,\n",
    "        'total_true_entities': total_true,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'examples_evaluated': len(val_examples)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Zero-shot evaluation complete!\")\n",
    "    print(f\"  📊 True entities: {total_true}\")\n",
    "    print(f\"  🤖 Predicted entities: {total_pred}\")\n",
    "    print(f\"  ⚡ Average confidence: {avg_confidence:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate zero-shot baseline\n",
    "baseline_results = evaluate_zero_shot_baseline(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(baseline_results: Dict, finetuned_results: Dict) -> None:\n",
    "    \"\"\"Compare zero-shot baseline vs fine-tuned model performance\"\"\"\n",
    "    print(\"\\n📊 MODEL COMPARISON: Zero-Shot vs Fine-tuned GLiNER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not baseline_results or not finetuned_results:\n",
    "        print(\"❌ Missing evaluation results for comparison\")\n",
    "        return\n",
    "    \n",
    "    # Overall statistics comparison\n",
    "    print(f\"\\n📈 Overall Performance:\")\n",
    "    print(f\"{'Metric':<25} {'Zero-Shot':<15} {'Fine-tuned':<15} {'Improvement':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Predicted entities\n",
    "    baseline_pred = baseline_results['total_pred_entities']\n",
    "    finetuned_pred = finetuned_results['total_pred_entities']\n",
    "    pred_change = ((finetuned_pred - baseline_pred) / baseline_pred * 100) if baseline_pred > 0 else 0\n",
    "    print(f\"{'Predicted Entities':<25} {baseline_pred:<15} {finetuned_pred:<15} {pred_change:+.1f}%\")\n",
    "    \n",
    "    # Average confidence\n",
    "    baseline_conf = baseline_results['avg_confidence']\n",
    "    finetuned_conf = finetuned_results['avg_confidence']\n",
    "    conf_change = ((finetuned_conf - baseline_conf) / baseline_conf * 100) if baseline_conf > 0 else 0\n",
    "    print(f\"{'Avg Confidence':<25} {baseline_conf:<15.3f} {finetuned_conf:<15.3f} {conf_change:+.1f}%\")\n",
    "    \n",
    "    # True entities (should be same)\n",
    "    true_entities = baseline_results['total_true_entities']\n",
    "    print(f\"{'True Entities':<25} {true_entities:<15} {true_entities:<15} {'Same':<15}\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(f\"\\n🔍 Analysis:\")\n",
    "    if pred_change > 0:\n",
    "        print(f\"  ✅ Fine-tuning increased entity detection by {pred_change:.1f}%\")\n",
    "    elif pred_change < 0:\n",
    "        print(f\"  ⚠️ Fine-tuning decreased entity detection by {abs(pred_change):.1f}%\")\n",
    "    else:\n",
    "        print(f\"  ➡️ Fine-tuning had no effect on entity detection count\")\n",
    "    \n",
    "    if conf_change > 0:\n",
    "        print(f\"  ✅ Fine-tuning improved confidence by {conf_change:.1f}%\")\n",
    "    elif conf_change < 0:\n",
    "        print(f\"  ⚠️ Fine-tuning decreased confidence by {abs(conf_change):.1f}%\")\n",
    "    else:\n",
    "        print(f\"  ➡️ Fine-tuning had no effect on confidence\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 Recommendations:\")\n",
    "    if pred_change > 10 and conf_change > 5:\n",
    "        print(f\"  🎯 Fine-tuning was successful! Deploy the fine-tuned model.\")\n",
    "    elif pred_change > 0 and conf_change > 0:\n",
    "        print(f\"  ✅ Fine-tuning shows improvement. Consider more training data or epochs.\")\n",
    "    elif pred_change < -10 or conf_change < -10:\n",
    "        print(f\"  ⚠️ Fine-tuning may have caused overfitting. Try lower learning rate or more regularization.\")\n",
    "    else:\n",
    "        print(f\"  🤔 Results are mixed. Analyze entity-level performance for insights.\")\n",
    "\n",
    "# Compare the models\n",
    "compare_models(baseline_results, finetuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(baseline_results: Dict, finetuned_results: Dict) -> None:\n",
    "    \"\"\"Create visualizations comparing model performance\"\"\"\n",
    "    if not baseline_results or not finetuned_results:\n",
    "        print(\"❌ Missing results for visualization\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('GLiNER Fine-tuning Results: Zero-Shot vs Fine-tuned', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Entity count comparison\n",
    "    models = ['Zero-Shot', 'Fine-tuned']\n",
    "    pred_counts = [baseline_results['total_pred_entities'], finetuned_results['total_pred_entities']]\n",
    "    true_count = baseline_results['total_true_entities']\n",
    "    \n",
    "    axes[0, 0].bar(models, pred_counts, color=['lightcoral', 'lightblue'], alpha=0.7)\n",
    "    axes[0, 0].axhline(y=true_count, color='green', linestyle='--', label=f'True Entities ({true_count})')\n",
    "    axes[0, 0].set_title('Entity Detection Count')\n",
    "    axes[0, 0].set_ylabel('Number of Entities')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(pred_counts):\n",
    "        axes[0, 0].text(i, v + max(pred_counts) * 0.01, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Confidence comparison\n",
    "    confidences = [baseline_results['avg_confidence'], finetuned_results['avg_confidence']]\n",
    "    \n",
    "    axes[0, 1].bar(models, confidences, color=['lightcoral', 'lightblue'], alpha=0.7)\n",
    "    axes[0, 1].set_title('Average Confidence Score')\n",
    "    axes[0, 1].set_ylabel('Confidence')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(confidences):\n",
    "        axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Improvement percentages\n",
    "    pred_improvement = ((finetuned_results['total_pred_entities'] - baseline_results['total_pred_entities']) / \n",
    "                       baseline_results['total_pred_entities'] * 100) if baseline_results['total_pred_entities'] > 0 else 0\n",
    "    conf_improvement = ((finetuned_results['avg_confidence'] - baseline_results['avg_confidence']) / \n",
    "                       baseline_results['avg_confidence'] * 100) if baseline_results['avg_confidence'] > 0 else 0\n",
    "    \n",
    "    improvements = [pred_improvement, conf_improvement]\n",
    "    metrics = ['Entity Count', 'Confidence']\n",
    "    colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "    \n",
    "    axes[1, 0].bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "    axes[1, 0].set_title('Fine-tuning Improvement (%)')\n",
    "    axes[1, 0].set_ylabel('Improvement (%)')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(improvements):\n",
    "        axes[1, 0].text(i, v + (max(improvements) - min(improvements)) * 0.02, f'{v:+.1f}%', \n",
    "                       ha='center', va='bottom' if v > 0 else 'top')\n",
    "    \n",
    "    # 4. Model summary table\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    summary_data = [\n",
    "        ['Metric', 'Zero-Shot', 'Fine-tuned', 'Change'],\n",
    "        ['Predicted Entities', f\"{baseline_results['total_pred_entities']}\", \n",
    "         f\"{finetuned_results['total_pred_entities']}\", f\"{pred_improvement:+.1f}%\"],\n",
    "        ['Avg Confidence', f\"{baseline_results['avg_confidence']:.3f}\", \n",
    "         f\"{finetuned_results['avg_confidence']:.3f}\", f\"{conf_improvement:+.1f}%\"],\n",
    "        ['True Entities', f\"{baseline_results['total_true_entities']}\", \n",
    "         f\"{finetuned_results['total_true_entities']}\", \"Same\"],\n",
    "        ['Examples Evaluated', f\"{baseline_results['examples_evaluated']}\", \n",
    "         f\"{finetuned_results['examples_evaluated']}\", \"Same\"]\n",
    "    ]\n",
    "    \n",
    "    table = axes[1, 1].table(cellText=summary_data[1:], colLabels=summary_data[0], \n",
    "                            cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1, 1].set_title('Performance Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{OUTPUT_DIR}/finetuning_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n💾 Visualization saved to {OUTPUT_DIR}/finetuning_comparison.png\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_results(baseline_results, finetuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_results(baseline_results: Dict, finetuned_results: Dict, training_history: Dict) -> None:\n",
    "    \"\"\"Save all experiment results for future reference\"\"\"\n",
    "    print(\"💾 Saving experiment results...\")\n",
    "    \n",
    "    # Create results summary\n",
    "    experiment_results = {\n",
    "        \"experiment_info\": {\n",
    "            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"base_model\": MODEL_NAME,\n",
    "            \"output_dir\": OUTPUT_DIR,\n",
    "            \"entity_types\": ENTITY_TYPES,\n",
    "            \"num_entity_types\": len(ENTITY_TYPES),\n",
    "            \"confidence_threshold\": CONFIDENCE_THRESHOLD\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"warmup_ratio\": WARMUP_RATIO,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n",
    "            \"test_size\": TEST_SIZE,\n",
    "            \"random_seed\": RANDOM_SEED\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"total_examples\": len(gliner_examples),\n",
    "            \"train_examples\": len(train_examples),\n",
    "            \"val_examples\": len(val_examples),\n",
    "            \"train_val_split\": f\"{int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}\"\n",
    "        },\n",
    "        \"training_history\": training_history,\n",
    "        \"baseline_results\": baseline_results,\n",
    "        \"finetuned_results\": finetuned_results\n",
    "    }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if baseline_results and finetuned_results:\n",
    "        pred_improvement = ((finetuned_results['total_pred_entities'] - baseline_results['total_pred_entities']) / \n",
    "                           baseline_results['total_pred_entities'] * 100) if baseline_results['total_pred_entities'] > 0 else 0\n",
    "        conf_improvement = ((finetuned_results['avg_confidence'] - baseline_results['avg_confidence']) / \n",
    "                           baseline_results['avg_confidence'] * 100) if baseline_results['avg_confidence'] > 0 else 0\n",
    "        \n",
    "        experiment_results[\"improvements\"] = {\n",
    "            \"entity_count_improvement_percent\": pred_improvement,\n",
    "            \"confidence_improvement_percent\": conf_improvement\n",
    "        }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    results_file = f\"{OUTPUT_DIR}/experiment_results.json\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(experiment_results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Results saved to {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n🎉 GLiNER Fine-tuning Experiment Complete!\")\n",
    "    print(f\"📊 Dataset: {len(gliner_examples)} examples ({len(train_examples)} train, {len(val_examples)} val)\")\n",
    "    print(f\"🏷️ Entity types: {len(ENTITY_TYPES)}\")\n",
    "    print(f\"🤖 Base model: {MODEL_NAME}\")\n",
    "    print(f\"💾 Fine-tuned model saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"📄 Results saved to: {results_file}\")\n",
    "    \n",
    "    if baseline_results and finetuned_results:\n",
    "        print(f\"\\n📈 Key Results:\")\n",
    "        print(f\"  Entity detection: {pred_improvement:+.1f}% change\")\n",
    "        print(f\"  Confidence: {conf_improvement:+.1f}% change\")\n",
    "        \n",
    "        if pred_improvement > 5 and conf_improvement > 5:\n",
    "            print(f\"  🎯 Recommendation: Deploy fine-tuned model\")\n",
    "        elif pred_improvement > 0 and conf_improvement > 0:\n",
    "            print(f\"  ✅ Recommendation: Fine-tuning shows promise, consider more data\")\n",
    "        else:\n",
    "            print(f\"  🤔 Recommendation: Analyze results and consider different approach\")\n",
    "\n",
    "# Save all results\n",
    "save_experiment_results(baseline_results, finetuned_results, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps and Usage\n",
    "\n",
    "### 🚀 **Using the Fine-tuned Model:**\n",
    "\n",
    "```python\n",
    "from gliner import GLiNER\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = GLiNER.from_pretrained('./gliner_finetuned_serbian_legal')\n",
    "\n",
    "# Use for prediction\n",
    "text = \"Your Serbian legal document text here...\"\n",
    "entities = model.predict_entities(text, ENTITY_TYPES, threshold=0.3)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"{entity['label']}: {entity['text']} (confidence: {entity['score']:.3f})\")\n",
    "```\n",
    "\n",
    "### 📈 **Potential Improvements:**\n",
    "\n",
    "1. **More Training Data**: Collect additional annotated Serbian legal documents\n",
    "2. **Data Augmentation**: Use techniques like back-translation or paraphrasing\n",
    "3. **Hyperparameter Tuning**: Experiment with different learning rates, batch sizes\n",
    "4. **Domain-Adaptive Pretraining**: Pre-train on unlabeled Serbian legal text first\n",
    "5. **Ensemble Methods**: Combine multiple fine-tuned models\n",
    "6. **Active Learning**: Iteratively select most informative examples for annotation\n",
    "\n",
    "### 🔍 **Evaluation Considerations:**\n",
    "\n",
    "- **Entity-level F1 Score**: Calculate precision, recall, F1 for each entity type\n",
    "- **Exact Match vs Partial Match**: Consider different evaluation criteria\n",
    "- **Cross-validation**: Use k-fold CV for more robust evaluation\n",
    "- **Error Analysis**: Manually inspect misclassified entities\n",
    "- **Real-world Testing**: Test on completely unseen legal documents\n",
    "\n",
    "### 📚 **References:**\n",
    "\n",
    "- [GLiNER Paper](https://arxiv.org/abs/2311.08526)\n",
    "- [GLiNER GitHub](https://github.com/urchade/GLiNER)\n",
    "- [Fine-tuning Guide](https://medium.com/@dzenan.hamzic_94012/from-data-to-defense-how-i-fine-tuned-aitsecner-to-master-cybersecurity-ner-6b631820d968)\n",
    "- [Serbian Legal NER Research](https://your-research-link-here)\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Experiment completed! Check the results above and the saved model in `./gliner_finetuned_serbian_legal/`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
