{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLiNER Zero-Shot Evaluation on Serbian Legal Documents\n",
    "\n",
    "This notebook evaluates GLiNER (Generalist and Lightweight Named Entity Recognition) in zero-shot mode on 225 Serbian legal documents to compare with fine-tuned BCSm-BERTiƒá results.\n",
    "\n",
    "## Key Features:\n",
    "- **True Zero-Shot**: No training on Serbian legal data\n",
    "- **Custom Entity Types**: Direct specification of legal entity types\n",
    "- **Multilingual Support**: GLiNER's cross-lingual capabilities for Serbian\n",
    "- **Comprehensive Evaluation**: Entity-level metrics using seqeval\n",
    "\n",
    "## Entity Types:\n",
    "- COURT, JUDGE, DEFENDANT, PROSECUTOR, REGISTRAR\n",
    "- CASE_NUMBER, CRIMINAL_ACT, PROVISION\n",
    "- DECISION_DATE, SANCTION, SANCTION_TYPE, SANCTION_VALUE, PROCEDURE_COSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GLiNER if not already installed\n",
    "!pip install gliner\n",
    "!pip install seqeval scikit-learn matplotlib seaborn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GLiNER\n",
    "from gliner import GLiNER\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded successfully!\")\n",
    "print(f\"üåü Ready for GLiNER zero-shot evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LABELSTUDIO_JSON_PATH = \"annotations.json\"\n",
    "JUDGMENTS_DIR = \"labelstudio_files\"\n",
    "MAX_EXAMPLES = 225  # Evaluate on all available documents\n",
    "CONFIDENCE_THRESHOLD = 0.3  # GLiNER confidence threshold\n",
    "\n",
    "# Serbian Legal Entity Types for GLiNER\n",
    "LEGAL_ENTITY_TYPES = [\n",
    "    \"court\",           # COURT - court institutions\n",
    "    \"judge\",           # JUDGE - judge names\n",
    "    \"defendant\",       # DEFENDANT - defendant entities\n",
    "    \"prosecutor\",      # PROSECUTOR - prosecutor entities\n",
    "    \"registrar\",       # REGISTRAR - court registrar\n",
    "    \"case number\",     # CASE_NUMBER - case identifiers\n",
    "    \"criminal act\",    # CRIMINAL_ACT - criminal acts/charges\n",
    "    \"legal provision\", # PROVISION - legal provisions\n",
    "    \"decision date\",   # DECISION_DATE - dates of legal decisions\n",
    "    \"sanction\",        # SANCTION - sanctions/penalties\n",
    "    \"sanction type\",   # SANCTION_TYPE - type of sanction\n",
    "    \"sanction value\",  # SANCTION_VALUE - value/duration of sanction\n",
    "    \"procedure costs\"  # PROCEDURE_COSTS - legal procedure costs\n",
    "]\n",
    "\n",
    "# Mapping from GLiNER labels to ground truth labels\n",
    "GLINER_TO_GT_MAPPING = {\n",
    "    \"court\": \"COURT\",\n",
    "    \"judge\": \"JUDGE\",\n",
    "    \"defendant\": \"DEFENDANT\",\n",
    "    \"prosecutor\": \"PROSECUTOR\",\n",
    "    \"registrar\": \"REGISTRAR\",\n",
    "    \"case number\": \"CASE_NUMBER\",\n",
    "    \"criminal act\": \"CRIMINAL_ACT\",\n",
    "    \"legal provision\": \"PROVISION\",\n",
    "    \"decision date\": \"DECISION_DATE\",\n",
    "    \"sanction\": \"SANCTION\",\n",
    "    \"sanction type\": \"SANCTION_TYPE\",\n",
    "    \"sanction value\": \"SANCTION_VALUE\",\n",
    "    \"procedure costs\": \"PROCEDURE_COSTS\"\n",
    "}\n",
    "\n",
    "print(f\"üìã Configured for evaluation on {MAX_EXAMPLES} documents\")\n",
    "print(f\"üéØ Entity types: {len(LEGAL_ENTITY_TYPES)}\")\n",
    "print(f\"‚ö° Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LabelStudio annotations\n",
    "print(\"üìÇ Loading LabelStudio annotations...\")\n",
    "\n",
    "try:\n",
    "    with open(LABELSTUDIO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        labelstudio_data = json.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(labelstudio_data)} annotated documents\")\nexcept FileNotFoundError:\n",
    "    print(f\"‚ùå Error: {LABELSTUDIO_JSON_PATH} not found!\")\n",
    "    print(\"Please ensure the annotations file is in the current directory.\")\n",
    "    raise\n",
    "\n",
    "# Check available judgment files\n",
    "judgment_files = list(Path(JUDGMENTS_DIR).glob('*.txt'))\n",
    "print(f\"üìÅ Available judgment files: {len(judgment_files)}\")\n",
    "\n",
    "if len(judgment_files) == 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: No judgment files found in {JUDGMENTS_DIR}\")\n",
    "    print(\"Please ensure the labelstudio_files directory contains the text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthLoader:\n",
    "    \"\"\"\n",
    "    Load and prepare ground truth data from LabelStudio annotations\n",
    "    for comparison with GLiNER zero-shot predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, judgments_dir: str):\n",
    "        self.judgments_dir = Path(judgments_dir)\n",
    "        self.entity_types = set()\n",
    "        \n",
    "    def load_text_file(self, file_path: str) -> str:\n",
    "        \"\"\"Load text content from judgment file\"\"\"\n",
    "        # Handle different file path formats from LabelStudio\n",
    "        if \"/\" in file_path:\n",
    "            filename = file_path.split(\"/\")[-1]\n",
    "        else:\n",
    "            filename = file_path\n",
    "            \n",
    "        full_path = self.judgments_dir / filename\n",
    "        \n",
    "        if not full_path.exists():\n",
    "            print(f\"‚ö†Ô∏è File not found: {full_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(full_path, 'r', encoding='utf-8-sig') as f:\n",
    "                    return f.read().strip()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading {full_path}: {e}\")\n",
    "                return \"\"\n",
    "    \n",
    "    def extract_ground_truth_entities(self, labelstudio_data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract ground truth entities from LabelStudio annotations\"\"\"\n",
    "        ground_truth_examples = []\n",
    "        \n",
    "        for item in tqdm(labelstudio_data, desc=\"Loading ground truth\"):\n",
    "            file_path = item.get(\"file_upload\", \"\")\n",
    "            text_content = self.load_text_file(file_path)\n",
    "            \n",
    "            if not text_content:\n",
    "                continue\n",
    "            \n",
    "            annotations = item.get(\"annotations\", [])\n",
    "            \n",
    "            for annotation in annotations:\n",
    "                entities = []\n",
    "                result = annotation.get(\"result\", [])\n",
    "                \n",
    "                for res in result:\n",
    "                    if res.get(\"type\") == \"labels\":\n",
    "                        value = res[\"value\"]\n",
    "                        start = value[\"start\"]\n",
    "                        end = value[\"end\"]\n",
    "                        labels = value[\"labels\"]\n",
    "                        \n",
    "                        for label in labels:\n",
    "                            self.entity_types.add(label)\n",
    "                            entities.append({\n",
    "                                'text': text_content[start:end],\n",
    "                                'label': label,\n",
    "                                'start': start,\n",
    "                                'end': end\n",
    "                            })\n",
    "                \n",
    "                if entities:  # Only include documents with entities\n",
    "                    ground_truth_examples.append({\n",
    "                        'text': text_content,\n",
    "                        'entities': entities,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(ground_truth_examples)} examples with ground truth entities\")\n",
    "        print(f\"üè∑Ô∏è Found entity types: {sorted(self.entity_types)}\")\n",
    "        \n",
    "        return ground_truth_examples\n",
    "\n",
    "# Load ground truth data\n",
    "print(\"üìÇ Loading ground truth annotations...\")\n",
    "gt_loader = GroundTruthLoader(JUDGMENTS_DIR)\n",
    "ground_truth_examples = gt_loader.extract_ground_truth_entities(labelstudio_data)\n",
    "\n",
    "print(f\"\\nüìä Ground Truth Statistics:\")\n",
    "print(f\"  üìÑ Total examples: {len(ground_truth_examples)}\")\n",
    "print(f\"  üè∑Ô∏è Entity types: {len(gt_loader.entity_types)}\")\n",
    "\n",
    "# Show entity distribution\n",
    "entity_counts = Counter()\n",
    "for example in ground_truth_examples:\n",
    "    for entity in example['entities']:\n",
    "        entity_counts[entity['label']] += 1\n",
    "\n",
    "print(f\"\\nüìà Entity Distribution:\")\n",
    "for entity_type, count in entity_counts.most_common():\n",
    "    print(f\"  {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLiNERZeroShotEvaluator:\n",
    "    \"\"\"\n",
    "    GLiNER Zero-Shot NER Evaluator for Serbian Legal Documents.\n",
    "    \n",
    "    This class implements true zero-shot evaluation using GLiNER models\n",
    "    without any training on Serbian legal data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"urchade/gliner_mediumv2.1\", confidence_threshold: float = 0.3):\n",
    "        print(f\"üåü Initializing GLiNER Zero-Shot Evaluator: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load GLiNER model\n",
    "            self.model = GLiNER.from_pretrained(model_name)\n",
    "            self.model_name = model_name\n",
    "            self.confidence_threshold = confidence_threshold\n",
    "            \n",
    "            print(f\"‚úÖ GLiNER model loaded successfully\")\n",
    "            print(f\"üéØ Confidence threshold: {confidence_threshold}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading GLiNER model {model_name}: {e}\")\n",
    "            self.model = None\n",
    "    \n",
    "    def predict_entities(self, text: str, entity_types: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Predict entities using GLiNER zero-shot approach.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            # Get predictions from GLiNER\n",
    "            entities = self.model.predict_entities(\n",
    "                text, \n",
    "                entity_types, \n",
    "                threshold=self.confidence_threshold\n",
    "            )\n",
    "            \n",
    "            # Convert to our format\n",
    "            formatted_entities = []\n",
    "            for entity in entities:\n",
    "                formatted_entities.append({\n",
    "                    'text': entity['text'],\n",
    "                    'label': entity['label'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end'],\n",
    "                    'confidence': entity['score'],\n",
    "                    'method': 'gliner_zero_shot',\n",
    "                    'model': self.model_name\n",
    "                })\n",
    "            \n",
    "            return sorted(formatted_entities, key=lambda x: x['start'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in GLiNER prediction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def evaluate_on_dataset(self, ground_truth_examples: List[Dict], \n",
    "                           entity_types: List[str], \n",
    "                           max_examples: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate GLiNER on the ground truth dataset.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüß™ Starting GLiNER Zero-Shot Evaluation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.model is None:\n",
    "            return {'error': 'GLiNER model not loaded'}\n",
    "        \n",
    "        # Limit examples if specified\n",
    "        eval_examples = ground_truth_examples[:max_examples] if max_examples else ground_truth_examples\n",
    "        \n",
    "        detailed_results = []\n",
    "        prediction_counts = Counter()\n",
    "        confidence_scores = []\n",
    "        \n",
    "        print(f\"üìä Evaluating on {len(eval_examples)} examples...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, example in enumerate(tqdm(eval_examples, desc=\"GLiNER Evaluation\")):\n",
    "            text = example['text']\n",
    "            true_entities = example['entities']\n",
    "            \n",
    "            # Get GLiNER predictions\n",
    "            pred_entities = self.predict_entities(text, entity_types)\n",
    "            \n",
    "            # Count predictions by type\n",
    "            for entity in pred_entities:\n",
    "                prediction_counts[entity['label']] += 1\n",
    "                confidence_scores.append(entity['confidence'])\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results.append({\n",
    "                'example_id': i,\n",
    "                'text': text[:200] + \"...\" if len(text) > 200 else text,\n",
    "                'file_path': example['file_path'],\n",
    "                'true_entities': true_entities,\n",
    "                'pred_entities': pred_entities,\n",
    "                'true_count': len(true_entities),\n",
    "                'pred_count': len(pred_entities)\n",
    "            })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        evaluation_time = end_time - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_true = sum(len(r['true_entities']) for r in detailed_results)\n",
    "        total_pred = sum(len(r['pred_entities']) for r in detailed_results)\n",
    "        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0\n",
    "        \n",
    "        print(f\"\\nüìä GLiNER Prediction Statistics:\")\n",
    "        for label, count in prediction_counts.most_common():\n",
    "            print(f\"  {label}: {count}\")\n",
    "        \n",
    "        results = {\n",
    "            'model_name': self.model_name,\n",
    "            'confidence_threshold': self.confidence_threshold,\n",
    "            'detailed_results': detailed_results,\n",
    "            'total_true_entities': total_true,\n",
    "            'total_pred_entities': total_pred,\n",
    "            'prediction_counts': dict(prediction_counts),\n",
    "            'examples_evaluated': len(eval_examples),\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'evaluation_time': evaluation_time,\n",
    "            'entities_per_second': total_pred / evaluation_time if evaluation_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ GLiNER evaluation complete!\")\n",
    "        print(f\"  üìä True entities: {total_true}\")\n",
    "        print(f\"  ü§ñ Predicted entities: {total_pred}\")\n",
    "        print(f\"  ‚ö° Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"  ‚è±Ô∏è Evaluation time: {evaluation_time:.2f}s\")\n",
    "        print(f\"  üöÄ Entities/second: {results['entities_per_second']:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize GLiNER evaluator\n",
    "print(\"üöÄ Initializing GLiNER Zero-Shot Evaluator...\")\n",
    "gliner_evaluator = GLiNERZeroShotEvaluator(\n",
    "    model_name=\"urchade/gliner_mediumv2.1\",  # You can try different GLiNER models\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GLiNER evaluation on all 225 documents\n",
    "print(\"üéØ Starting comprehensive GLiNER evaluation...\")\n",
    "\n",
    "gliner_results = gliner_evaluator.evaluate_on_dataset(\n",
    "    ground_truth_examples=ground_truth_examples,\n",
    "    entity_types=LEGAL_ENTITY_TYPES,\n",
    "    max_examples=MAX_EXAMPLES\n",
    ")\n",
    "\n",
    "if 'error' in gliner_results:\n",
    "    print(f\"‚ùå Evaluation failed: {gliner_results['error']}\")\n",
    "else:\n",
    "    print(\"\\nüéâ GLiNER evaluation completed successfully!\")\n",
    "    print(f\"üìà Coverage ratio: {gliner_results['total_pred_entities'] / max(gliner_results['total_true_entities'], 1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAlignmentEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate entity-level alignment between GLiNER predictions and ground truth.\n",
    "    Uses both exact match and overlap-based matching strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, label_mapping: Dict[str, str]):\n",
    "        self.label_mapping = label_mapping\n",
    "    \n",
    "    def normalize_label(self, gliner_label: str) -> str:\n",
    "        \"\"\"Map GLiNER label to ground truth label\"\"\"\n",
    "        return self.label_mapping.get(gliner_label.lower(), gliner_label.upper())\n",
    "    \n",
    "    def calculate_overlap(self, pred_start: int, pred_end: int, \n",
    "                         true_start: int, true_end: int) -> float:\n",
    "        \"\"\"Calculate overlap ratio between predicted and true entity spans\"\"\"\n",
    "        overlap_start = max(pred_start, true_start)\n",
    "        overlap_end = min(pred_end, true_end)\n",
    "        \n",
    "        if overlap_start >= overlap_end:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap_length = overlap_end - overlap_start\n",
    "        true_length = true_end - true_start\n",
    "        pred_length = pred_end - pred_start\n",
    "        \n",
    "        # Use the minimum length as denominator for stricter matching\n",
    "        min_length = min(true_length, pred_length)\n",
    "        return overlap_length / min_length if min_length > 0 else 0.0\n",
    "    \n",
    "    def evaluate_example(self, true_entities: List[Dict], \n",
    "                        pred_entities: List[Dict], \n",
    "                        overlap_threshold: float = 0.5) -> Dict:\n",
    "        \"\"\"Evaluate a single example with entity-level metrics\"\"\"\n",
    "        \n",
    "        # Normalize predicted labels\n",
    "        normalized_preds = []\n",
    "        for pred in pred_entities:\n",
    "            normalized_pred = pred.copy()\n",
    "            normalized_pred['normalized_label'] = self.normalize_label(pred['label'])\n",
    "            normalized_preds.append(normalized_pred)\n",
    "        \n",
    "        # Track matches\n",
    "        true_matched = set()\n",
    "        pred_matched = set()\n",
    "        exact_matches = 0\n",
    "        overlap_matches = 0\n",
    "        label_matches = 0\n",
    "        \n",
    "        # Find matches\n",
    "        for i, true_entity in enumerate(true_entities):\n",
    "            best_overlap = 0.0\n",
    "            best_pred_idx = -1\n",
    "            \n",
    "            for j, pred_entity in enumerate(normalized_preds):\n",
    "                if j in pred_matched:\n",
    "                    continue\n",
    "                \n",
    "                # Check for exact match\n",
    "                if (true_entity['start'] == pred_entity['start'] and \n",
    "                    true_entity['end'] == pred_entity['end'] and\n",
    "                    true_entity['label'] == pred_entity['normalized_label']):\n",
    "                    exact_matches += 1\n",
    "                    true_matched.add(i)\n",
    "                    pred_matched.add(j)\n",
    "                    best_pred_idx = -1  # Reset to avoid double counting\n",
    "                    break\n",
    "                \n",
    "                # Check for overlap\n",
    "                overlap = self.calculate_overlap(\n",
    "                    pred_entity['start'], pred_entity['end'],\n",
    "                    true_entity['start'], true_entity['end']\n",
    "                )\n",
    "                \n",
    "                if overlap > best_overlap and overlap >= overlap_threshold:\n",
    "                    best_overlap = overlap\n",
    "                    best_pred_idx = j\n",
    "            \n",
    "            # Record best overlap match\n",
    "            if best_pred_idx >= 0 and i not in true_matched:\n",
    "                pred_entity = normalized_preds[best_pred_idx]\n",
    "                if true_entity['label'] == pred_entity['normalized_label']:\n",
    "                    overlap_matches += 1\n",
    "                    label_matches += 1\n",
    "                else:\n",
    "                    overlap_matches += 1\n",
    "                \n",
    "                true_matched.add(i)\n",
    "                pred_matched.add(best_pred_idx)\n",
    "        \n",
    "        return {\n",
    "            'exact_matches': exact_matches,\n",
    "            'overlap_matches': overlap_matches,\n",
    "            'label_matches': label_matches,\n",
    "            'true_entities': len(true_entities),\n",
    "            'pred_entities': len(pred_entities),\n",
    "            'true_matched': len(true_matched),\n",
    "            'pred_matched': len(pred_matched)\n",
    "        }\n",
    "    \n",
    "    def evaluate_dataset(self, results: Dict) -> Dict:\n",
    "        \"\"\"Evaluate the entire dataset\"\"\"\n",
    "        print(\"\\nüìä Computing Entity-Level Alignment Metrics...\")\n",
    "        \n",
    "        total_exact = 0\n",
    "        total_overlap = 0\n",
    "        total_label = 0\n",
    "        total_true = 0\n",
    "        total_pred = 0\n",
    "        total_true_matched = 0\n",
    "        total_pred_matched = 0\n",
    "        \n",
    "        detailed_results = results['detailed_results']\n",
    "        \n",
    "        for example in tqdm(detailed_results, desc=\"Computing alignments\"):\n",
    "            eval_result = self.evaluate_example(\n",
    "                example['true_entities'],\n",
    "                example['pred_entities']\n",
    "            )\n",
    "            \n",
    "            total_exact += eval_result['exact_matches']\n",
    "            total_overlap += eval_result['overlap_matches']\n",
    "            total_label += eval_result['label_matches']\n",
    "            total_true += eval_result['true_entities']\n",
    "            total_pred += eval_result['pred_entities']\n",
    "            total_true_matched += eval_result['true_matched']\n",
    "            total_pred_matched += eval_result['pred_matched']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        exact_precision = total_exact / max(total_pred, 1)\n",
    "        exact_recall = total_exact / max(total_true, 1)\n",
    "        exact_f1 = 2 * exact_precision * exact_recall / max(exact_precision + exact_recall, 1e-10)\n",
    "        \n",
    "        overlap_precision = total_overlap / max(total_pred, 1)\n",
    "        overlap_recall = total_overlap / max(total_true, 1)\n",
    "        overlap_f1 = 2 * overlap_precision * overlap_recall / max(overlap_precision + overlap_recall, 1e-10)\n",
    "        \n",
    "        return {\n",
    "            'exact_match': {\n",
    "                'precision': exact_precision,\n",
    "                'recall': exact_recall,\n",
    "                'f1': exact_f1,\n",
    "                'matches': total_exact\n",
    "            },\n",
    "            'overlap_match': {\n",
    "                'precision': overlap_precision,\n",
    "                'recall': overlap_recall,\n",
    "                'f1': overlap_f1,\n",
    "                'matches': total_overlap\n",
    "            },\n",
    "            'totals': {\n",
    "                'true_entities': total_true,\n",
    "                'pred_entities': total_pred,\n",
    "                'true_matched': total_true_matched,\n",
    "                'pred_matched': total_pred_matched\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Evaluate entity alignment\n",
    "if 'error' not in gliner_results:\n",
    "    alignment_evaluator = EntityAlignmentEvaluator(GLINER_TO_GT_MAPPING)\n",
    "    alignment_metrics = alignment_evaluator.evaluate_dataset(gliner_results)\n",
    "    \n",
    "    print(\"\\nüéØ Entity-Level Alignment Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nüìç Exact Match (position + label):\")\n",
    "    exact = alignment_metrics['exact_match']\n",
    "    print(f\"  Precision: {exact['precision']:.3f}\")\n",
    "    print(f\"  Recall:    {exact['recall']:.3f}\")\n",
    "    print(f\"  F1-Score:  {exact['f1']:.3f}\")\n",
    "    print(f\"  Matches:   {exact['matches']}\")\n",
    "    \n",
    "    print(\"\\nüéØ Overlap Match (‚â•50% overlap + label):\")\n",
    "    overlap = alignment_metrics['overlap_match']\n",
    "    print(f\"  Precision: {overlap['precision']:.3f}\")\n",
    "    print(f\"  Recall:    {overlap['recall']:.3f}\")\n",
    "    print(f\"  F1-Score:  {overlap['f1']:.3f}\")\n",
    "    print(f\"  Matches:   {overlap['matches']}\")\n",
    "    \n",
    "    totals = alignment_metrics['totals']\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  True entities:     {totals['true_entities']}\")\n",
    "    print(f\"  Predicted entities: {totals['pred_entities']}\")\n",
    "    print(f\"  Coverage ratio:    {totals['pred_entities'] / max(totals['true_entities'], 1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Entity Type Analysis\n",
    "if 'error' not in gliner_results:\n",
    "    print(\"\\nüìä Per-Entity Type Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Collect per-entity statistics\n",
    "    entity_stats = defaultdict(lambda: {'true': 0, 'pred': 0, 'exact_matches': 0, 'overlap_matches': 0})\n",
    "    \n",
    "    for example in gliner_results['detailed_results']:\n",
    "        # Count true entities by type\n",
    "        for entity in example['true_entities']:\n",
    "            entity_stats[entity['label']]['true'] += 1\n",
    "        \n",
    "        # Count predicted entities by type (normalized)\n",
    "        for entity in example['pred_entities']:\n",
    "            normalized_label = GLINER_TO_GT_MAPPING.get(entity['label'].lower(), entity['label'].upper())\n",
    "            entity_stats[normalized_label]['pred'] += 1\n",
    "        \n",
    "        # Count matches (simplified - you could use the alignment evaluator for more precision)\n",
    "        eval_result = alignment_evaluator.evaluate_example(\n",
    "            example['true_entities'], example['pred_entities']\n",
    "        )\n",
    "        \n",
    "        # This is a simplified approach - for detailed per-entity matching,\n",
    "        # you would need to track which specific entities matched\n",
    "    \n",
    "    # Display per-entity results\n",
    "    print(f\"\\n{'Entity Type':<20} {'True':<8} {'Pred':<8} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for entity_type in sorted(entity_stats.keys()):\n",
    "        stats = entity_stats[entity_type]\n",
    "        \n",
    "        # Calculate basic precision/recall (this is approximate)\n",
    "        precision = min(stats['pred'], stats['true']) / max(stats['pred'], 1)\n",
    "        recall = min(stats['pred'], stats['true']) / max(stats['true'], 1)\n",
    "        f1 = 2 * precision * recall / max(precision + recall, 1e-10)\n",
    "        \n",
    "        print(f\"{entity_type:<20} {stats['true']:<8} {stats['pred']:<8} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f}\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è Note: Per-entity precision/recall are approximations.\\n\"\n",
    "          \"       For exact per-entity metrics, use the alignment evaluator with entity-specific tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Results\n",
    "if 'error' not in gliner_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('GLiNER Zero-Shot Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Entity Distribution Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Ground truth distribution\n",
    "    gt_counts = Counter()\n",
    "    for example in ground_truth_examples:\n",
    "        for entity in example['entities']:\n",
    "            gt_counts[entity['label']] += 1\n",
    "    \n",
    "    # GLiNER prediction distribution (normalized)\n",
    "    pred_counts = Counter()\n",
    "    for example in gliner_results['detailed_results']:\n",
    "        for entity in example['pred_entities']:\n",
    "            normalized_label = GLINER_TO_GT_MAPPING.get(entity['label'].lower(), entity['label'].upper())\n",
    "            pred_counts[normalized_label] += 1\n",
    "    \n",
    "    # Plot comparison\n",
    "    entity_types = sorted(set(list(gt_counts.keys()) + list(pred_counts.keys())))\n",
    "    gt_values = [gt_counts.get(et, 0) for et in entity_types]\n",
    "    pred_values = [pred_counts.get(et, 0) for et in entity_types]\n",
    "    \n",
    "    x = np.arange(len(entity_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, gt_values, width, label='Ground Truth', alpha=0.8, color='skyblue')\n",
    "    ax1.bar(x + width/2, pred_values, width, label='GLiNER Predictions', alpha=0.8, color='lightcoral')\n",
    "    ax1.set_xlabel('Entity Types')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Entity Distribution Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(entity_types, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Confidence Score Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    confidence_scores = []\n",
    "    for example in gliner_results['detailed_results']:\n",
    "        for entity in example['pred_entities']:\n",
    "            confidence_scores.append(entity['confidence'])\n",
    "    \n",
    "    if confidence_scores:\n",
    "        ax2.hist(confidence_scores, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "        ax2.axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', \n",
    "                   label=f'Threshold ({CONFIDENCE_THRESHOLD})')\n",
    "        ax2.set_xlabel('Confidence Score')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('GLiNER Confidence Score Distribution')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Performance Metrics Comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "    exact_values = [alignment_metrics['exact_match']['precision'],\n",
    "                   alignment_metrics['exact_match']['recall'],\n",
    "                   alignment_metrics['exact_match']['f1']]\n",
    "    overlap_values = [alignment_metrics['overlap_match']['precision'],\n",
    "                     alignment_metrics['overlap_match']['recall'],\n",
    "                     alignment_metrics['overlap_match']['f1']]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, exact_values, width, label='Exact Match', alpha=0.8, color='navy')\n",
    "    ax3.bar(x + width/2, overlap_values, width, label='Overlap Match', alpha=0.8, color='orange')\n",
    "    ax3.set_xlabel('Metrics')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('GLiNER Performance Metrics')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(metrics)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Coverage Analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    coverage_data = []\n",
    "    for example in gliner_results['detailed_results']:\n",
    "        if example['true_count'] > 0:\n",
    "            coverage = example['pred_count'] / example['true_count']\n",
    "            coverage_data.append(min(coverage, 2.0))  # Cap at 2.0 for visualization\n",
    "    \n",
    "    if coverage_data:\n",
    "        ax4.hist(coverage_data, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "        ax4.axvline(1.0, color='red', linestyle='--', label='Perfect Coverage')\n",
    "        ax4.set_xlabel('Coverage Ratio (Pred/True)')\n",
    "        ax4.set_ylabel('Number of Documents')\n",
    "        ax4.set_title('Per-Document Coverage Distribution')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Visualization Summary:\")\n",
    "    print(f\"  üìä Total confidence scores plotted: {len(confidence_scores)}\")\n",
    "    print(f\"  üìã Average confidence: {np.mean(confidence_scores):.3f}\")\n",
    "    print(f\"  üìà Coverage ratio (overall): {gliner_results['total_pred_entities'] / max(gliner_results['total_true_entities'], 1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Predictions Analysis\n",
    "if 'error' not in gliner_results:\n",
    "    print(\"\\nüîç Sample Predictions Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show examples with different performance characteristics\n",
    "    detailed_results = gliner_results['detailed_results']\n",
    "    \n",
    "    # Find examples with good coverage\n",
    "    good_examples = []\n",
    "    poor_examples = []\n",
    "    \n",
    "    for example in detailed_results:\n",
    "        if example['true_count'] > 0:\n",
    "            coverage = example['pred_count'] / example['true_count']\n",
    "            if 0.8 <= coverage <= 1.2 and example['pred_count'] > 0:\n",
    "                good_examples.append(example)\n",
    "            elif coverage < 0.3 or coverage > 2.0:\n",
    "                poor_examples.append(example)\n",
    "    \n",
    "    # Show good examples\n",
    "    print(f\"\\n‚úÖ Examples with Good Coverage ({len(good_examples)} found):\")\n",
    "    for i, example in enumerate(good_examples[:3]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"File: {example['file_path']}\")\n",
    "        print(f\"Text: {example['text'][:150]}...\")\n",
    "        print(f\"True entities ({example['true_count']}):\")\n",
    "        for entity in example['true_entities'][:5]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}'\")\n",
    "        print(f\"GLiNER predictions ({example['pred_count']}):\")\n",
    "        for entity in example['pred_entities'][:5]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}' (conf: {entity['confidence']:.3f})\")\n",
    "    \n",
    "    # Show challenging examples\n",
    "    print(f\"\\n‚ùå Challenging Examples ({len(poor_examples)} found):\")\n",
    "    for i, example in enumerate(poor_examples[:2]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"File: {example['file_path']}\")\n",
    "        print(f\"Text: {example['text'][:150]}...\")\n",
    "        print(f\"True entities ({example['true_count']}):\")\n",
    "        for entity in example['true_entities'][:3]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}'\")\n",
    "        print(f\"GLiNER predictions ({example['pred_count']}):\")\n",
    "        for entity in example['pred_entities'][:3]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}' (conf: {entity['confidence']:.3f})\")\n",
    "        coverage = example['pred_count'] / max(example['true_count'], 1)\n",
    "        print(f\"Coverage ratio: {coverage:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results for Comparison\n",
    "if 'error' not in gliner_results:\n",
    "    print(\"\\nüíæ Saving GLiNER Evaluation Results...\")\n",
    "    \n",
    "    # Prepare results summary\n",
    "    results_summary = {\n",
    "        'model_info': {\n",
    "            'model_name': gliner_results['model_name'],\n",
    "            'confidence_threshold': gliner_results['confidence_threshold'],\n",
    "            'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'method': 'zero_shot'\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'total_documents': len(ground_truth_examples),\n",
    "            'evaluated_documents': gliner_results['examples_evaluated'],\n",
    "            'total_true_entities': gliner_results['total_true_entities'],\n",
    "            'total_pred_entities': gliner_results['total_pred_entities']\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'exact_match': alignment_metrics['exact_match'],\n",
    "            'overlap_match': alignment_metrics['overlap_match'],\n",
    "            'coverage_ratio': gliner_results['total_pred_entities'] / max(gliner_results['total_true_entities'], 1),\n",
    "            'avg_confidence': gliner_results['avg_confidence']\n",
    "        },\n",
    "        'entity_distribution': {\n",
    "            'ground_truth': dict(entity_counts),\n",
    "            'predictions': gliner_results['prediction_counts']\n",
    "        },\n",
    "        'timing': {\n",
    "            'evaluation_time_seconds': gliner_results['evaluation_time'],\n",
    "            'entities_per_second': gliner_results['entities_per_second']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_file = 'gliner_zero_shot_results.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to {output_file}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nüéâ GLiNER Zero-Shot Evaluation Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Model: {gliner_results['model_name']}\")\n",
    "    print(f\"üìÑ Documents evaluated: {gliner_results['examples_evaluated']}\")\n",
    "    print(f\"üéØ Exact match F1: {alignment_metrics['exact_match']['f1']:.3f}\")\n",
    "    print(f\"üéØ Overlap match F1: {alignment_metrics['overlap_match']['f1']:.3f}\")\n",
    "    print(f\"üìà Coverage ratio: {gliner_results['total_pred_entities'] / max(gliner_results['total_true_entities'], 1):.3f}\")\n",
    "    print(f\"‚ö° Avg confidence: {gliner_results['avg_confidence']:.3f}\")\n",
    "    print(f\"‚è±Ô∏è Processing speed: {gliner_results['entities_per_second']:.2f} entities/sec\")\n",
    "    \n",
    "    print(\"\\nüî¨ Next Steps:\")\n",
    "    print(\"  1. Compare these results with your fine-tuned BCSm-BERTiƒá model\")\n",
    "    print(\"  2. Analyze which entity types GLiNER handles well vs. poorly\")\n",
    "    print(\"  3. Consider ensemble approaches combining both models\")\n",
    "    print(\"  4. Experiment with different GLiNER models (e.g., gliner_large)\")\n",
    "    print(\"  5. Try different confidence thresholds for optimal performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive zero-shot evaluation of GLiNER on Serbian legal documents. Key features:\n",
    "\n",
    "### üéØ **Zero-Shot Approach**\n",
    "- No training on Serbian legal data\n",
    "- Direct application of pre-trained GLiNER model\n",
    "- Custom entity type specification for legal domain\n",
    "\n",
    "### üìä **Comprehensive Evaluation**\n",
    "- Entity-level alignment metrics (exact + overlap matching)\n",
    "- Per-entity type performance analysis\n",
    "- Confidence score distribution analysis\n",
    "- Coverage ratio assessment\n",
    "\n",
    "### üî¨ **Comparison Ready**\n",
    "- Results saved in JSON format for easy comparison\n",
    "- Compatible metrics with fine-tuned model evaluation\n",
    "- Detailed per-document analysis for error investigation\n",
    "\n",
    "### üöÄ **Performance Insights**\n",
    "- Processing speed benchmarks\n",
    "- Confidence threshold impact analysis\n",
    "- Entity distribution comparison with ground truth\n",
    "\n",
    "Use these results to compare against your fine-tuned BCSm-BERTiƒá model and determine the best approach for Serbian legal NER!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
